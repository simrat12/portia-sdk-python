# Contents of . source tree

## README

```markdown
<p align="center">
  <img style="width: 200px; height: 178px" src="Logo_Portia_Stacked_Black.png" />
</p>

# Portia SDK Python

Portia AI is an open source developer framework for stateful, authenticated agentic workflows. The core product accessible in this repository is extensible with our complimentary cloud features which are aimed at making production deployments easier and faster.
Play around, break things and tell us how you're getting on in our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a>. Most importantly please be kind to your fellow humans (<a href="https://github.com/portiaAI/portia-sdk-python/blob/main/CODE_OF_CONDUCT.md" target="_blank" rel="noopener noreferrer">**Code of Conduct (↗)**</a>).

If you want to dive straight in with an example, check out our <a href="https://github.com/portiaAI/portia-agent-examples/tree/main/get_started_google_tools" target="_blank">**Google Tools example (↗)**</a>.

## Why Portia AI
| Problem | Portia's answer |
| ------- | --------------- |
| **Planning:** Many use cases require visibility into the LLM’s reasoning, particularly for complex tasks requiring multiple steps and tools. LLMs also struggle picking the right tools as their tool set grows: a recurring limitation for production deployments | **Multi-agent plans:** Our open source, multi-shot prompter guides your LLM to produce a [`Plan`](https://docs.portialabs.ai/generate-plan) in response to a prompt, weaving the relevant tools, inputs and outputs for every step. |
| **Execution:** Tracking an LLM’s progress mid-task is difficult, making it harder to intervene when guidance is needed. This is especially critical for enforcing company policies or correcting hallucinations (hello, missing arguments in tool calls!) | **Stateful PlanRuns:** Portia will spin up a multi-agent [`PlanRun`](https://docs.portialabs.ai/execute-workflow) to execute on generated plans and track their state throughout execution. Using our [`Clarification`](https://docs.portialabs.ai/manage-clarifications) abstraction you can define points where you want to take control of run execution e.g. to resolve missing information or multiple choice decisions. Portia serialises the run state, and you can manage its storage / retrieval yourself or use our cloud offering for simplicity. |
| **Authentication:** Existing solutions often disrupt the user experience with cumbersome authentication flows or require pre-emptive, full access to every tool—an approach that doesn’t scale for multi-agent assistants. | **Extensible, authenticated tool calling:** Bring your own tools on our extensible [`Tool`](https://docs.portialabs.ai/extend-tool-definitions) abstraction, or use our growing plug and play authenticated [tool library](https://docs.portialabs.ai/run-portia-tools), which will include a number of popular SaaS providers over time (Google, Zendesk, Hubspot, Github etc.). All Portia tools feature just-in-time authentication with token refresh, offering security without compromising on user experience. |


## Quickstart

### Installation

0. Ensure you have python 3.11 or higher installed. If you need to update your python version please visit their [docs](https://www.python.org/downloads/).
```bash
python --version
```

1. Install the Portia Python SDK
```bash
pip install portia-sdk-python 
```

>[!NOTE]
> OpenAI and Anthropic dependencies are included by default. We also provide the following extras:<br/>
> * **MistralAI**: `portia-sdk-python[mistral]`
> * **Google Generative AI**: `portia-sdk-python[google]`
> 
> Alternatively you can add all dependencies with `portia-sdk-python[all]`

2. Ensure you have an API key set up
```bash
export OPENAI_API_KEY='your-api-key-here'
```
3. Validate your installation by submitting a simple maths prompt from the command line
```
portia-cli run "add 1 + 2"
```
>[!NOTE]
> We support Anthropic and Mistral AI as well and we're working on adding more models asap. For now if you want to use either model you'd have to set up the relevant API key and add one of these args to your CLI command:<br/>
> `portia-cli run --llm-provider="anthropic" "add 1 + 2"` or `portia-cli run --llm-provider="mistralai" "add 1 + 2"`

**All set? Now let's explore some basic usage of the product 🚀**

### E2E example repo
We have a repo that showcases some of our core concepts to get you started. It's available <a href="https://github.com/portiaAI/portia-agent-examples" target="_blank">**here (↗)**</a>. We recommend starting with the <a href="https://github.com/portiaAI/portia-agent-examples/tree/main/get_started_google_tools" target="_blank">**Google Tools example (↗)**</a> if you are brand new to Portia.

### E2E example with open source tools
This example is meant to get you familiar with a few of our core abstractions:
- A `Plan` is the set of steps an LLM thinks it should take in order to respond to a user prompt. They are immutable, structured and human-readable.
- A `PlanRun` is a unique instantiation of a `Plan`. The purpose of a `PlanRun` is to capture the state of a unique plan run at every step in an auditable way.
- `Portia` orchestrates plan generation and execution, including the creation, pausing and resumption of plan runs.

Before running the code below, make sure you have the following keys set as environment variables in your .env file:
- An OpenAI API key (or other LLM API key) set as `OPENAI_API_KEY=`
- A Tavily <a href="https://tavily.com/" target="_blank">(**↗**)</a> API key set as `TAVILY_API_KEY=`

```python
from dotenv import load_dotenv
from portia import Portia, default_config, example_tool_registry

load_dotenv()

# Instantiate a Portia client. Load it with the default config and with the example tools.
portia = Portia(config=default_config(), tools=example_tool_registry)

# Generate the plan from the user query
plan = portia.plan('Which stock price grew faster in 2024, Amazon or Google?')
print(plan.model_dump_json(indent=2))

# Create and execute the run from the generated plan
plan_run = portia.run_plan(plan)

# Serialise into JSON and print the output
print(plan_run.model_dump_json(indent=2))
```

### E2E example with Portia cloud storage
Our cloud offering will allow you to easily store and retrieve plans in the Portia cloud, access our library of cloud hosted tools, and use the Portia dashboard to view plan runs, clarifications and tool call logs. Head over to <a href="https://app.portialabs.ai" target="_blank">**app.portialabs.ai (↗)**</a> and get your Portia API key. You will need to set it as the env variable `PORTIA_API_KEY`.<br/>
Note that this example also requires the environment variables `OPENAI_API_KEY` (or ANTHROPIC or MISTRALAI if you're using either) and `TAVILY_API_KEY` as the [previous one](#e2e-example-with-open-source-tools).

The example below introduces **some** of the config options available with Portia AI:
- The `storage_class` is set using the `StorageClass.CLOUD` ENUM. So long as your `PORTIA_API_KEY` is set, runs and tool calls will be logged and appear automatically in your Portia dashboard at <a href="https://app.portialabs.ai" target="_blank">**app.portialabs.ai (↗)**</a>.
- The `default_log_level` is set using the `LogLevel.DEBUG` ENUM to `DEBUG` so you can get some insight into the sausage factory in your terminal, including plan generation, run states, tool calls and outputs at every step 😅
- The `llm_provider`, `llm_model` and `xxx_api_key` (varies depending on model provider chosen) are used to choose the specific LLM provider and model. In the example below we're splurging and using GPT 4.0!

Finally we also introduce the concept of a `tool_registry`, which is a flexible grouping of tools.

```python
import os
from dotenv import load_dotenv
from portia import (
    Portia,
    Config,
    StorageClass,
    LogLevel,
    LLMProvider,
    LLMModel,
    example_tool_registry,
)

load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Load the default config and override the storage class to point to the Portia cloud
my_config = Config.from_default(
    storage_class=StorageClass.CLOUD,
    default_log_level=LogLevel.DEBUG,
    llm_provider=LLMProvider.OPENAI, # You can use `MISTRAL`, `ANTHROPIC` instead
    llm_model_name=LLMModel.GPT_4_O, # You can use any of the available models instead
    openai_api_key=OPENAI_API_KEY # Use `mistralai_api_key=MISTRALAI` or `anthropic_api_key=ANTHROPIC_API_KEY` instead
)

# Instantiate a Portia client. Load it with the config and with the open source example tool registry
portia = Portia(config=my_config, tools=example_tool_registry)

# Execute query.
plan_run = portia.run('Which stock price grew faster in 2024, Amazon or Google?')

# Serialise into JSON an print the output
print(plan_run.model_dump_json(indent=2))
```

## Learn more
- Head over to our docs at <a href="https://docs.portialabs.ai" target="_blank">**docs.portialabs.ai (↗)**</a>.
- Join the conversation on our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a>.
- Watch us embarrass ourselves on our <a href="https://www.youtube.com/@PortiaAI" target="_blank">**YouTube channel (↗)**</a>.
- Follow us on <a href="https://www.producthunt.com/posts/portia-ai" target="_blank">**Product Hunt (↗)**</a>.

## Contribution guidelines
Head on over to our <a href="https://github.com/portiaAI/portia-sdk-python/blob/main/CONTRIBUTING.md" target="_blank">**contribution guide (↗)**</a> for details.

## Support
We love feedback and suggestions. Please join our <a href="https://discord.gg/DvAJz9ffaR" target="_blank">**Discord channel (↗)**</a> to chat with us.

We also particularly appreciate github stars. If you've liked what you've seen, please give us a star <a href="https://github.com/portiaAI/portia-sdk-python" target="_blank">at the top of the page</a>.
```

## File: example.py

```python
"""Simple Example."""

from portia import (
    Config,
    LogLevel,
    PlanRunState,
    Portia,
    example_tool_registry,
    execution_context,
)
from portia.cli import CLIExecutionHooks

portia = Portia(
    Config.from_default(default_log_level=LogLevel.DEBUG),
    tools=example_tool_registry,
)


# Simple Example
plan_run = portia.run(
    "Get the temperature in London and Sydney and then add the two temperatures rounded to 2DP",
)

# We can also provide additional execution context to the process
with execution_context(end_user_id="123", additional_data={"email_address": "hello@portialabs.ai"}):
    plan_run = portia.run(
        "Get the temperature in London and Sydney and then add the two temperatures rounded to 2DP",
    )

# When we hit a clarification we can ask our end user for clarification then resume the process
with execution_context(end_user_id="123", additional_data={"email_address": "hello@portialabs.ai"}):
    # Deliberate typo in the second place name to hit the clarification
    plan_run = portia.run(
        "Get the temperature in London and xydwne and then add the two temperatures rounded to 2DP",
    )

# Fetch run
plan_run = portia.storage.get_plan_run(plan_run.id)
# Update clarifications
if plan_run.state == PlanRunState.NEED_CLARIFICATION:
    for c in plan_run.get_outstanding_clarifications():
        # Here you prompt the user for the response to the clarification
        # via whatever mechanism makes sense for your use-case.
        new_value = "Sydney"
        plan_run = portia.resolve_clarification(
            plan_run=plan_run,
            clarification=c,
            response=new_value,
        )

# Execute again with the same execution context
with execution_context(context=plan_run.execution_context):
    portia.resume(plan_run)

# You can also pass in a clarification handler to manage clarifications
portia = Portia(
    Config.from_default(default_log_level=LogLevel.DEBUG),
    tools=example_tool_registry,
    execution_hooks=CLIExecutionHooks(),
)
plan_run = portia.run(
    "Get the temperature in London and xydwne and then add the two temperatures rounded to 2DP",
)

```

## File: CONTRIBUTING.md

```markdown
# Contributing to Portia SDK 🏗️

Thank you for your interest in contributing to Portia SDK! We welcome contributions that improve the library and help us build a great experience for the community.

## What to contribute
* **Documentation** Tutorials, how-to guides and revisions to our existing docs go a long way in making our repo easier to setup and use
* **Examples** Show our community what you can do with our SDK. We particularly encourage end-to-end, real-world applications
* **Bug reports** Please include a detailed method to reproduce any bugs you spot. We would be grateful if you give the [Issue Tracker](https://github.com/portiaAI/portia-sdk-python/issues) a quick skim first to avoid duplicates 🙌
* **Bug fixes** Those are our favourites! Please avoid breaking changes. The next section has some helpful tips for that.
* **Feedback** Help us be better. Come chat to us on [Discord](https://discord.gg/DvAJz9ffaR) about your experience using the SDK 🫶

⚠️ **A note on new features** If you have something in mind, please give us a shout on our Discord channel. Features like new core abstractions, changes to infra or to dependencies will require careful consideration before we can move forward with them.

## How to contribute

1. **Fork the Repository**: Start by forking the repository and cloning it locally.
2. **Create a Branch**: Create a branch for your feature or bug fix. Use a descriptive name for your branch (e.g., `fix-typo`, `add-feature-x`).
3. **Install the dependencies** We use Poetry to manage dependencies. Run ``poetry install --all-extras``
4. **Make Your Changes**: Implement your changes in small, focused commits. Be sure to follow our linting rules and style guide.
5. **Run Tests**: If your changes affect functionality, please test thoroughly 🌡️ Details on how run tests are in the **Tests** section below.
6. **Lint Your Code**: We use [ruff](https://github.com/charliermarsh/ruff) for linting. Please ensure your code passes all linting checks. We prefer per-line disables for rules rather than global ignores, and please leave comments explaining why you disable any rules.
7. **Open a Pull Request**: Once you're happy with your changes, open a pull request. Ensure that your PR description clearly explains the changes and the problem it addresses. The **Release** section below has some useful tips on this process.
8. **Code Review**: Your PR will be reviewed by the maintainers. They may suggest improvements or request changes. We will do our best to review your PRs promptly but we're still a tiny team with limited resource. Please bear with us 🙏
10. **Merge Your PR**: Once approved, the author of the PR can merge the changes. 🚀

## Linting

We lint our code using [Ruff](https://github.com/astral-sh/ruff). We also have [pre-commit](https://pre-commit.com/) setup to allow running this easily locally.

## Tests

We write two types of tests:
- Unit tests should mock out the LLM providers, and aim to give quick feedback. They should mock out LLM providers.
- Integration tests actually call LLM providers, are much slower but test the system works fully.

To run tests:
- Run all tests with `poetry run pytest`.
- Run unit tests with `poetry run pytest tests/unit`.
- Run integration tests with `poetry run pytest tests/integration`.

We utilize [pytest-parallel](https://pypi.org/project/pytest-parallel/) to execute tests in parallel. You can add the `--workers=4` argument to the commands above to run in parallel. If you run into issues running this try setting `export NO_PROXY=true` first.

## Release

Releases are controlled via Github Actions and the version field of the `pyproject.toml`. To release:

1. Create a PR that updates the version field in the `pyproject.toml`.
2. Merge the PR to main.
3. Github Actions will create a new tag and push the new version to PyPi.

## Contributor License Agreement (CLA)

By submitting a pull request, you agree to sign our Contributor License Agreement (CLA), which ensures that contributions can be included in the project under the terms of our current [license](https://github.com/portiaAI/portia-sdk-python/edit/main/CONTRIBUTING.md#:~:text=CONTRIBUTING.md-,LICENSE,-Logo_Portia_Stacked_Black.png). We will ask you to sign this CLA when submitting your first contribution.

## Thank you

Thank you for contributing to Portia SDK Python!

```

## File: CODE_OF_CONDUCT.md

```markdown
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team directly on complaints@portialabs.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. 
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq

```

## File: pyproject.toml

```toml
[tool.poetry]
name = "portia-sdk-python"
version = "0.1.12"
description = "Portia Labs Python SDK for building agentic workflows."
authors = ["Hello <hello@portialabs.ai>"]
readme = "README.md"
repository = "https://github.com/portiaAI/portia-sdk-python"
homepage = "https://www.portialabs.ai/"
license = "MIT License"
packages = [
  {include = "portia"}
]
documentation = "https://docs.portialabs.ai"
keywords = ["LLM", "agentic", "workflow"]
classifiers = [
    "Development Status :: 3 - Alpha"
]

[tool.poetry.dependencies]
python = ">=3.11"
pydantic = "^2.10.3"
jinja2 = "^3.1.4"
instructor = {version = "^1.7.7", python = ">=3.11,<4.0"}
anthropic = ">=0.41.0"
langchain-anthropic = {version = "^0.3.0", python = ">=3.11,<4.0"}
langchain-core = {version = "^0.3.25", python = ">=3.11,<4.0"}
langchain-mistralai = {version = "^0.2.3", optional = true, python = ">=3.11,<4.0"}
langchain-openai = {version = "^0.3", python = ">=3.11,<4.0"}
mistralai = {version = "^1.2.5", optional = true}
langchain = {version = "^0.3.17", python = ">=3.11,<4.0"}
langgraph = {version = "^0.2.59", python = ">=3.11,<4.0"}
click = "^8.1.7"
loguru = {version = "^0.7.3", python = ">=3.11,<4.0"}
python-dotenv = "^1.0.1"
pandas = "^2.2.3"
pytest-mock = "^3.14.0"
openpyxl = "^3.1.5"
mcp = "^1.6.0"
langsmith = {version = "^0.3.15", python = ">=3.11,<4.0"}
google-generativeai = {version = "^0.8.4", optional = true}
langchain-google-genai = {version = "^2.0.10", optional = true, python = ">=3.11,<4.0"}
playwright = { version = "^1.49.0", optional = true }
browser-use = { version = "^0.1.40", optional = true, python = ">=3.11,<4.0" }
tiktoken = "^0.9.0"
jsonref = "^1.1.0"
browserbase = { version = "^1.2.0", optional = true }

[tool.poetry.group.dev.dependencies]
pre-commit = "^3.8.0"
ruff = "^0.8.0"
pytest = ">=8.3.3"
pytest-rerunfailures = "^14.0"
pytest-cov = "^5.0.0"
pyright = "^1.1.382"
pytest-httpx = "^0.33.0"
pytest-xdist = {extras = ["psutil"], version = "^3.6.1"}
pytest-asyncio = "^0.25.3"

[tool.poetry.extras]
mistral = ["langchain-mistralai", "mistralai"]
google = ["langchain-google-genai", "google-generativeai"]
all = ["langchain-mistralai", "mistralai", "langchain-google-genai", "google-generativeai"]

# Tools that require extra dependencies
tools-browser-local = ["playwright", "browser-use"]
tools-browser-browserbase = ["playwright", "browser-use", "browserbase"]

[tool.ruff]
line-length=100

[tool.ruff.lint]
select = ["ALL"]
ignore = [
  "D203",    # Disables checks for having a blank line before a class docstring. We instead have no-blank-line-before-class (D211) enabled.
  "D213",    # Disables checks for multi-line docstrings not starting on the first line. We instead have multi-line-summary-first-line (D212) enabled.
  "EM101",   # Disables checks for missing exception message arguments. We prefer single-line exception statements for simplicity and terseness.
  "EM102",   # Disables checks for f-string usage in exception messages. We prefer single-line exception statements with f-strings for simplicity and terseness.
  "TRY003",  # Disables checks for long error messages. We prefer to provide as much context to users as possible but want to avoid a proliferation of error classes.
]

[tool.ruff.lint.per-file-ignores]
"**/tests/*" = [
  "S101",    # Disables check for asserts. Asserts in test cases can be useful.
  "PLR2004", # Disables magic number checks. Its normal to assert using magic numbers for things like array length.
  "INP001",  # Disables checks for implicit namespace packages. Tests are not part of the package.
]

[tool.ruff.lint.flake8-type-checking]
runtime-evaluated-base-classes = [
  "pydantic.BaseModel", # Tells ruff that BaseModel instances need to be evaluated at runtime.
]

[tool.ruff.lint.flake8-annotations]
allow-star-arg-any = true  # Allows **kwargs: Any in type signatures.

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.setuptools.package-data]
portia = ["templates/**/*.jinja"]

[tool.pytest.ini_options]
filterwarnings = [
    "ignore:Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate':DeprecationWarning",  # this comes from LangChain
    "ignore::DeprecationWarning:langchain_core.load.serializable",  # Pydantic via LangChain
]
addopts = [
  "--cov",
  "--cov-report=term-missing",
  "--cov-report=html",
  "--import-mode=importlib"
]

[tool.coverage.run]
omit = [
    "*/tests/*", # Don't cover test files themselves
    "example.py", # Don't cover example
    "*/_unstable/**",  # Don't check _unstable files
]

[tool.coverage.report]
exclude_lines = [
    "if TYPE_CHECKING:",
    "pragma: no cover",
]

[[tool.pydoc-markdown.loaders]]
type = "python"

[[tool.pydoc-markdown.processors]]
type = "filter"
expression = "not 'test' in name and not 'cli' in name and not 'prefixed_uuid' in name and not 'common' in name and not 'templates' in name and not '_unstable' in name and default()"
skip_empty_modules = true

[[tool.pydoc-markdown.processors]]
type = "smart"

[[tool.pydoc-markdown.processors]]
type = "crossref"

[tool.pydoc-markdown.renderer]
type = "docusaurus"
relative_output_path="SDK"
sidebar_top_level_label=""

[tool.poetry.scripts]
portia-cli = "portia.cli:cli"

[tool.licensecheck]
using = "poetry"
ignore_packages = [
  "mistralai", # MistralAI is Apache 2.0 licensed: https://github.com/mistralai/client-python?tab=Apache-2.0-1-ov-file
] 

```

## File: .pre-commit-config.yaml

```yaml
default_stages: [pre-commit]
repos:
-   repo: https://github.com/astral-sh/ruff-pre-commit
    # Ruff version.
    rev: v0.8.0
    hooks:
      # Run the linter.
      - id: ruff
        args: ["--fix"]
    # Run the formatter.
      - id: ruff-format

-   repo: local
    hooks:
      - id: pyright
        name: Pyright
        entry: poetry run pyright
        language: system
        pass_filenames: false
        verbose: true  # Show output only on failure


```

## File: tests/conftest.py

```python
"""Configuration for pytest."""

import dotenv
import pytest


def pytest_sessionstart(session: pytest.Session) -> None:  # noqa: ARG001
    """Load environment variables from .env file for testing.

    NB This is a pytest hook that is called before test discovery runs,
    meaning module-level objects will be configured using the env vars
    in .env.
    """
    dotenv.load_dotenv(override=True)

```

## File: tests/utils.py

```python
"""Helpers to testing."""

from __future__ import annotations

from contextlib import asynccontextmanager
from typing import TYPE_CHECKING, Any, Callable, override
from unittest.mock import MagicMock

from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import BaseModel, Field, SecretStr

from portia.clarification import Clarification, InputClarification
from portia.clarification_handler import ClarificationHandler
from portia.config import Config, LogLevel, StorageClass
from portia.errors import ToolHardError, ToolSoftError
from portia.execution_agents.output import LocalOutput
from portia.execution_context import ExecutionContext, empty_context
from portia.model import LangChainGenerativeModel
from portia.plan import Plan, PlanContext, Step, Variable
from portia.plan_run import PlanRun, PlanRunUUID
from portia.tool import Tool, ToolRunContext
from portia.tool_call import ToolCallRecord, ToolCallStatus

if TYPE_CHECKING:
    from collections.abc import AsyncIterator, Sequence

    from langchain_core.messages import BaseMessage
    from langchain_core.tools import BaseTool
    from mcp import ClientSession

    from portia.execution_context import ExecutionContext
    from portia.mcp_session import McpClientConfig


def get_test_tool_context(
    plan_run_id: PlanRunUUID | None = None,
    config: Config | None = None,
) -> ToolRunContext:
    """Return a test tool context."""
    if not plan_run_id:
        plan_run_id = PlanRunUUID()
    if not config:
        config = get_test_config()
    return ToolRunContext(
        execution_context=get_execution_ctx(),
        plan_run_id=plan_run_id,
        config=config,
        clarifications=[],
    )


def get_test_plan_run() -> tuple[Plan, PlanRun]:
    """Generate a simple test plan_run."""
    step1 = Step(
        task="Add $a + 2",
        inputs=[
            Variable(name="$a", description="the first number"),
        ],
        output="$sum",
    )
    plan = Plan(
        plan_context=PlanContext(
            query="Add $a + 2",
            tool_ids=["add_tool"],
        ),
        steps=[step1],
    )
    plan_run = PlanRun(plan_id=plan.id, current_step_index=0)
    plan_run.outputs.step_outputs = {
        "$a": LocalOutput(value="3"),
    }
    return plan, plan_run


def get_test_tool_call(plan_run: PlanRun) -> ToolCallRecord:
    """Return a test tool call record."""
    return ToolCallRecord(
        tool_name="",
        plan_run_id=plan_run.id,
        step=1,
        end_user_id="1",
        additional_data={},
        output={},
        input={},
        latency_seconds=10,
        status=ToolCallStatus.SUCCESS,
    )


def get_test_config(**kwargs) -> Config:  # noqa: ANN003
    """Get test config."""
    return Config.from_default(
        **kwargs,
        default_log_level=LogLevel.INFO,
        openai_api_key=SecretStr("123"),
        storage_class=StorageClass.MEMORY,
    )


def get_execution_ctx(plan_run: PlanRun | None = None) -> ExecutionContext:
    """Return an execution context from a PlanRun."""
    if plan_run:
        return plan_run.execution_context
    return empty_context()


class AdditionToolSchema(BaseModel):
    """Input for AdditionTool."""

    a: int = Field(..., description="The first number to add")
    b: int = Field(..., description="The second number to add")


class AdditionTool(Tool):
    """Adds two numbers."""

    id: str = "add_tool"
    name: str = "Add Tool"
    description: str = "Use this tool to add two numbers together, it takes two numbers a + b"
    args_schema: type[BaseModel] = AdditionToolSchema
    output_schema: tuple[str, str] = ("int", "int: The value of the addition")

    def run(self, _: ToolRunContext, a: int, b: int) -> int:
        """Add the numbers."""
        return a + b


class ClarificationToolSchema(BaseModel):
    """Input for ClarificationTool."""

    user_guidance: str = Field(..., description="The user guidance for the clarification")


class ClarificationTool(Tool):
    """Returns a Clarification."""

    id: str = "clarification_tool"
    name: str = "Clarification Tool"
    description: str = "Returns a clarification"
    args_schema: type[BaseModel] = ClarificationToolSchema
    output_schema: tuple[str, str] = (
        "Clarification",
        "Clarification: The value of the Clarification",
    )

    def run(
        self,
        ctx: ToolRunContext,
        user_guidance: str,
    ) -> Clarification | None:
        """Add the numbers."""
        if len(ctx.clarifications) == 0:
            return InputClarification(
                plan_run_id=ctx.plan_run_id,
                user_guidance=user_guidance,
                argument_name="raise_clarification",
            )
        return None


class MockToolSchema(BaseModel):
    """Input for MockTool."""


class MockTool(Tool):
    """A mock tool class for testing purposes."""

    name: str = "Mock Tool"
    description: str = "do nothing"
    args_schema: type[BaseModel] = MockToolSchema
    output_schema: tuple[str, str] = ("None", "None: returns nothing")

    def run(
        self,
        _: ToolRunContext,
    ) -> None:
        """Do nothing."""
        return


class ErrorToolSchema(BaseModel):
    """Input for ErrorTool."""

    error_str: str
    return_soft_error: bool
    return_uncaught_error: bool


class ErrorTool(Tool):
    """Returns an Error."""

    id: str = "error_tool"
    name: str = "Error Tool"
    description: str = "Returns a error"
    args_schema: type[BaseModel] = ErrorToolSchema
    output_schema: tuple[str, str] = (
        "Error",
        "Error: The value of the error",
    )

    def run(
        self,
        _: ToolRunContext,
        error_str: str,
        return_uncaught_error: bool,  # noqa: FBT001
        return_soft_error: bool,  # noqa: FBT001
    ) -> None:
        """Return the error."""
        if return_uncaught_error:
            raise Exception(error_str)  # noqa: TRY002
        if return_soft_error:
            raise ToolSoftError(error_str)
        raise ToolHardError(error_str)


class NoneTool(Tool):
    """Returns None."""

    id: str = "none_tool"
    name: str = "None Tool"
    description: str = "returns None"
    output_schema: tuple[str, str] = ("None", "None: nothing")

    def run(self, _: ToolRunContext) -> None:
        """Return."""
        return


class TestClarificationHandler(ClarificationHandler):  # noqa: D101
    received_clarification: Clarification | None = None
    clarification_response: object = "Test"

    @override
    def handle_input_clarification(
        self,
        clarification: InputClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        self.received_clarification = clarification
        return on_resolution(clarification, self.clarification_response)

    def reset(self) -> None:
        """Reset the received clarification."""
        self.received_clarification = None


class MockMcpSessionWrapper:
    """Wrapper for mocking out an MCP ClientSession for testing MCP integration."""

    def __init__(self, session: MagicMock) -> None:
        """Initialize the wrapper."""
        self.session = session

    @asynccontextmanager
    async def mock_mcp_session(self, _: McpClientConfig) -> AsyncIterator[ClientSession]:
        """Mock method to swap out with the mcp_session context manager."""
        yield self.session


def get_mock_base_chat_model(
    response: Any = None,  # noqa: ANN401
) -> MagicMock:
    """Get a mock base chat model."""
    model = MagicMock(spec=BaseChatModel)

    def invoke(*_: Any, **__: Any) -> BaseMessage:
        """Mock invoke."""
        assert response is not None
        return response

    def with_structured_output(_: BaseModel, *__: Any, **___: Any) -> BaseChatModel:
        """Mock with structured output."""
        return model

    def bind_tools(_: Sequence[BaseTool], *__: Any, **___: Any) -> BaseChatModel:
        """Mock bind tools."""
        return model

    model.invoke.side_effect = invoke
    model.with_structured_output.side_effect = with_structured_output
    model.bind_tools.side_effect = bind_tools
    return model


def get_mock_langchain_generative_model(response: Any = None) -> LangChainGenerativeModel:  # noqa: ANN401
    """Get a mock langchain generative model."""
    return LangChainGenerativeModel(
        client=get_mock_base_chat_model(response),
        model_name="test",
    )

```

## File: tests/unit/test_plan.py

```python
"""Plan tests."""

import pytest
from pydantic import ValidationError

from portia.plan import Plan, PlanContext, PlanUUID, ReadOnlyPlan, Step, Variable
from tests.utils import get_test_plan_run


def test_plan_serialization() -> None:
    """Test plan can be serialized to string."""
    plan, _ = get_test_plan_run()
    assert str(plan) == (
        f"PlanModel(id={plan.id!r},plan_context={plan.plan_context!r}, steps={plan.steps!r}"
    )
    # check we can also serialize to JSON
    plan.model_dump_json()


def test_plan_uuid_assign() -> None:
    """Test plan assign correct UUIDs."""
    plan = Plan(
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[Step(task="test task", output="$output")],
    )
    assert isinstance(plan.id, PlanUUID)


def test_read_only_plan_immutable() -> None:
    """Test immutability of ReadOnlyPlan."""
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=[]),
        steps=[
            Step(task="test task", output="$output"),
        ],
    )
    read_only = ReadOnlyPlan.from_plan(plan)

    with pytest.raises(ValidationError):
        read_only.steps = []

    with pytest.raises(ValidationError):
        read_only.plan_context = PlanContext(query="new query", tool_ids=[])


def test_read_only_plan_preserves_data() -> None:
    """Test that ReadOnlyPlan preserves all data from original Plan."""
    original_plan = Plan(
        plan_context=PlanContext(
            query="What's the weather?",
            tool_ids=["weather_tool"],
        ),
        steps=[
            Step(task="Get weather", output="$weather"),
            Step(task="Format response", output="$response"),
        ],
    )

    read_only = ReadOnlyPlan.from_plan(original_plan)

    # Verify all data is preserved
    assert read_only.id == original_plan.id
    assert read_only.plan_context.query == original_plan.plan_context.query
    assert read_only.plan_context.tool_ids == original_plan.plan_context.tool_ids
    assert len(read_only.steps) == len(original_plan.steps)
    for ro_step, orig_step in zip(read_only.steps, original_plan.steps):
        assert ro_step.task == orig_step.task
        assert ro_step.output == orig_step.output


def test_read_only_plan_serialization() -> None:
    """Test that ReadOnlyPlan can be serialized and deserialized."""
    original_plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=["tool1"]),
        steps=[Step(task="test task", output="$output")],
    )
    read_only = ReadOnlyPlan.from_plan(original_plan)

    json_str = read_only.model_dump_json()

    deserialized = ReadOnlyPlan.model_validate_json(json_str)

    # Verify data is preserved through serialization
    assert deserialized.id == read_only.id
    assert deserialized.plan_context.query == read_only.plan_context.query
    assert deserialized.plan_context.tool_ids == read_only.plan_context.tool_ids
    assert len(deserialized.steps) == len(read_only.steps)
    assert deserialized.steps[0].task == read_only.steps[0].task
    assert deserialized.steps[0].output == read_only.steps[0].output


def test_plan_outputs_must_be_unique() -> None:
    """Test that plan outputs must be unique."""
    with pytest.raises(ValidationError, match="Outputs \\+ conditions must be unique"):
        Plan(
            plan_context=PlanContext(query="test query", tool_ids=["tool1"]),
            steps=[
                Step(task="test task", output="$output"),
                Step(task="test task", output="$output"),
            ],
        )


def test_plan_outputs_and_conditions_must_be_unique() -> None:
    """Test that plan outputs and conditions must be unique."""
    with pytest.raises(ValidationError, match="Outputs \\+ conditions must be unique"):
        Plan(
            plan_context=PlanContext(query="test query", tool_ids=["tool1"]),
            steps=[
                Step(task="test task", output="$output", condition="x > 10"),
                Step(task="test task", output="$output", condition="x > 10"),
            ],
        )
    # should not fail if conditions are different
    Plan(
        plan_context=PlanContext(query="test query", tool_ids=["tool1"]),
        steps=[
            Step(task="test task", output="$output", condition="x > 10"),
            Step(task="test task", output="$output", condition="x < 10"),
        ],
    )


def test_pretty_print() -> None:
    """Test pretty print."""
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=["tool1"]),
        steps=[
            Step(
                task="test task",
                output="$output",
                inputs=[Variable(name="$input", description="test input")],
                condition="x > 10",
            ),
        ],
    )
    output = plan.pretty_print()
    assert isinstance(output, str)

```

## File: tests/unit/test_clarifications.py

```python
"""Test simple agent."""

from __future__ import annotations

from typing import TYPE_CHECKING

import pytest
from pydantic import HttpUrl

from portia.clarification import (
    ActionClarification,
    ClarificationUUID,
    CustomClarification,
    MultipleChoiceClarification,
)
from portia.prefixed_uuid import PlanRunUUID
from portia.storage import DiskFileStorage
from tests.utils import get_test_plan_run

if TYPE_CHECKING:
    from pathlib import Path


def test_action_clarification_ser() -> None:
    """Test action clarifications can be serialized."""
    clarification = ActionClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        action_url=HttpUrl("https://example.com"),
    )
    clarification_model = clarification.model_dump()
    assert clarification_model["action_url"] == "https://example.com/"


def test_clarification_uuid_assign() -> None:
    """Test clarification assign correct UUIDs."""
    clarification = ActionClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        action_url=HttpUrl("https://example.com"),
    )
    assert isinstance(clarification.id, ClarificationUUID)


def test_value_multi_choice_validation() -> None:
    """Test clarifications error on invalid response."""
    with pytest.raises(ValueError):  # noqa: PT011
        MultipleChoiceClarification(
            plan_run_id=PlanRunUUID(),
            argument_name="test",
            user_guidance="test",
            options=["yes"],
            resolved=True,
            response="No",
        )

    MultipleChoiceClarification(
        plan_run_id=PlanRunUUID(),
        argument_name="test",
        user_guidance="test",
        options=["yes"],
        resolved=True,
        response="yes",
    )


def test_custom_clarification_deserialize(tmp_path: Path) -> None:
    """Test clarifications error on invalid response."""
    (plan, plan_run) = get_test_plan_run()

    clarification_one = CustomClarification(
        plan_run_id=plan_run.id,
        user_guidance="Please provide data",
        name="My Clarification",
        data={"email": {"test": "hello@example.com"}},
    )

    storage = DiskFileStorage(storage_dir=str(tmp_path))

    plan_run.outputs.clarifications = [clarification_one]

    storage.save_plan(plan)
    storage.save_plan_run(plan_run)
    retrieved = storage.get_plan_run(plan_run.id)
    assert isinstance(retrieved.outputs.clarifications[0], CustomClarification)
    assert retrieved.outputs.clarifications[0].data == {"email": {"test": "hello@example.com"}}

```

## File: tests/unit/test_tool.py

```python
"""Tests for the Tool class."""

import json
from enum import Enum
from unittest.mock import MagicMock, patch

import httpx
import mcp
import pytest
from mcp import ClientSession
from pydantic import BaseModel, HttpUrl

from portia.clarification import (
    ActionClarification,
    ClarificationUUID,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)
from portia.errors import InvalidToolDescriptionError, ToolHardError, ToolSoftError
from portia.mcp_session import StdioMcpClientConfig
from portia.tool import PortiaMcpTool, PortiaRemoteTool
from tests.utils import (
    AdditionTool,
    ClarificationTool,
    ErrorTool,
    MockMcpSessionWrapper,
    get_test_tool_context,
)


@pytest.fixture
def add_tool() -> AdditionTool:
    """Fixture to create a mock tool instance."""
    return AdditionTool()


@pytest.fixture
def clarification_tool() -> ClarificationTool:
    """Fixture to create a mock tool instance."""
    return ClarificationTool()


def test_tool_initialization(add_tool: AdditionTool) -> None:
    """Test initialization of a Tool."""
    assert add_tool.name == "Add Tool"
    assert (
        add_tool.description
        == "Use this tool to add two numbers together, it takes two numbers a + b"
    )


def test_tool_initialization_long_description() -> None:
    """Test initialization of a Tool."""

    class FakeAdditionTool(AdditionTool):
        description: str = "this is a description" * 100

    with pytest.raises(InvalidToolDescriptionError):
        FakeAdditionTool()


def test_tool_to_langchain() -> None:
    """Test langchain rep of a Tool."""
    tool = AdditionTool()
    tool.to_langchain(ctx=get_test_tool_context())


def test_run_method(add_tool: AdditionTool) -> None:
    """Test the run method of the AddTool."""
    a, b = 1, 2
    ctx = get_test_tool_context()
    result = add_tool.run(ctx, a, b)
    assert result == a + b


def test_handle(add_tool: AdditionTool) -> None:
    """Test the run method of the AddTool."""
    a, b = 1, 2
    ctx = get_test_tool_context()
    result = add_tool.run(ctx, a, b)
    assert result == a + b


def test_run_method_with_uncaught_error() -> None:
    """Test the _run method wraps errors."""
    tool = ErrorTool()
    with pytest.raises(ToolSoftError):
        tool._run(  # noqa: SLF001
            ctx=get_test_tool_context(),
            error_str="this is an error",
            return_uncaught_error=True,
            return_soft_error=False,
        )


def test_ready() -> None:
    """Test the ready method."""
    tool = ErrorTool()
    assert tool.ready(get_test_tool_context())


def test_tool_serialization() -> None:
    """Test tools can be serialized to string."""
    tool = AdditionTool()
    assert str(tool) == (
        f"ToolModel(id={tool.id!r}, name={tool.name!r}, "
        f"description={tool.description!r}, "
        f"args_schema={tool.args_schema.__name__!r}, "
        f"output_schema={tool.output_schema!r})"
    )
    # check we can also serialize to JSON
    AdditionTool().model_dump_json()


def test_remote_tool_hard_error_from_server() -> None:
    """Test http errors come back to hard errors."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.raise_for_status.side_effect = Exception()
    mock_response.json.return_value = {"output": {"value": "An error occurred."}}

    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )
    ctx = get_test_tool_context()
    with pytest.raises(ToolHardError):
        tool.run(ctx)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_soft_error() -> None:
    """Test remote soft errors come back to soft errors."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={"output": {"value": "ToolSoftError: An error occurred."}},
    )
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    with pytest.raises(ToolSoftError):
        tool.run(ctx)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }
    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_bad_response() -> None:
    """Test remote soft errors come back to soft errors."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json.return_value = {"ot": {"value": "An error occurred."}}
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    with pytest.raises(ToolHardError):
        tool.run(ctx)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_hard_error() -> None:
    """Test remote hard errors come back to hard errors."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json.return_value = {"output": {"value": "ToolHardError: An error occurred."}}
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    with pytest.raises(ToolHardError):
        tool.run(ctx)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }
    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_ready() -> None:
    """Test remote tool ready."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={"success": "true"},
    )
    mock_client.post.return_value = mock_response
    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )
    ctx = get_test_tool_context()
    assert tool.ready(ctx)

    content = {
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/ready/",
        content=json.dumps(content),
    )


def test_remote_tool_ready_error() -> None:
    """Test remote tool ready."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.raise_for_status.side_effect = Exception()
    mock_response.json = MagicMock(
        return_value={"success": "true"},
    )
    mock_client.post.return_value = mock_response
    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    assert not tool.ready(ctx)

    content = {
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/ready/",
        content=json.dumps(content),
    )


def test_remote_tool_action_clarifications() -> None:
    """Test action clarifications."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={
            "output": {
                "value": [
                    {
                        "id": str(ClarificationUUID()),
                        "category": "Action",
                        "action_url": "https://example.com",
                        "user_guidance": "blah",
                    },
                ],
            },
        },
    )
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )
    ctx = get_test_tool_context()
    output = tool.run(ctx)
    assert output is not None
    assert isinstance(output, ActionClarification)
    assert output.action_url == HttpUrl("https://example.com")

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_input_clarifications() -> None:
    """Test Input clarifications."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={
            "output": {
                "value": [
                    {
                        "id": str(ClarificationUUID()),
                        "category": "Input",
                        "user_guidance": "blah",
                        "argument_name": "t",
                    },
                ],
            },
        },
    )
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    output = tool.run(ctx)
    assert output is not None
    assert isinstance(output, InputClarification)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_mc_clarifications() -> None:
    """Test Multi Choice clarifications."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={
            "output": {
                "value": [
                    {
                        "id": str(ClarificationUUID()),
                        "category": "Multiple Choice",
                        "user_guidance": "blah",
                        "argument_name": "t",
                        "options": [1],
                    },
                ],
            },
        },
    )
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    output = tool.run(ctx)
    assert output is not None
    assert isinstance(output, MultipleChoiceClarification)
    assert output.options == [1]

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_remote_tool_value_confirm_clarifications() -> None:
    """Test value confirm clarifications."""
    mock_client = MagicMock(spec=httpx.Client)
    mock_response = MagicMock()
    mock_response.json = MagicMock(
        return_value={
            "output": {
                "value": [
                    {
                        "id": str(ClarificationUUID()),
                        "category": "Value Confirmation",
                        "user_guidance": "blah",
                        "argument_name": "t",
                    },
                ],
            },
        },
    )
    mock_client.post.return_value = mock_response

    tool = PortiaRemoteTool(
        id="test",
        name="test",
        description="",
        output_schema=("", ""),
        client=mock_client,
    )

    ctx = get_test_tool_context()
    output = tool.run(ctx)
    assert output is not None
    assert isinstance(output, ValueConfirmationClarification)

    content = {
        "arguments": {},
        "execution_context": {
            "end_user_id": ctx.execution_context.end_user_id or "",
            "plan_run_id": str(ctx.plan_run_id),
            "additional_data": ctx.execution_context.additional_data or {},
        },
    }

    mock_client.post.assert_called_once_with(
        url="/api/v0/tools/test/run/",
        content=json.dumps(content),
    )


def test_portia_mcp_tool_call() -> None:
    """Test invoking a tool via MCP."""
    mock_session = MagicMock(spec=ClientSession)
    mock_session.call_tool.return_value = mcp.types.CallToolResult(
        content=[mcp.types.TextContent(type="text", text="Hello, world!")],
        isError=False,
    )

    class MyEnum(str, Enum):
        A = "A"

    class TestArgSchema(BaseModel):
        a: MyEnum
        b: int

    tool = PortiaMcpTool(
        id="mcp:mock_mcp:test_tool",
        name="test_tool",
        description="I am a tool",
        output_schema=("str", "Tool output formatted as a JSON string"),
        args_schema=TestArgSchema,
        mcp_client_config=StdioMcpClientConfig(
            server_name="mock_mcp",
            command="test",
            args=["test"],
        ),
    )
    expected = (
        '{"meta":null,"content":[{"type":"text","text":"Hello, world!","annotations":null}],'
        '"isError":false}'
    )

    with patch(
        "portia.tool.get_mcp_session",
        new=MockMcpSessionWrapper(mock_session).mock_mcp_session,
    ):
        tool_result = tool.run(get_test_tool_context(), a=1, b=2)
        assert tool_result == expected


def test_portia_mcp_tool_call_with_error() -> None:
    """Test invoking a tool via MCP."""
    mock_session = MagicMock(spec=ClientSession)
    mock_session.call_tool.return_value = mcp.types.CallToolResult(
        content=[],
        isError=True,
    )

    class TestArgSchema(BaseModel):
        a: int
        b: int

    tool = PortiaMcpTool(
        id="mcp:mock_mcp:test_tool",
        name="test_tool",
        description="I am a tool",
        output_schema=("str", "Tool output formatted as a JSON string"),
        args_schema=TestArgSchema,
        mcp_client_config=StdioMcpClientConfig(
            server_name="mock_mcp",
            command="test",
            args=["test"],
        ),
    )

    with (
        patch(
            "portia.tool.get_mcp_session",
            new=MockMcpSessionWrapper(mock_session).mock_mcp_session,
        ),
        pytest.raises(ToolHardError),
    ):
        tool.run(get_test_tool_context(), a=1, b=2)

```

## File: tests/unit/test_tool_wrapper.py

```python
"""Tests for the ToolCallWrapper class."""

import pytest

from portia.clarification import Clarification
from portia.errors import ToolHardError
from portia.execution_agents.output import LocalOutput
from portia.storage import AdditionalStorage, ToolCallRecord, ToolCallStatus
from portia.tool import Tool
from portia.tool_wrapper import ToolCallWrapper
from tests.utils import (
    AdditionTool,
    ClarificationTool,
    ErrorTool,
    NoneTool,
    get_test_plan_run,
    get_test_tool_context,
)


class MockStorage(AdditionalStorage):
    """Mock implementation of AdditionalStorage for testing."""

    def __init__(self) -> None:
        """Save records in array."""
        self.records = []

    def save_tool_call(self, tool_call: ToolCallRecord) -> None:
        """Save records in array."""
        self.records.append(tool_call)


@pytest.fixture
def mock_tool() -> Tool:
    """Fixture to create a mock tool instance."""
    return AdditionTool()


@pytest.fixture
def mock_storage() -> MockStorage:
    """Fixture to create a mock storage instance."""
    return MockStorage()


def test_tool_call_wrapper_initialization(mock_tool: Tool, mock_storage: MockStorage) -> None:
    """Test initialization of the ToolCallWrapper."""
    (_, plan_run) = get_test_plan_run()
    wrapper = ToolCallWrapper(child_tool=mock_tool, storage=mock_storage, plan_run=plan_run)
    assert wrapper.name == mock_tool.name
    assert wrapper.description == mock_tool.description


def test_tool_call_wrapper_run_success(mock_tool: Tool, mock_storage: MockStorage) -> None:
    """Test successful run of the ToolCallWrapper."""
    (_, plan_run) = get_test_plan_run()
    wrapper = ToolCallWrapper(mock_tool, mock_storage, plan_run)
    ctx = get_test_tool_context()
    result = wrapper.run(ctx, 1, 2)
    assert result == 3
    assert mock_storage.records[-1].status == ToolCallStatus.SUCCESS


def test_tool_call_wrapper_run_with_exception(
    mock_storage: MockStorage,
) -> None:
    """Test run of the ToolCallWrapper when the child tool raises an exception."""
    tool = ErrorTool()
    (_, plan_run) = get_test_plan_run()
    wrapper = ToolCallWrapper(tool, mock_storage, plan_run)
    ctx = get_test_tool_context()
    with pytest.raises(ToolHardError, match="Test error"):
        wrapper.run(ctx, "Test error", False, False)  # noqa: FBT003
    assert mock_storage.records[-1].status == ToolCallStatus.FAILED


def test_tool_call_wrapper_run_with_clarification(
    mock_storage: MockStorage,
) -> None:
    """Test run of the ToolCallWrapper when the child tool returns a Clarification."""
    (_, plan_run) = get_test_plan_run()
    tool = ClarificationTool()
    wrapper = ToolCallWrapper(tool, mock_storage, plan_run)
    ctx = get_test_tool_context()
    result = wrapper.run(ctx, "new clarification")
    assert isinstance(result, Clarification)
    assert mock_storage.records[-1].status == ToolCallStatus.NEED_CLARIFICATION


def test_tool_call_wrapper_run_records_latency(mock_tool: Tool, mock_storage: MockStorage) -> None:
    """Test that the ToolCallWrapper records latency correctly."""
    (_, plan_run) = get_test_plan_run()
    wrapper = ToolCallWrapper(mock_tool, mock_storage, plan_run)
    ctx = get_test_tool_context()
    wrapper.run(ctx, 1, 2)
    assert mock_storage.records[-1].latency_seconds > 0


def test_tool_call_wrapper_run_returns_none(mock_storage: MockStorage) -> None:
    """Test that the ToolCallWrapper records latency correctly."""
    (_, plan_run) = get_test_plan_run()
    wrapper = ToolCallWrapper(NoneTool(), mock_storage, plan_run)
    ctx = get_test_tool_context()
    wrapper.run(ctx)
    assert mock_storage.records[-1].output
    assert mock_storage.records[-1].output == LocalOutput(value=None).model_dump(mode="json")

```

## File: tests/unit/test_logging.py

```python
"""Tests for logging functions."""

from unittest.mock import Mock

import pytest

from portia.config import LogLevel
from portia.logger import (
    FUNCTION_COLOR_MAP,
    Formatter,
    LoggerInterface,
    LoggerManager,
    logger,
    logger_manager,
)


@pytest.mark.parametrize(
    ("record", "expected_color"),
    [
        (
            {"name": "portia.portia", "function": "_execute_plan_run"},
            FUNCTION_COLOR_MAP["run"],
        ),
        (
            {"name": "portia.portia", "function": "_handle_introspection_outcome"},
            FUNCTION_COLOR_MAP["introspection"],
        ),
        (
            {"name": "portia.storage", "function": "save_tool_call"},
            FUNCTION_COLOR_MAP["tool"],
        ),
        (
            {"name": "portia.portia", "function": "plan"},
            FUNCTION_COLOR_MAP["plan"],
        ),
        (
            {"name": "portia.portia", "function": "_raise_clarifications"},
            FUNCTION_COLOR_MAP["clarification"],
        ),
        (
            {"name": "portia.tool_wrapper", "function": "run"},
            FUNCTION_COLOR_MAP["tool"],
        ),
    ],
)
def test_logger_formatter_get_function_color(record: dict, expected_color: str) -> None:
    """Test the logger formatter get_function_color method."""
    logger_formatter = Formatter()
    assert logger_formatter._get_function_color_(record) == expected_color


def test_logger_sanitize_message() -> None:
    """Test the logger sanitize_message method."""
    logger_formatter = Formatter()
    assert logger_formatter._sanitize_message_("<test>") == r"\<test\>"
    assert logger_formatter._sanitize_message_("{test} {{test}}") == "{{test}} {{test}}"
    assert logger_formatter._sanitize_message_('{"test": "<test>"}') == '{{"test": "\\<test\\>"}}'

    # a long message gets truncated correctly
    long_message = "test\n" * 100
    truncated_message = logger_formatter._sanitize_message_(long_message)
    assert len(truncated_message.split("\n")) == logger_formatter.max_lines
    assert truncated_message.endswith("test\n")
    assert truncated_message.startswith("test\n")


def test_logger_manager_initialization() -> None:
    """Test initialization of LoggerManager with default logger."""
    logger_manager = LoggerManager()
    assert logger_manager.custom_logger is False


def test_logger_manager_with_custom_logger() -> None:
    """Test initialization of LoggerManager with a custom logger."""
    mock_logger = Mock(spec=LoggerInterface)
    logger_manager = LoggerManager(custom_logger=mock_logger)

    assert logger_manager.logger == mock_logger
    assert logger_manager.custom_logger is False


def test_set_logger() -> None:
    """Test setting a custom logger."""
    logger_manager = LoggerManager()
    mock_logger = Mock(spec=LoggerInterface)

    logger_manager.set_logger(mock_logger)
    assert logger_manager.logger == mock_logger
    assert logger_manager.custom_logger is True


def test_configure_from_config() -> None:
    """Test configuring the logger from a Config instance."""
    logger_manager = LoggerManager()
    mock_config = Mock(
        default_log_sink="sys.stdout",
        default_log_level=LogLevel.DEBUG,
        json_log_serialize=False,
    )

    logger_manager.configure_from_config(mock_config)

    # Verify log level and sink configuration
    assert mock_config.default_log_level == LogLevel.DEBUG
    assert mock_config.default_log_sink == "sys.stdout"


def test_configure_from_config_stderr() -> None:
    """Test configuring the logger from a Config instance."""
    logger_manager = LoggerManager()
    mock_config = Mock(
        default_log_sink="sys.stderr",
        default_log_level=LogLevel.INFO,
        json_log_serialize=False,
    )

    logger_manager.configure_from_config(mock_config)

    # Verify log level and sink configuration
    assert mock_config.default_log_level == LogLevel.INFO
    assert mock_config.default_log_sink == "sys.stderr"


def test_configure_from_config_custom_logger() -> None:
    """Test warning when configuring logger with a custom logger set."""
    mock_logger = Mock(spec=LoggerInterface)
    logger_manager = LoggerManager(custom_logger=mock_logger)
    logger_manager.set_logger(mock_logger)

    mock_config = Mock(
        default_log_sink="sys.stderr",
        default_log_level="INFO",
        json_log_serialize=True,
    )

    logger_manager.configure_from_config(mock_config)
    mock_logger.warning.assert_called_once_with(
        "Custom logger is in use; skipping log level configuration.",
    )


def test_logger() -> None:
    """Test the LoggerProxy provides access to the current logger."""
    mock_logger = Mock(spec=LoggerInterface)
    logger_manager.set_logger(mock_logger)

    assert logger() == mock_logger

```

## File: tests/unit/test_llm_wrapper.py

```python
"""Unit tests for the LLMWrapper."""

from unittest.mock import MagicMock, patch

import pytest
from langchain_core.language_models.chat_models import BaseChatModel
from pydantic import SecretStr

from portia.config import DEFAULT_MODEL_KEY, Config, LLMModel
from portia.llm_wrapper import LLMWrapper
from portia.model import (
    GenerativeModel,
    LangChainGenerativeModel,
)
from tests.utils import MockToolSchema, get_mock_base_chat_model


def test_llm_wrapper() -> None:
    """Test the LLMWrapper."""
    model = LangChainGenerativeModel(
        client=get_mock_base_chat_model(response=MockToolSchema()),
        model_name="test",
    )

    config = Config(
        custom_models={
            DEFAULT_MODEL_KEY: model,
        },
        openai_api_key=SecretStr("123"),
    )
    wrapper = LLMWrapper.for_usage(config=config, usage=DEFAULT_MODEL_KEY)
    wrapper.to_langchain()
    wrapper.to_instructor(MockToolSchema, [])


def test_llm_wrapper_langchain_not_supported() -> None:
    """Test the LLMWrapper."""
    model = MagicMock(spec=GenerativeModel, create=True)
    wrapper = LLMWrapper(model=model)
    with pytest.raises(
        ValueError,
        match="LangChain is not supported for this model type",
    ):
        wrapper.to_langchain()


def test_llm_wrapper_no_model_name() -> None:
    """Test the LLMWrapper."""
    with pytest.raises(
        ValueError,
        match="model_name and api_key must be provided if model is not provided",
    ):
        LLMWrapper(model=None)


@pytest.mark.parametrize(
    "model_name",
    [
        LLMModel.GPT_4_O,
        LLMModel.CLAUDE_3_5_SONNET,
        LLMModel.MISTRAL_LARGE,
        LLMModel.GEMINI_2_0_FLASH,
        LLMModel.AZURE_GPT_4_O,
    ],
)
def test_llm_wrapper_providers(
    model_name: LLMModel,
) -> None:
    """Test LLMWrapper with different providers."""
    wrapper = LLMWrapper(
        model_name=model_name,
        api_key=SecretStr("test-key"),
        api_endpoint="https://test.example.com",
    )
    assert isinstance(wrapper.to_langchain(), BaseChatModel)

    with patch.object(wrapper.model, "get_structured_response", return_value=MockToolSchema()):
        response = wrapper.to_instructor(MockToolSchema, [])
        assert response == MockToolSchema()

```

## File: tests/unit/test_plan_storage.py

```python
"""Tests for the Storage classes."""

from pathlib import Path

import pytest

from portia.plan import Plan, PlanContext
from portia.plan_run import PlanRun, PlanRunState
from portia.storage import (
    DiskFileStorage,
    InMemoryStorage,
    PlanNotFoundError,
    PlanRunNotFoundError,
    PlanRunUUID,
    PlanUUID,
)


def test_in_memory_storage_save_and_get_plan() -> None:
    """Test saving and retrieving a Plan in InMemoryStorage."""
    storage = InMemoryStorage()
    plan = Plan(plan_context=PlanContext(query="query", tool_ids=[]), steps=[])
    storage.save_plan(plan)
    retrieved_plan = storage.get_plan(plan.id)

    assert retrieved_plan.id == plan.id

    with pytest.raises(PlanNotFoundError):
        storage.get_plan(PlanUUID())


def test_in_memory_storage_save_and_get_plan_run() -> None:
    """Test saving and retrieving PlanRun in InMemoryStorage."""
    storage = InMemoryStorage()
    plan = Plan(plan_context=PlanContext(query="query", tool_ids=[]), steps=[])
    plan_run = PlanRun(plan_id=plan.id)
    storage.save_plan_run(plan_run)
    retrieved_plan_run = storage.get_plan_run(plan_run.id)

    assert retrieved_plan_run.id == plan_run.id

    with pytest.raises(PlanRunNotFoundError):
        storage.get_plan_run(PlanRunUUID())


def test_disk_file_storage_save_and_get_plan(tmp_path: Path) -> None:
    """Test saving and retrieving a Plan in DiskFileStorage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    plan = Plan(plan_context=PlanContext(query="query", tool_ids=[]), steps=[])
    storage.save_plan(plan)
    retrieved_plan = storage.get_plan(plan.id)

    assert retrieved_plan.id == plan.id

    with pytest.raises(PlanNotFoundError):
        storage.get_plan(PlanUUID())


def test_disk_file_storage_save_and_get_plan_run(tmp_path: Path) -> None:
    """Test saving and retrieving PlanRun in DiskFileStorage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    plan = Plan(
        plan_context=PlanContext(query="query", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(plan_id=plan.id)
    storage.save_plan_run(plan_run)
    retrieved_plan_run = storage.get_plan_run(plan_run.id)

    assert retrieved_plan_run.id == plan_run.id

    with pytest.raises(PlanRunNotFoundError):
        storage.get_plan_run(PlanRunUUID())


def test_disk_file_storage_save_and_get_plan_runs(tmp_path: Path) -> None:
    """Test saving and retrieving PlanRun in DiskFileStorage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    plan = Plan(
        plan_context=PlanContext(query="query", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(plan_id=plan.id, state=PlanRunState.IN_PROGRESS)
    storage.save_plan_run(plan_run)
    plan_run = PlanRun(plan_id=plan.id, state=PlanRunState.FAILED)
    storage.save_plan_run(plan_run)

    runs = storage.get_plan_runs(PlanRunState.IN_PROGRESS)
    assert len(runs.results) == 1


def test_disk_file_storage_invalid_plan_retrieval(tmp_path: Path) -> None:
    """Test handling of invalid Plan data in DiskFileStorage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    invalid_file = tmp_path / "plan-invalid.json"
    invalid_file.write_text('{"id": "not-a-valid-uuid"}')  # Write invalid JSON

    with pytest.raises(PlanNotFoundError):
        storage.get_plan(PlanUUID())


def test_disk_file_storage_invalid_run_retrieval(tmp_path: Path) -> None:
    """Test handling of invalid Run data in DiskFileStorage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    invalid_file = tmp_path / "run-invalid.json"
    invalid_file.write_text('{"id": "not-a-valid-uuid"}')  # Write invalid JSON

    with pytest.raises(PlanRunNotFoundError):
        storage.get_plan_run(PlanRunUUID())

```

## File: tests/unit/test_clarification_handler.py

```python
"""Test clarification handler."""

from unittest.mock import MagicMock

import pytest
from pydantic import HttpUrl

from portia.clarification import (
    ActionClarification,
    Clarification,
    ClarificationCategory,
    CustomClarification,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)
from portia.clarification_handler import ClarificationHandler
from portia.prefixed_uuid import PlanRunUUID


class TestClarificationHandler(ClarificationHandler):
    """Handles clarifications using mocks in this test."""


def test_action_clarification() -> None:
    """Test that ActionClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    on_resolution = MagicMock()
    on_error = MagicMock()
    clarification = ActionClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        action_url=HttpUrl("https://example.com"),
    )

    # Test without implementation
    with pytest.raises(NotImplementedError):
        handler.handle(clarification, on_resolution, on_error)

    # Test with implementation
    handler.handle_action_clarification = MagicMock()
    handler.handle(clarification, on_resolution, on_error)
    handler.handle_action_clarification.assert_called_once_with(
        clarification,
        on_resolution,
        on_error,
    )


def test_input_clarification() -> None:
    """Test that InputClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    on_resolution = MagicMock()
    on_error = MagicMock()
    clarification = InputClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        argument_name="test",
    )

    # Test without implementation
    with pytest.raises(NotImplementedError):
        handler.handle(clarification, on_resolution, on_error)

    # Test with implementation
    handler.handle_input_clarification = MagicMock()
    handler.handle(clarification, on_resolution, on_error)
    handler.handle_input_clarification.assert_called_once_with(
        clarification,
        on_resolution,
        on_error,
    )


def test_multiple_choice_clarification() -> None:
    """Test that MultipleChoiceClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    on_resolution = MagicMock()
    on_error = MagicMock()
    clarification = MultipleChoiceClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        argument_name="test",
        options=["option1", "option2"],
    )

    # Test without implementation
    with pytest.raises(NotImplementedError):
        handler.handle(clarification, on_resolution, on_error)

    # Test with implementation
    handler.handle_multiple_choice_clarification = MagicMock()
    handler.handle(clarification, on_resolution, on_error)
    handler.handle_multiple_choice_clarification.assert_called_once_with(
        clarification,
        on_resolution,
        on_error,
    )


def test_value_confirmation_clarification() -> None:
    """Test that ValueConfirmationClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    on_resolution = MagicMock()
    on_error = MagicMock()
    clarification = ValueConfirmationClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        argument_name="test",
    )

    # Test without implementation
    with pytest.raises(NotImplementedError):
        handler.handle(clarification, on_resolution, on_error)

    # Test with implementation
    handler.handle_value_confirmation_clarification = MagicMock()
    handler.handle(clarification, on_resolution, on_error)
    handler.handle_value_confirmation_clarification.assert_called_once_with(
        clarification,
        on_resolution,
        on_error,
    )


def test_custom_clarification_routing() -> None:
    """Test that CustomClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    on_resolution = MagicMock()
    on_error = MagicMock()
    clarification = CustomClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        name="test",
        data={"key": "value"},
    )

    # Test without implementation
    with pytest.raises(NotImplementedError):
        handler.handle(clarification, on_resolution, on_error)

    # Test with implementation
    handler.handle_custom_clarification = MagicMock()
    handler.handle(clarification, on_resolution, on_error)
    handler.handle_custom_clarification.assert_called_once_with(
        clarification,
        on_resolution,
        on_error,
    )


def test_invalid_clarification() -> None:
    """Test that CustomClarification is routed to the correct handler method."""
    handler = TestClarificationHandler()

    class UnhandledClarification(Clarification):
        pass

    clarification = UnhandledClarification(
        category=ClarificationCategory.CUSTOM,
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
    )

    on_resolution = MagicMock()
    on_error = MagicMock()
    with pytest.raises(ValueError):  # noqa: PT011
        handler.handle(clarification, on_resolution, on_error)

```

## File: tests/unit/test_plan_run.py

```python
"""Tests for Run primitives."""

from uuid import UUID, uuid4

import pytest
from pydantic import ValidationError

from portia.clarification import Clarification, InputClarification
from portia.errors import ToolHardError, ToolSoftError
from portia.execution_agents.output import LocalOutput
from portia.plan import PlanUUID, ReadOnlyStep, Step
from portia.plan_run import PlanRun, PlanRunOutputs, PlanRunState, ReadOnlyPlanRun
from portia.prefixed_uuid import PlanRunUUID


@pytest.fixture
def mock_clarification() -> InputClarification:
    """Create a mock clarification for testing."""
    return InputClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="test",
        resolved=False,
        argument_name="test",
    )


@pytest.fixture
def plan_run(mock_clarification: InputClarification) -> PlanRun:
    """Create PlanRun instance for testing."""
    return PlanRun(
        plan_id=PlanUUID(),
        current_step_index=1,
        state=PlanRunState.IN_PROGRESS,
        outputs=PlanRunOutputs(
            clarifications=[mock_clarification],
            step_outputs={"step1": LocalOutput(value="Test output")},
        ),
    )


def test_run_initialization() -> None:
    """Test initialization of PlanRun instance."""
    plan_id = PlanUUID()
    plan_run = PlanRun(plan_id=plan_id)

    assert plan_run.id is not None
    assert plan_run.plan_id == plan_id
    assert isinstance(plan_run.plan_id.uuid, UUID)
    assert plan_run.current_step_index == 0
    assert plan_run.outputs.clarifications == []
    assert plan_run.state == PlanRunState.NOT_STARTED
    assert plan_run.outputs.step_outputs == {}


def test_run_get_outstanding_clarifications(
    plan_run: PlanRun,
    mock_clarification: Clarification,
) -> None:
    """Test get_outstanding_clarifications method."""
    outstanding = plan_run.get_outstanding_clarifications()

    assert len(outstanding) == 1
    assert outstanding[0] == mock_clarification


def test_run_get_outstanding_clarifications_none() -> None:
    """Test get_outstanding_clarifications when no clarifications are outstanding."""
    plan_run = PlanRun(plan_id=PlanUUID(), outputs=PlanRunOutputs(clarifications=[]))

    assert plan_run.get_outstanding_clarifications() == []


def test_run_state_enum() -> None:
    """Test the RunState enum values."""
    assert PlanRunState.NOT_STARTED == "NOT_STARTED"
    assert PlanRunState.IN_PROGRESS == "IN_PROGRESS"
    assert PlanRunState.COMPLETE == "COMPLETE"
    assert PlanRunState.NEED_CLARIFICATION == "NEED_CLARIFICATION"
    assert PlanRunState.FAILED == "FAILED"


def test_read_only_run_immutable() -> None:
    """Test immutability of plan_run."""
    plan_run = PlanRun(plan_id=PlanUUID(uuid=uuid4()))
    read_only = ReadOnlyPlanRun.from_plan_run(plan_run)

    with pytest.raises(ValidationError):
        read_only.state = PlanRunState.IN_PROGRESS


def test_read_only_step_immutable() -> None:
    """Test immutability of step."""
    step = Step(task="add", output="$out")
    read_only = ReadOnlyStep.from_step(step)

    with pytest.raises(ValidationError):
        read_only.output = "$in"


def test_run_serialization() -> None:
    """Test run can be serialized to string."""
    plan_run_id = PlanRunUUID()
    plan_run = PlanRun(
        id=plan_run_id,
        plan_id=PlanUUID(),
        outputs=PlanRunOutputs(
            clarifications=[
                InputClarification(
                    plan_run_id=plan_run_id,
                    step=0,
                    argument_name="test",
                    user_guidance="help",
                    response="yes",
                ),
            ],
            step_outputs={
                "1": LocalOutput(value=ToolHardError("this is a tool hard error")),
                "2": LocalOutput(value=ToolSoftError("this is a tool soft error")),
            },
            final_output=LocalOutput(value="This is the end"),
        ),
    )
    assert str(plan_run) == (
        f"Run(id={plan_run.id}, plan_id={plan_run.plan_id}, "
        f"state={plan_run.state}, current_step_index={plan_run.current_step_index}, "
        f"final_output={'set' if plan_run.outputs.final_output else 'unset'})"
    )

    # check we can also serialize to JSON
    json_str = plan_run.model_dump_json()
    # parse back to run
    parsed_plan_run = PlanRun.model_validate_json(json_str)
    # ensure clarification types are maintained
    assert isinstance(parsed_plan_run.outputs.clarifications[0], InputClarification)

```

## File: tests/unit/test_common.py

```python
"""Tests for common classes."""

import json
from unittest import mock
from uuid import UUID

import pytest
from pydantic import BaseModel, Field

from portia.common import (
    EXTRAS_GROUPS_DEPENDENCIES,
    PortiaEnum,
    combine_args_kwargs,
    validate_extras_dependencies,
)
from portia.prefixed_uuid import PrefixedUUID


def test_portia_enum() -> None:
    """Test PortiaEnums can enumerate."""

    class MyEnum(PortiaEnum):
        OK = "OK"

    assert MyEnum.enumerate() == (("OK", "OK"),)


def test_combine_args_kwargs() -> None:
    """Test combining args and kwargs into a single dictionary."""
    result = combine_args_kwargs(1, 2, three=3, four=4)
    assert result == {"0": 1, "1": 2, "three": 3, "four": 4}


class TestPrefixedUUID:
    """Tests for PrefixedUUID."""

    def test_default_prefix(self) -> None:
        """Test PrefixedUUID with default empty prefix."""
        prefixed_uuid = PrefixedUUID()
        assert prefixed_uuid.prefix == ""
        assert isinstance(prefixed_uuid.uuid, UUID)
        assert str(prefixed_uuid) == str(prefixed_uuid.uuid)

    def test_custom_prefix(self) -> None:
        """Test PrefixedUUID with custom prefix."""

        class CustomPrefixUUID(PrefixedUUID):
            prefix = "test"

        prefixed_uuid = CustomPrefixUUID()
        assert prefixed_uuid.prefix == "test"
        assert str(prefixed_uuid).startswith("test-")
        assert str(prefixed_uuid) == f"test-{prefixed_uuid.uuid}"
        assert isinstance(prefixed_uuid.uuid, UUID)
        assert str(prefixed_uuid)[5:] == str(prefixed_uuid.uuid)

    def test_from_string(self) -> None:
        """Test creating PrefixedUUID from string."""
        # Test with default prefix
        uuid_str = "123e4567-e89b-12d3-a456-426614174000"
        prefixed_uuid = PrefixedUUID.from_string(uuid_str)
        assert str(prefixed_uuid) == uuid_str

        # Test with custom prefix
        class CustomPrefixUUID(PrefixedUUID):
            prefix = "test"

        prefixed_str = f"test-{uuid_str}"
        prefixed_uuid = CustomPrefixUUID.from_string(prefixed_str)
        assert str(prefixed_uuid) == prefixed_str
        assert str(prefixed_uuid)[5:] == str(prefixed_uuid.uuid)

        with pytest.raises(ValueError, match="Prefix monkey does not match expected prefix test"):
            CustomPrefixUUID.from_string("monkey-123e4567-e89b-12d3-a456-426614174000")

    def test_serialization(self) -> None:
        """Test PrefixedUUID serialization."""
        uuid = PrefixedUUID()
        assert str(uuid) == uuid.model_dump_json().strip('"')

    def test_model_validation(self) -> None:
        """Test JSON validation and deserialization."""

        class CustomID(PrefixedUUID):
            prefix = "test"

        class TestModel(BaseModel):
            id: CustomID = Field(default_factory=CustomID)

        uuid_str = "123e4567-e89b-12d3-a456-426614174000"

        # Test with string ID
        json_data = f'{{"id": "test-{uuid_str}"}}'
        model = TestModel.model_validate_json(json_data)
        assert isinstance(model.id, CustomID)
        assert str(model.id.uuid) == uuid_str
        assert isinstance(model.id.uuid, UUID)
        assert model.id.prefix == "test"

        # Test with full representation of ID
        json_data = json.dumps(
            {
                "id": {
                    "uuid": uuid_str,
                },
            },
        )
        model = TestModel.model_validate_json(json_data)
        assert isinstance(model.id, CustomID)
        assert str(model.id.uuid) == uuid_str
        assert isinstance(model.id.uuid, UUID)
        assert model.id.prefix == "test"

        json_data = f'{{"id": "monkey-{uuid_str}"}}'
        with pytest.raises(ValueError, match="Prefix monkey does not match expected prefix test"):
            TestModel.model_validate_json(json_data)

        class TestModelNoPrefix(BaseModel):
            id: PrefixedUUID

        json_data = f'{{"id": "{uuid_str}"}}'
        model = TestModelNoPrefix.model_validate_json(json_data)
        assert isinstance(model.id, PrefixedUUID)
        assert str(model.id.uuid) == uuid_str
        assert isinstance(model.id.uuid, UUID)
        assert model.id.prefix == ""

    def test_hash(self) -> None:
        """Test PrefixedUUID hash."""
        uuid = PrefixedUUID()
        assert hash(uuid) == hash(uuid.uuid)


def test_validate_extras_dependencies() -> None:
    """Test function raises correct error when non-existing top level package is installed."""
    with mock.patch.dict(EXTRAS_GROUPS_DEPENDENCIES, {"fake-extras-package": ["fake_package.bar"]}):
        with pytest.raises(ImportError) as e:
            validate_extras_dependencies("fake-extras-package")
        assert "portia-sdk-python[fake-extras-package]" in str(e.value)


def test_validate_extras_dependencies_raise_error_false() -> None:
    """Test function doesn't raise an error when raise_error is False."""
    with mock.patch.dict(EXTRAS_GROUPS_DEPENDENCIES, {"test": ["foobarbaz"]}):
        extras_installed = validate_extras_dependencies("test", raise_error=False)
        assert extras_installed is False


def test_validate_extras_dependencies_success() -> None:
    """Test function succeeds when package is installed."""
    with mock.patch.dict(EXTRAS_GROUPS_DEPENDENCIES, {"test": ["pytest"]}):
        validate_extras_dependencies("test")

```

## File: tests/unit/test_config.py

```python
"""Tests for portia classes."""

from unittest.mock import MagicMock, Mock

import pytest
from pydantic import SecretStr

from portia.config import (
    EXECUTION_MODEL_KEY,
    FEATURE_FLAG_AGENT_MEMORY_ENABLED,
    PLANNING_MODEL_KEY,
    Config,
    ExecutionAgentType,
    LLMModel,
    LLMProvider,
    LogLevel,
    PlanningAgentType,
    StorageClass,
)
from portia.errors import ConfigNotFoundError, InvalidConfigError
from portia.model import (
    AzureOpenAIGenerativeModel,
    GenerativeModel,
    LangChainGenerativeModel,
)
from tests.utils import get_test_config


def test_from_default() -> None:
    """Test from default."""
    c = Config.from_default(
        default_log_level=LogLevel.CRITICAL,
        openai_api_key=SecretStr("123"),
    )
    assert c.default_log_level == LogLevel.CRITICAL


def test_set_keys(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test setting keys."""
    monkeypatch.setenv("PORTIA_API_KEY", "test-key")
    monkeypatch.setenv("OPENAI_API_KEY", "test-openai-key")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    monkeypatch.setenv("MISTRAL_API_KEY", "test-mistral-key")
    c = Config.from_default(default_log_level=LogLevel.CRITICAL)
    assert c.portia_api_key == SecretStr("test-key")
    assert c.openai_api_key == SecretStr("test-openai-key")
    assert c.anthropic_api_key == SecretStr("test-anthropic-key")
    assert c.mistralai_api_key == SecretStr("test-mistral-key")


def test_set_with_strings(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test setting keys as string."""
    monkeypatch.setenv("PORTIA_API_KEY", "test-key")
    monkeypatch.setenv("OPENAI_API_KEY", "test-openai-key")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    monkeypatch.setenv("MISTRAL_API_KEY", "test-mistral-key")
    # storage
    c = Config.from_default(storage_class="MEMORY")
    assert c.storage_class == StorageClass.MEMORY

    c = Config.from_default(storage_class="DISK", storage_dir="/test")
    assert c.storage_class == StorageClass.DISK
    assert c.storage_dir == "/test"

    # Need to specify storage_dir if using DISK
    with pytest.raises(InvalidConfigError):
        c = Config.from_default(storage_class="DISK")

    with pytest.raises(InvalidConfigError):
        c = Config.from_default(storage_class="OTHER")

    with pytest.raises(InvalidConfigError):
        c = Config.from_default(storage_class=123)

    # log level
    c = Config.from_default(default_log_level="CRITICAL")
    assert c.default_log_level == LogLevel.CRITICAL
    with pytest.raises(InvalidConfigError):
        c = Config.from_default(default_log_level="some level")

    # execution_agent_type
    c = Config.from_default(execution_agent_type="default")
    assert c.execution_agent_type == ExecutionAgentType.DEFAULT
    with pytest.raises(InvalidConfigError):
        c = Config.from_default(execution_agent_type="my agent")

    # Large output threshold value
    c = Config.from_default(
        large_output_threshold_tokens=100,
        feature_flags={
            FEATURE_FLAG_AGENT_MEMORY_ENABLED: True,
        },
    )
    assert c.large_output_threshold_tokens == 100
    assert c.exceeds_output_threshold("Test " * 1000)
    c = Config.from_default(
        large_output_threshold_tokens=100,
        feature_flags={
            FEATURE_FLAG_AGENT_MEMORY_ENABLED: False,
        },
    )
    assert c.large_output_threshold_tokens == 100
    assert not c.exceeds_output_threshold("Test " * 1000)


def test_set_llms(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test setting LLM models."""
    monkeypatch.setenv("OPENAI_API_KEY", "test-openai-key")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    monkeypatch.setenv("MISTRAL_API_KEY", "test-mistral-key")

    # Models can be set individually
    c = Config.from_default(
        planning_model_name=LLMModel.GPT_4_O,
        execution_model_name=LLMModel.GPT_4_O_MINI,
    )
    assert c.model(PLANNING_MODEL_KEY) == LLMModel.GPT_4_O
    assert c.model(EXECUTION_MODEL_KEY) == LLMModel.GPT_4_O_MINI

    # llm_model_name sets all models
    c = Config.from_default(llm_model_name="mistral_large")
    assert c.model(PLANNING_MODEL_KEY) == LLMModel.MISTRAL_LARGE
    assert c.model(EXECUTION_MODEL_KEY) == LLMModel.MISTRAL_LARGE

    # llm_provider sets default model for all providers
    c = Config.from_default(llm_provider="mistralai")
    assert c.model(PLANNING_MODEL_KEY) == LLMModel.MISTRAL_LARGE
    assert c.model(EXECUTION_MODEL_KEY) == LLMModel.MISTRAL_LARGE

    # With nothing specified, it chooses a model we have API keys for
    monkeypatch.setenv("OPENAI_API_KEY", "")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "")
    monkeypatch.setenv("MISTRAL_API_KEY", "test-mistral-key")
    c = Config.from_default()
    assert c.model(PLANNING_MODEL_KEY) == LLMModel.MISTRAL_LARGE
    assert c.model(EXECUTION_MODEL_KEY) == LLMModel.MISTRAL_LARGE

    # With all API key set, correct default models are chosen
    monkeypatch.setenv("OPENAI_API_KEY", "test-openai-key")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    c = Config.from_default()
    assert c.model(PLANNING_MODEL_KEY) == LLMModel.O_3_MINI
    assert c.model(EXECUTION_MODEL_KEY) == LLMModel.GPT_4_O

    # No api key for provider model
    monkeypatch.setenv("OPENAI_API_KEY", "")
    monkeypatch.setenv("MISTRAL_API_KEY", "")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "")
    monkeypatch.setenv("GOOGLE_API_KEY", "")
    monkeypatch.setenv("AZURE_OPENAI_API_KEY", "")
    for provider in [
        LLMProvider.OPENAI,
        LLMProvider.ANTHROPIC,
        LLMProvider.MISTRALAI,
        LLMProvider.GOOGLE_GENERATIVE_AI,
        LLMProvider.AZURE_OPENAI,
    ]:
        with pytest.raises(InvalidConfigError):
            Config.from_default(
                storage_class=StorageClass.MEMORY,
                llm_provider=provider,
                execution_agent_type=ExecutionAgentType.DEFAULT,
                planning_agent_type=PlanningAgentType.DEFAULT,
            )

    # Wrong api key for provider model
    monkeypatch.setenv("OPENAI_API_KEY", "")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    monkeypatch.setenv("MISTRAL_API_KEY", "")
    with pytest.raises(InvalidConfigError):
        Config.from_default(
            storage_class=StorageClass.MEMORY,
            llm_model_name=LLMModel.MISTRAL_LARGE,
            execution_agent_type=ExecutionAgentType.DEFAULT,
            planning_agent_type=PlanningAgentType.DEFAULT,
        )

    # Unrecognised providers error
    with pytest.raises(InvalidConfigError):
        c = Config.from_default(llm_provider="personal", llm_model_name="other-model")


def test_resolve_model_azure() -> None:
    """Test resolve model for Azure OpenAI."""
    c = Config.from_default(
        llm_provider=LLMProvider.AZURE_OPENAI,
        azure_openai_endpoint="http://test-azure-openai-endpoint",
        azure_openai_api_key="test-azure-openai-api-key",
    )
    assert isinstance(c.resolve_model(PLANNING_MODEL_KEY), AzureOpenAIGenerativeModel)


def test_resolve_langchain_model() -> None:
    """Test resolve langchain model."""
    conf = get_test_config(
        custom_models={
            PLANNING_MODEL_KEY: LangChainGenerativeModel(client=MagicMock(), model_name="test"),
        },
    )
    assert isinstance(conf.resolve_langchain_model(PLANNING_MODEL_KEY), LangChainGenerativeModel)


def test_resolve_langchain_model_error() -> None:
    """Test resolve langchain model raises TypeError if model is not a LangChainGenerativeModel."""
    conf = get_test_config(
        custom_models={
            PLANNING_MODEL_KEY: Mock(spec=GenerativeModel),
        },
    )
    with pytest.raises(TypeError, match="A LangChainGenerativeModel is required"):
        conf.resolve_langchain_model(PLANNING_MODEL_KEY)


def test_custom_models() -> None:
    """Test custom models."""
    c = Config.from_default(
        custom_models={
            PLANNING_MODEL_KEY: LangChainGenerativeModel(
                client=MagicMock(),
                model_name="gpt-4o",
            ),
        },
        openai_api_key=SecretStr("test-openai-key"),
    )
    resolved_model = c.resolve_model(PLANNING_MODEL_KEY)
    assert isinstance(resolved_model, LangChainGenerativeModel)
    assert resolved_model.model_name == "gpt-4o"


def test_getters() -> None:
    """Test getters work."""
    c = Config.from_default(
        openai_api_key=SecretStr("123"),
    )

    assert c.has_api_key("openai_api_key")

    with pytest.raises(ConfigNotFoundError):
        c.must_get("not real", str)

    c = Config.from_default(
        openai_api_key=SecretStr("123"),
        portia_api_key=SecretStr("123"),
        anthropic_api_key=SecretStr(""),
        portia_api_endpoint="",
        portia_dashboard_url="",
    )
    with pytest.raises(InvalidConfigError):
        c.must_get("portia_api_key", int)

    with pytest.raises(InvalidConfigError):
        c.must_get("portia_api_endpoint", str)

    with pytest.raises(InvalidConfigError):
        c.must_get("portia_dashboard_url", str)

    # no Portia API Key
    with pytest.raises(InvalidConfigError):
        Config.from_default(
            storage_class=StorageClass.CLOUD,
            portia_api_key=SecretStr(""),
            execution_agent_type=ExecutionAgentType.DEFAULT,
            planning_agent_type=PlanningAgentType.DEFAULT,
        )


def test_azure_openai_requires_endpoint(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test Azure OpenAI requires endpoint."""
    monkeypatch.setenv("OPENAI_API_KEY", "")
    monkeypatch.setenv("MISTRAL_API_KEY", "")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "")
    monkeypatch.setenv("GOOGLE_API_KEY", "")
    monkeypatch.setenv("AZURE_OPENAI_API_KEY", "test-azure-openai-key")
    monkeypatch.setenv("AZURE_OPENAI_ENDPOINT", "")

    # Without endpoint set, it errors
    with pytest.raises(InvalidConfigError):
        Config.from_default(llm_provider=LLMProvider.AZURE_OPENAI)

    # With endpoint set, it works
    monkeypatch.setenv("AZURE_OPENAI_ENDPOINT", "test-azure-openai-endpoint")
    c = Config.from_default(llm_provider=LLMProvider.AZURE_OPENAI)
    assert c.llm_provider == LLMProvider.AZURE_OPENAI
    assert c.model(PLANNING_MODEL_KEY).provider() == LLMProvider.AZURE_OPENAI

    # Also works with passing parameters to constructor
    monkeypatch.setenv("AZURE_OPENAI_ENDPOINT", "")
    monkeypatch.setenv("AZURE_OPENAI_API_KEY", "")
    c = Config.from_default(
        llm_provider=LLMProvider.AZURE_OPENAI,
        azure_openai_endpoint="test-azure-openai-endpoint",
        azure_openai_api_key="test-azure-openai-api-key",
    )
    assert c.llm_provider == LLMProvider.AZURE_OPENAI


@pytest.mark.parametrize("model", list(LLMModel))
def test_all_models_have_provider(model: LLMModel) -> None:
    """Test all models have a provider."""
    assert model.provider() is not None


@pytest.mark.parametrize(
    ("model_name", "expected"),
    [
        ("gpt-4o", LLMModel.GPT_4_O),
        ("openai/gpt-4o", LLMModel.GPT_4_O),
        ("azure_openai/gpt-4o", LLMModel.AZURE_GPT_4_O),
        ("claude-3-5-haiku-latest", LLMModel.CLAUDE_3_5_HAIKU),
        ("mistral-large-latest", LLMModel.MISTRAL_LARGE),
        ("gemini-2.0-flash", LLMModel.GEMINI_2_0_FLASH),
    ],
)
def test_llm_model_instantiate_from_string(model_name: str, expected: LLMModel) -> None:
    """Test LLM model from string."""
    model = LLMModel(model_name)
    assert model == expected


def test_llm_model_instantiate_from_string_missing() -> None:
    """Test LLM model from string missing."""
    with pytest.raises(ValueError, match="Invalid LLM model"):
        LLMModel("not-a-model")


PROVIDER_ENV_VARS = [
    "OPENAI_API_KEY",
    "ANTHROPIC_API_KEY",
    "MISTRAL_API_KEY",
    "GOOGLE_API_KEY",
    "AZURE_OPENAI_API_KEY",
    "AZURE_OPENAI_ENDPOINT",
]


def clear_env_vars(monkeypatch: pytest.MonkeyPatch) -> None:
    """Clear env vars for the Provider APIs."""
    for env_var in PROVIDER_ENV_VARS:
        monkeypatch.delenv(env_var, raising=False)


@pytest.mark.parametrize(
    ("env_vars", "provider"),
    [
        ({"OPENAI_API_KEY": "test-openai-api-key"}, LLMProvider.OPENAI),
        ({"ANTHROPIC_API_KEY": "test-anthropic-api-key"}, LLMProvider.ANTHROPIC),
        ({"MISTRAL_API_KEY": "test-mistral-api-key"}, LLMProvider.MISTRALAI),
        ({"GOOGLE_API_KEY": "test-google-api-key"}, LLMProvider.GOOGLE_GENERATIVE_AI),
        (
            {
                "AZURE_OPENAI_API_KEY": "test-azure-openai-api-key",
                "AZURE_OPENAI_ENDPOINT": "test-azure-openai-endpoint",
            },
            LLMProvider.AZURE_OPENAI,
        ),
    ],
)
def test_llm_provider_default_from_api_keys_env_vars(
    env_vars: dict[str, str],
    provider: LLMProvider,
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    """Test LLM provider default from API keys env vars."""
    clear_env_vars(monkeypatch)
    for env_var_name, env_var_value in env_vars.items():
        monkeypatch.setenv(env_var_name, env_var_value)

    c = Config.from_default()
    assert c.llm_provider == provider


@pytest.mark.parametrize(
    ("config_kwargs", "provider"),
    [
        ({"openai_api_key": "test-openai-api-key"}, LLMProvider.OPENAI),
        ({"anthropic_api_key": "test-anthropic-api-key"}, LLMProvider.ANTHROPIC),
        ({"mistralai_api_key": "test-mistral-api-key"}, LLMProvider.MISTRALAI),
        ({"google_api_key": "test-google-api-key"}, LLMProvider.GOOGLE_GENERATIVE_AI),
        (
            {
                "azure_openai_api_key": "test-azure-openai-api-key",
                "azure_openai_endpoint": "test-azure-openai-endpoint",
            },
            LLMProvider.AZURE_OPENAI,
        ),
    ],
)
def test_llm_provider_default_from_api_keys_config_kwargs(
    config_kwargs: dict[str, str],
    provider: LLMProvider,
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    """Test LLM provider default from API keys config kwargs."""
    clear_env_vars(monkeypatch)
    c = Config.from_default(**config_kwargs)
    assert c.llm_provider == provider

```

## File: tests/unit/test_tool_registry.py

```python
"""tests for the ToolRegistry classes."""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Union
from unittest.mock import MagicMock, patch

import mcp
import pytest
from mcp import ClientSession
from pydantic import BaseModel
from pydantic_core import PydanticUndefined

from portia.errors import DuplicateToolError, ToolNotFoundError
from portia.model import GenerativeModel
from portia.open_source_tools.llm_tool import LLMTool
from portia.open_source_tools.registry import open_source_tool_registry
from portia.tool_registry import (
    InMemoryToolRegistry,
    McpToolRegistry,
    PortiaToolRegistry,
    ToolRegistry,
    generate_pydantic_model_from_json_schema,
)
from tests.utils import MockMcpSessionWrapper, MockTool, get_test_tool_context

if TYPE_CHECKING:
    from collections.abc import Iterator

    from pytest_mock import MockerFixture


MOCK_TOOL_ID = "mock_tool"
OTHER_MOCK_TOOL_ID = "other_mock_tool"


def test_tool_registry_register_tool() -> None:
    """Test registering tools in the ToolRegistry."""
    tool_registry = ToolRegistry()
    tool_registry.with_tool(MockTool(id=MOCK_TOOL_ID))
    tool1 = tool_registry.get_tool(MOCK_TOOL_ID)
    assert tool1.id == MOCK_TOOL_ID

    with pytest.raises(ToolNotFoundError):
        tool_registry.get_tool("tool3")

    with pytest.raises(DuplicateToolError):
        tool_registry.with_tool(MockTool(id=MOCK_TOOL_ID))

    tool_registry.replace_tool(
        MockTool(
            id=MOCK_TOOL_ID,
            name="New Mock Tool",
        ),
    )
    tool2 = tool_registry.get_tool(MOCK_TOOL_ID)
    assert tool2.id == MOCK_TOOL_ID
    assert tool2.name == "New Mock Tool"


def test_tool_registry_get_and_plan_run() -> None:
    """Test getting and running tools in the InMemoryToolRegistry."""
    tool_registry = ToolRegistry()
    tool_registry.with_tool(MockTool(id=MOCK_TOOL_ID))
    tool1 = tool_registry.get_tool(MOCK_TOOL_ID)
    ctx = get_test_tool_context()
    tool1.run(ctx)


def test_tool_registry_get_tools() -> None:
    """Test the get_tools method of InMemoryToolRegistry."""
    tool_registry = ToolRegistry(
        [MockTool(id=MOCK_TOOL_ID), MockTool(id=OTHER_MOCK_TOOL_ID)],
    )
    tools = tool_registry.get_tools()
    assert len(tools) == 2
    assert any(tool.id == MOCK_TOOL_ID for tool in tools)
    assert any(tool.id == OTHER_MOCK_TOOL_ID for tool in tools)


def test_tool_registry_match_tools() -> None:
    """Test matching tools in the InMemoryToolRegistry."""
    tool_registry = ToolRegistry(
        [MockTool(id=MOCK_TOOL_ID), MockTool(id=OTHER_MOCK_TOOL_ID)],
    )

    # Test matching specific tool ID
    matched_tools = tool_registry.match_tools(tool_ids=[MOCK_TOOL_ID])
    assert len(matched_tools) == 1
    assert matched_tools[0].id == MOCK_TOOL_ID

    # Test matching multiple tool IDs
    matched_tools = tool_registry.match_tools(
        tool_ids=[MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID],
    )
    assert len(matched_tools) == 2
    assert {tool.id for tool in matched_tools} == {MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID}

    # Test matching non-existent tool ID
    matched_tools = tool_registry.match_tools(tool_ids=["non_existent_tool"])
    assert len(matched_tools) == 0

    # Test with no tool_ids (should return all tools)
    matched_tools = tool_registry.match_tools()
    assert len(matched_tools) == 2
    assert {tool.id for tool in matched_tools} == {MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID}


def test_combined_tool_registry_duplicate_tool() -> None:
    """Test searching across multiple registries in ToolRegistry."""
    tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    other_tool_registry = ToolRegistry(
        [MockTool(id=MOCK_TOOL_ID)],
    )
    combined_tool_registry = tool_registry + other_tool_registry

    tool1 = combined_tool_registry.get_tool(MOCK_TOOL_ID)
    assert tool1.id == MOCK_TOOL_ID


def test_combined_tool_registry_get_tool() -> None:
    """Test searching across multiple registries in ToolRegistry."""
    tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    other_tool_registry = ToolRegistry(
        [MockTool(id=OTHER_MOCK_TOOL_ID)],
    )
    combined_tool_registry = tool_registry + other_tool_registry

    tool1 = combined_tool_registry.get_tool(MOCK_TOOL_ID)
    assert tool1.id == MOCK_TOOL_ID

    with pytest.raises(ToolNotFoundError):
        combined_tool_registry.get_tool("tool_not_found")


def test_combined_tool_registry_get_tools() -> None:
    """Test getting all tools from an ToolRegistry."""
    tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    other_tool_registry = ToolRegistry(
        [MockTool(id=OTHER_MOCK_TOOL_ID)],
    )
    combined_tool_registry = tool_registry + other_tool_registry

    tools = combined_tool_registry.get_tools()
    assert len(tools) == 2
    assert any(tool.id == MOCK_TOOL_ID for tool in tools)


def test_combined_tool_registry_match_tools() -> None:
    """Test matching tools across multiple registries in ToolRegistry."""
    tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    other_tool_registry = ToolRegistry(
        [MockTool(id=OTHER_MOCK_TOOL_ID)],
    )
    combined_tool_registry = tool_registry + other_tool_registry

    # Test matching specific tool IDs
    matched_tools = combined_tool_registry.match_tools(tool_ids=[MOCK_TOOL_ID])
    assert len(matched_tools) == 1
    assert matched_tools[0].id == MOCK_TOOL_ID

    # Test matching multiple tool IDs
    matched_tools = combined_tool_registry.match_tools(
        tool_ids=[MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID],
    )
    assert len(matched_tools) == 2
    assert {tool.id for tool in matched_tools} == {MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID}

    # Test matching non-existent tool IDs
    matched_tools = combined_tool_registry.match_tools(tool_ids=["non_existent_tool"])
    assert len(matched_tools) == 0


def test_tool_registry_add_operators(mocker: MockerFixture) -> None:
    """Test the __add__ and __radd__ operators for ToolRegistry."""
    # Mock the logger
    mock_logger = mocker.Mock()
    mocker.patch("portia.tool_registry.logger", return_value=mock_logger)

    # Create registries and tools
    registry1 = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    registry2 = ToolRegistry([MockTool(id=OTHER_MOCK_TOOL_ID)])
    tool_list = [MockTool(id="tool3")]

    # Test registry + registry
    combined = registry1 + registry2
    assert isinstance(combined, ToolRegistry)
    assert len(combined.get_tools()) == 2
    assert {tool.id for tool in combined.get_tools()} == {MOCK_TOOL_ID, OTHER_MOCK_TOOL_ID}

    # Test registry + list
    combined = registry1 + tool_list  # type: ignore reportOperatorIssue
    assert isinstance(combined, ToolRegistry)
    assert len(combined.get_tools()) == 2
    assert {tool.id for tool in combined.get_tools()} == {MOCK_TOOL_ID, "tool3"}

    # Test list + registry (radd)
    combined = tool_list + registry1  # type: ignore reportOperatorIssue
    assert isinstance(combined, ToolRegistry)
    assert len(combined.get_tools()) == 2
    assert {tool.id for tool in combined.get_tools()} == {MOCK_TOOL_ID, "tool3"}

    # Test warning on duplicate tools
    duplicate_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID)])
    combined = registry1 + duplicate_registry
    mock_logger.warning.assert_called_once_with(
        f"Duplicate tool ID found: {MOCK_TOOL_ID}. Unintended behavior may occur.",
    )


def test_in_memory_tool_registry_from_local_tools() -> None:
    """Test creating an InMemoryToolRegistry from a list of local tools."""
    tool_registry = InMemoryToolRegistry.from_local_tools([MockTool(id=MOCK_TOOL_ID)])
    assert isinstance(tool_registry, InMemoryToolRegistry)
    assert len(tool_registry.get_tools()) == 1
    assert tool_registry.get_tool(MOCK_TOOL_ID).id == MOCK_TOOL_ID


def test_tool_registry_filter_tools() -> None:
    """Test filtering tools in a ToolRegistry."""
    tool_registry = ToolRegistry([MockTool(id=MOCK_TOOL_ID), MockTool(id=OTHER_MOCK_TOOL_ID)])
    filtered_registry = tool_registry.filter_tools(lambda tool: tool.id == MOCK_TOOL_ID)
    filtered_tools = filtered_registry.get_tools()
    assert len(filtered_tools) == 1
    assert filtered_tools[0].id == MOCK_TOOL_ID


def test_portia_tool_registry_missing_required_args() -> None:
    """Test that PortiaToolRegistry raises an error if required args are missing."""
    with pytest.raises(ValueError, match="Either config, client or tools must be provided"):
        PortiaToolRegistry()


def test_tool_registry_reconfigure_llm_tool() -> None:
    """Test replacing the LLMTool with a new LLMTool."""
    registry = ToolRegistry(open_source_tool_registry.get_tools())
    llm_tool = registry.get_tool("llm_tool")

    assert llm_tool is not None
    assert getattr(llm_tool, "model", None) is None

    registry.replace_tool(LLMTool(model=MagicMock(spec=GenerativeModel)))

    llm_tool = registry.get_tool("llm_tool")
    assert llm_tool is not None
    assert getattr(llm_tool, "model", None) is not None


@pytest.fixture
def mock_get_mcp_session() -> Iterator[None]:
    """Fixture to mock the get_mcp_session function."""
    mock_session = MagicMock(spec=ClientSession)
    mock_session.list_tools.return_value = mcp.ListToolsResult(
        tools=[
            mcp.Tool(
                name="test_tool",
                description="I am a tool",
                inputSchema={"type": "object", "properties": {"input": {"type": "string"}}},
            ),
            mcp.Tool(
                name="test_tool_2",
                description="I am another tool",
                inputSchema={"type": "object", "properties": {"input": {"type": "number"}}},
            ),
        ],
    )

    with patch(
        "portia.tool_registry.get_mcp_session",
        new=MockMcpSessionWrapper(mock_session).mock_mcp_session,
    ):
        yield


@pytest.fixture
def mcp_tool_registry(mock_get_mcp_session: None) -> McpToolRegistry:  # noqa: ARG001
    """Fixture for a McpToolRegistry."""
    return McpToolRegistry.from_stdio_connection(
        server_name="mock_mcp",
        command="test",
        args=["test"],
    )


@pytest.mark.usefixtures("mock_get_mcp_session")
def test_mcp_tool_registry_from_sse_connection() -> None:
    """Test constructing a McpToolRegistry from an SSE connection."""
    mcp_registry_sse = McpToolRegistry.from_sse_connection(
        server_name="mock_mcp",
        url="http://localhost:8000",
    )
    assert isinstance(mcp_registry_sse, McpToolRegistry)


def test_mcp_tool_registry_get_tools(mcp_tool_registry: McpToolRegistry) -> None:
    """Test getting tools from the MCPToolRegistry."""
    tools = mcp_tool_registry.get_tools()
    assert len(tools) == 2
    assert tools[0].id == "mcp:mock_mcp:test_tool"
    assert tools[0].name == "test_tool"
    assert tools[0].description == "I am a tool"
    assert issubclass(tools[0].args_schema, BaseModel)
    assert tools[1].id == "mcp:mock_mcp:test_tool_2"
    assert tools[1].name == "test_tool_2"
    assert tools[1].description == "I am another tool"
    assert issubclass(tools[1].args_schema, BaseModel)


def test_mcp_tool_registry_get_tool(mcp_tool_registry: McpToolRegistry) -> None:
    """Test getting a tool from the MCPToolRegistry."""
    tool = mcp_tool_registry.get_tool("mcp:mock_mcp:test_tool")
    assert tool.id == "mcp:mock_mcp:test_tool"
    assert tool.name == "test_tool"
    assert tool.description == "I am a tool"
    assert issubclass(tool.args_schema, BaseModel)


def test_generate_pydantic_model_from_json_schema() -> None:
    """Test generating a Pydantic model from a JSON schema."""
    json_schema = {
        "type": "object",
        "properties": {
            "name": {"type": "string", "description": "The name of the user"},
            "age": {"type": "integer", "description": "The age of the user"},
            "height": {"type": "number", "description": "The height of the user", "default": 185.2},
            "is_active": {"type": "boolean", "description": "Whether the user is active"},
            "pets": {
                "type": "array",
                "items": {"type": "string"},
                "description": "The pets of the user",
            },
            "address": {
                "type": "object",
                "properties": {
                    "street": {"type": "string", "description": "The street of the user"},
                    "city": {"type": "string", "description": "The city of the user"},
                    "zip": {"type": "string", "description": "The zip of the user"},
                },
                "description": "The address of the user",
                "required": ["city", "zip"],
            },
        },
        "required": ["name", "age"],
    }
    model = generate_pydantic_model_from_json_schema("TestModel", json_schema)
    assert model.model_fields["name"].annotation is str
    assert model.model_fields["name"].default is PydanticUndefined
    assert model.model_fields["name"].description == "The name of the user"
    assert model.model_fields["age"].annotation is int
    assert model.model_fields["age"].default is PydanticUndefined
    assert model.model_fields["age"].description == "The age of the user"
    assert model.model_fields["height"].annotation is float
    assert model.model_fields["height"].default == 185.2
    assert model.model_fields["height"].description == "The height of the user"
    assert model.model_fields["is_active"].annotation is bool
    assert model.model_fields["is_active"].default is None
    assert model.model_fields["is_active"].description == "Whether the user is active"
    assert model.model_fields["pets"].annotation == list[str]
    assert model.model_fields["pets"].default is None
    assert model.model_fields["pets"].description == "The pets of the user"
    address_type = model.model_fields["address"].annotation
    assert isinstance(address_type, type)
    assert issubclass(address_type, BaseModel)
    assert address_type.model_fields["street"].annotation is str
    assert address_type.model_fields["street"].default is None
    assert address_type.model_fields["street"].description == "The street of the user"
    assert address_type.model_fields["city"].annotation is str
    assert address_type.model_fields["city"].default is PydanticUndefined
    assert address_type.model_fields["city"].description == "The city of the user"
    assert address_type.model_fields["zip"].annotation is str
    assert address_type.model_fields["zip"].default is PydanticUndefined
    assert address_type.model_fields["zip"].description == "The zip of the user"
    assert model.model_fields["address"].default is None
    assert model.model_fields["address"].description == "The address of the user"


def test_generate_pydantic_model_from_json_schema_union_types() -> None:
    """Test generating a Pydantic model from a JSON schema with union types."""
    json_schema = {
        "type": "object",
        "properties": {
            "collaborators": {
                "anyOf": [
                    {"items": {"type": "integer"}, "type": "array"},
                    {"type": "null"},
                ],
                "default": None,
                "description": "Array of user IDs to CC on the ticket",
                "title": "Collaborator Ids",
            },
            "company_number": {
                "anyOf": [
                    {"type": "string"},
                    {"type": "integer"},
                ],
                "description": "Company number to search",
                "title": "Company Number",
            },
            "additional_company_numbers": {
                "type": "array",
                "items": {"oneOf": [{"type": "string"}, {"type": "integer"}]},
                "description": "Additional company numbers to search",
                "title": "Additional Company Numbers",
            },
        },
        "required": ["company_number"],
    }
    model = generate_pydantic_model_from_json_schema("TestUnionModel", json_schema)
    assert model.model_fields["collaborators"].annotation == Union[list[int], None]
    assert model.model_fields["collaborators"].default is None
    assert (
        model.model_fields["collaborators"].description == "Array of user IDs to CC on the ticket"
    )
    assert model.model_fields["company_number"].annotation == Union[str, int]
    assert model.model_fields["company_number"].default is PydanticUndefined
    assert model.model_fields["company_number"].description == "Company number to search"
    assert model.model_fields["additional_company_numbers"].annotation == list[Union[str, int]]
    assert model.model_fields["additional_company_numbers"].default is None
    assert (
        model.model_fields["additional_company_numbers"].description
        == "Additional company numbers to search"
    )


def test_generate_pydantic_model_from_json_schema_doesnt_handle_none_for_non_union_fields() -> None:
    """Test for generate_pydantic_model_from_json_schema.

    Test that generate_pydantic_model_from_json_schema maps 'null' to Any for non-union fields.
    """
    json_schema = {
        "type": "object",
        "properties": {
            "name": {
                "type": "null",
                "default": None,
                "description": "Array of user IDs to CC on the ticket",
            },
            "unknown_field_type": {
                "type": "random_type",
                "default": None,
                "description": "Array of user IDs to CC on the ticket",
            },
        },
    }
    model = generate_pydantic_model_from_json_schema("TestNullSchema", json_schema)
    assert model.model_fields["name"].annotation is Any
    assert model.model_fields["unknown_field_type"].annotation is Any


def test_generate_pydantic_model_from_json_schema_not_single_type_or_union_field() -> None:
    """Test for generate_pydantic_model_from_json_schema.

    Check it represents fields that are neither single type or union fields as Any type.
    """
    json_schema = {
        "type": "object",
        "properties": {
            "unknown": {
                "default": None,
                "description": "Array of user IDs to CC on the ticket",
            },
        },
    }
    model = generate_pydantic_model_from_json_schema("TestNullSchema", json_schema)
    assert model.model_fields["unknown"].annotation is Any

```

## File: tests/unit/test_model.py

```python
"""Unit tests for the Message class in portia.model."""

from types import SimpleNamespace
from unittest.mock import MagicMock

import pytest
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from pydantic import BaseModel, ValidationError

from portia.model import (
    GenerativeModel,
    LangChainGenerativeModel,
    Message,
    map_message_to_instructor,
)


@pytest.mark.parametrize(
    ("langchain_message", "expected_role", "expected_content"),
    [
        (HumanMessage(content="Hello"), "user", "Hello"),
        (AIMessage(content="Hi there"), "assistant", "Hi there"),
        (
            SystemMessage(content="You are a helpful assistant"),
            "system",
            "You are a helpful assistant",
        ),
    ],
)
def test_message_from_langchain(
    langchain_message: BaseMessage,
    expected_role: str,
    expected_content: str,
) -> None:
    """Test converting from LangChain messages to Portia Message."""
    message = Message.from_langchain(langchain_message)
    assert message.role == expected_role
    assert message.content == expected_content


def test_message_from_langchain_unsupported_type() -> None:
    """Test that converting from unsupported LangChain message type raises ValueError."""

    class UnsupportedMessage:
        content = "test"

    with pytest.raises(ValueError, match="Unsupported message type"):
        Message.from_langchain(UnsupportedMessage())  # type: ignore[arg-type]


@pytest.mark.parametrize(
    ("portia_message", "expected_type", "expected_content"),
    [
        (Message(role="user", content="Hello"), HumanMessage, "Hello"),
        (Message(role="assistant", content="Hi there"), AIMessage, "Hi there"),
        (
            Message(role="system", content="You are a helpful assistant"),
            SystemMessage,
            "You are a helpful assistant",
        ),
    ],
)
def test_message_to_langchain(
    portia_message: Message,
    expected_type: type[BaseMessage],
    expected_content: str,
) -> None:
    """Test converting from Portia Message to LangChain messages."""
    langchain_message = portia_message.to_langchain()
    assert isinstance(langchain_message, expected_type)
    assert langchain_message.content == expected_content


def test_message_to_langchain_unsupported_role() -> None:
    """Test that converting to LangChain message with unsupported role raises ValueError."""
    message = Message(role="user", content="test")
    # Force an invalid role to test the to_langchain method
    message.role = "invalid"  # type: ignore[assignment]
    with pytest.raises(ValueError, match="Unsupported role"):
        message.to_langchain()


@pytest.mark.parametrize(
    ("message", "expected_instructor_message"),
    [
        (Message(role="user", content="Hello"), {"role": "user", "content": "Hello"}),
        (
            Message(role="assistant", content="Hi there"),
            {"role": "assistant", "content": "Hi there"},
        ),
        (
            Message(role="system", content="You are a helpful assistant"),
            {"role": "system", "content": "You are a helpful assistant"},
        ),
    ],
)
def test_map_message_to_instructor(message: Message, expected_instructor_message: dict) -> None:
    """Test mapping a Message to an Instructor message."""
    assert map_message_to_instructor(message) == expected_instructor_message


def test_map_message_to_instructor_unsupported_role() -> None:
    """Test mapping a Message to an Instructor message with an unsupported role."""
    message = SimpleNamespace(role="invalid", content="Hello")
    with pytest.raises(ValueError, match="Unsupported message role"):
        map_message_to_instructor(message)  # type: ignore[arg-type]


def test_message_validation() -> None:
    """Test basic Message model validation."""
    # Valid message
    message = Message(role="user", content="Hello")
    assert message.role == "user"
    assert message.content == "Hello"

    # Invalid role
    with pytest.raises(ValidationError, match="Input should be 'user', 'assistant' or 'system'"):
        Message(role="invalid", content="Hello")  # type: ignore[arg-type]

    # Missing required fields
    with pytest.raises(ValidationError, match="Field required"):
        Message()  # type: ignore[call-arg]


class DummyGenerativeModel(GenerativeModel):
    """Dummy generative model."""

    provider_name: str = "portia"

    def __init__(self, model_name: str) -> None:
        """Initialize the model."""
        super().__init__(model_name)

    def get_response(self, messages: list[Message]) -> Message:  # noqa: ARG002
        """Get a response from the model."""
        return Message(role="assistant", content="Hello")

    def get_structured_response(
        self,
        messages: list[Message],  # noqa: ARG002
        schema: type[BaseModel],
    ) -> BaseModel:
        """Get a structured response from the model."""
        return schema()


def test_model_to_string() -> None:
    """Test that the model to string method works."""
    model = DummyGenerativeModel(model_name="test")
    assert str(model) == "portia/test"
    assert repr(model) == 'DummyGenerativeModel("portia/test")'


class StructuredOutputTestModel(BaseModel):
    """Test model for structured output."""

    test_field: str


def test_langchain_model_structured_output_returns_dict() -> None:
    """Test that LangchainModel.structured_output returns a dict."""
    base_chat_model = MagicMock(spec=BaseChatModel)
    structured_output = MagicMock()
    base_chat_model.with_structured_output.return_value = structured_output
    structured_output.invoke.return_value = {"test_field": "Response from model"}
    model = LangChainGenerativeModel(client=base_chat_model, model_name="test")
    result = model.get_structured_response(
        messages=[Message(role="user", content="Hello")],
        schema=StructuredOutputTestModel,
    )
    assert isinstance(result, StructuredOutputTestModel)
    assert result.test_field == "Response from model"

```

## File: tests/unit/test_portia.py

```python
"""Tests for portia classes."""

import tempfile
import threading
import time
from pathlib import Path
from unittest import mock
from unittest.mock import MagicMock

import pytest
from pydantic import HttpUrl, SecretStr

from portia.clarification import (
    ActionClarification,
    InputClarification,
    ValueConfirmationClarification,
)
from portia.config import (
    DEFAULT_MODEL_KEY,
    FEATURE_FLAG_AGENT_MEMORY_ENABLED,
    PLANNING_MODEL_KEY,
    Config,
    StorageClass,
)
from portia.errors import InvalidPlanRunStateError, PlanError, PlanRunNotFoundError
from portia.execution_agents.output import AgentMemoryOutput, LocalOutput
from portia.introspection_agents.introspection_agent import (
    PreStepIntrospection,
    PreStepIntrospectionOutcome,
)
from portia.model import LangChainGenerativeModel
from portia.open_source_tools.llm_tool import LLMTool
from portia.open_source_tools.registry import example_tool_registry, open_source_tool_registry
from portia.plan import Plan, PlanContext, ReadOnlyPlan, Step
from portia.plan_run import PlanRun, PlanRunOutputs, PlanRunState, PlanRunUUID, ReadOnlyPlanRun
from portia.planning_agents.base_planning_agent import StepsOrError
from portia.portia import ExecutionHooks, Portia
from portia.tool import Tool, ToolRunContext
from portia.tool_registry import ToolRegistry
from tests.utils import (
    AdditionTool,
    ClarificationTool,
    TestClarificationHandler,
    get_test_config,
    get_test_plan_run,
)


@pytest.fixture
def planning_model() -> MagicMock:
    """Fixture to create a mock planning model."""
    return MagicMock(spec=LangChainGenerativeModel)


@pytest.fixture
def default_model() -> MagicMock:
    """Fixture to create a mock default model."""
    return MagicMock(spec=LangChainGenerativeModel)


@pytest.fixture
def portia(planning_model: MagicMock, default_model: MagicMock) -> Portia:
    """Fixture to create a Portia instance for testing."""
    config = get_test_config(
        custom_models={
            PLANNING_MODEL_KEY: planning_model,
            DEFAULT_MODEL_KEY: default_model,
        },
    )
    tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
    return Portia(config=config, tools=tool_registry)


@pytest.fixture
def portia_with_agent_memory(planning_model: MagicMock, default_model: MagicMock) -> Portia:
    """Fixture to create a Portia instance for testing."""
    config = get_test_config(
        # Set a small threshold value so all outputs are stored in agent memory
        feature_flags={FEATURE_FLAG_AGENT_MEMORY_ENABLED: True},
        large_output_threshold_tokens=3,
        custom_models={
            PLANNING_MODEL_KEY: planning_model,
            DEFAULT_MODEL_KEY: default_model,
        },
    )
    tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
    return Portia(config=config, tools=tool_registry)


def test_portia_local_default_config_with_api_keys() -> None:
    """Test that the default config is used if no config is provided."""
    # Unset the portia API env that the portia doesn't try to use Portia Cloud
    with mock.patch.dict(
        "os.environ",
        {
            "PORTIA_API_KEY": "",
            "OPENAI_API_KEY": "123",
            "TAVILY_API_KEY": "123",
            "OPENWEATHERMAP_API_KEY": "123",
        },
    ):
        portia = Portia()
        assert portia.config == Config.from_default()
        assert len(portia.tool_registry.get_tools()) == len(open_source_tool_registry.get_tools())


def test_portia_local_default_config_without_api_keys() -> None:
    """Test that the default config when no API keys are provided."""
    # Unset the Tavily and weather API and check that these aren't included in
    # the default tool registry
    with mock.patch.dict(
        "os.environ",
        {
            "PORTIA_API_KEY": "",
            "OPENAI_API_KEY": "123",
            "TAVILY_API_KEY": "",
            "OPENWEATHERMAP_API_KEY": "",
        },
    ):
        portia = Portia()
        assert portia.config == Config.from_default()
        assert (
            len(portia.tool_registry.get_tools()) == len(open_source_tool_registry.get_tools()) - 2
        )


def test_portia_run_query(portia: Portia, planning_model: MagicMock) -> None:
    """Test running a query."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )

    plan_run = portia.run(query)

    assert plan_run.state == PlanRunState.COMPLETE


def test_portia_run_query_tool_list(planning_model: MagicMock) -> None:
    """Test running a query."""
    query = "example query"
    portia = Portia(
        config=get_test_config(
            custom_models={
                PLANNING_MODEL_KEY: planning_model,
            },
        ),
        tools=[AdditionTool(), ClarificationTool()],
    )

    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )
    plan_run = portia.run(query)

    assert plan_run.state == PlanRunState.COMPLETE


def test_portia_run_query_disk_storage(planning_model: MagicMock) -> None:
    """Test running a query."""
    with tempfile.TemporaryDirectory() as tmp_dir:
        query = "example query"
        config = Config.from_default(
            storage_class=StorageClass.DISK,
            openai_api_key=SecretStr("123"),
            storage_dir=tmp_dir,
            custom_models={
                PLANNING_MODEL_KEY: planning_model,
            },
        )
        tool_registry = ToolRegistry([AdditionTool(), ClarificationTool()])
        portia = Portia(config=config, tools=tool_registry)

        planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
        plan_run = portia.run(query)

        assert plan_run.state == PlanRunState.COMPLETE
        # Use Path to check for the files
        plan_files = list(Path(tmp_dir).glob("plan-*.json"))
        run_files = list(Path(tmp_dir).glob("prun-*.json"))

        assert len(plan_files) == 1
        assert len(run_files) == 1


def test_portia_generate_plan(portia: Portia, planning_model: MagicMock) -> None:
    """Test planning a query."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
    plan = portia.plan(query)

    assert plan.plan_context.query == query


def test_portia_generate_plan_error(portia: Portia, planning_model: MagicMock) -> None:
    """Test planning a query that returns an error."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error="could not plan",
    )
    with pytest.raises(PlanError):
        portia.plan(query)


def test_portia_generate_plan_with_tools(portia: Portia, planning_model: MagicMock) -> None:
    """Test planning a query."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
    plan = portia.plan(query, tools=["add_tool"])

    assert plan.plan_context.query == query
    assert plan.plan_context.tool_ids == ["add_tool"]


def test_portia_resume(portia: Portia, planning_model: MagicMock) -> None:
    """Test running a plan."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
    plan = portia.plan(query)
    plan_run = portia.create_plan_run(plan)
    plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.plan_id == plan.id


def test_portia_resume_after_interruption(portia: Portia, planning_model: MagicMock) -> None:
    """Test resuming PlanRun after interruption."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
    plan_run = portia.run(query)

    # Simulate run being in progress
    plan_run.state = PlanRunState.IN_PROGRESS
    plan_run.current_step_index = 1
    plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.current_step_index == 1


def test_portia_resume_edge_cases(portia: Portia, planning_model: MagicMock) -> None:
    """Test edge cases for execute."""
    with pytest.raises(ValueError):  # noqa: PT011
        portia.resume()

    query = "example query"
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )
    plan = portia.plan(query)
    plan_run = portia.create_plan_run(plan)

    # Simulate run being in progress
    plan_run.state = PlanRunState.IN_PROGRESS
    plan_run.current_step_index = 1
    plan_run = portia.resume(plan_run_id=plan_run.id)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.current_step_index == 1

    with pytest.raises(PlanRunNotFoundError):
        portia.resume(plan_run_id=PlanRunUUID())


def test_portia_run_invalid_state(portia: Portia, planning_model: MagicMock) -> None:
    """Test resuming PlanRun with an invalid state."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(steps=[], error=None)
    plan_run = portia.run(query)

    # Set invalid state
    plan_run.state = PlanRunState.COMPLETE

    with pytest.raises(InvalidPlanRunStateError):
        portia.resume(plan_run)


def test_portia_wait_for_ready(portia: Portia, planning_model: MagicMock) -> None:
    """Test wait for ready."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[Step(task="Example task", inputs=[], output="$output")],
        error=None,
    )
    plan_run = portia.run(query)

    plan_run.state = PlanRunState.FAILED
    with pytest.raises(InvalidPlanRunStateError):
        portia.wait_for_ready(plan_run)

    plan_run.state = PlanRunState.IN_PROGRESS
    plan_run = portia.wait_for_ready(plan_run)
    assert plan_run.state == PlanRunState.IN_PROGRESS

    def update_run_state() -> None:
        """Update the run state after sleeping."""
        time.sleep(1)  # Simulate some delay before state changes
        plan_run.state = PlanRunState.READY_TO_RESUME
        portia.storage.save_plan_run(plan_run)

    plan_run.state = PlanRunState.NEED_CLARIFICATION

    # Ensure current_step_index is set to a valid index
    plan_run.current_step_index = 0
    portia.storage.save_plan_run(plan_run)

    # start a thread to update in status
    update_thread = threading.Thread(target=update_run_state)
    update_thread.start()

    plan_run = portia.wait_for_ready(plan_run)
    assert plan_run.state == PlanRunState.READY_TO_RESUME


def test_portia_wait_for_ready_tool(portia: Portia) -> None:
    """Test wait for ready."""
    mock_call_count = MagicMock()
    mock_call_count.__iadd__ = (
        lambda self, other: setattr(self, "count", self.count + other) or self
    )
    mock_call_count.count = 0

    class ReadyTool(Tool):
        """Returns ready."""

        id: str = "ready_tool"
        name: str = "Ready Tool"
        description: str = "Returns a clarification"
        output_schema: tuple[str, str] = (
            "Clarification",
            "Clarification: The value of the Clarification",
        )

        def run(self, ctx: ToolRunContext, user_guidance: str) -> str:  # noqa: ARG002
            return "result"

        def ready(self, ctx: ToolRunContext) -> bool:  # noqa: ARG002
            mock_call_count.count += 1
            return mock_call_count.count == 3

    portia.tool_registry = ToolRegistry([ReadyTool()])
    step0 = Step(
        task="Do something",
        inputs=[],
        output="$ctx_0",
    )
    step1 = Step(
        task="Save Context",
        inputs=[],
        output="$ctx",
        tool_id="ready_tool",
    )
    plan = Plan(
        plan_context=PlanContext(
            query="run the tool",
            tool_ids=["ready_tool"],
        ),
        steps=[step0, step1],
    )
    unresolved_action = ActionClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="",
        action_url=HttpUrl("https://unresolved.step1.com"),
        step=1,
    )
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=1,
        state=PlanRunState.NEED_CLARIFICATION,
        outputs=PlanRunOutputs(
            clarifications=[
                ActionClarification(
                    plan_run_id=PlanRunUUID(),
                    user_guidance="",
                    action_url=HttpUrl("https://resolved.step0.com"),
                    resolved=True,
                    step=0,
                ),
                ActionClarification(
                    plan_run_id=PlanRunUUID(),
                    user_guidance="",
                    action_url=HttpUrl("https://resolved.step1.com"),
                    resolved=True,
                    step=1,
                ),
                unresolved_action,
            ],
        ),
    )
    portia.storage.save_plan(plan)
    portia.storage.save_plan_run(plan_run)
    assert plan_run.get_outstanding_clarifications() == [unresolved_action]
    plan_run = portia.wait_for_ready(plan_run)
    assert plan_run.state == PlanRunState.READY_TO_RESUME
    assert plan_run.get_outstanding_clarifications() == []
    for clarification in plan_run.outputs.clarifications:
        if clarification.step == 1:
            assert clarification.resolved
            assert clarification.response == "complete"


def test_get_clarifications_and_get_run_called_once(
    portia: Portia,
    planning_model: MagicMock,
) -> None:
    """Test that get_clarifications_for_step is called once after get_plan_run."""
    query = "example query"
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[Step(task="Example task", inputs=[], output="$output")],
        error=None,
    )
    plan_run = portia.run(query)

    # Set the run state to NEED_CLARIFICATION to ensure it goes through the wait logic
    plan_run.state = PlanRunState.NEED_CLARIFICATION
    plan_run.current_step_index = 0  # Set to a valid index

    # Mock the storage methods
    with (
        mock.patch.object(
            portia.storage,
            "get_plan_run",
            return_value=plan_run,
        ) as mock_get_plan_run,
        mock.patch.object(
            PlanRun,
            "get_clarifications_for_step",
            return_value=[],
        ) as mock_get_clarifications,
    ):
        # Call wait_for_ready
        portia.wait_for_ready(plan_run)

        # Assert that get_run was called once
        mock_get_plan_run.assert_called_once_with(plan_run.id)

        # Assert that get_clarifications_for_step was called once after get_run
        mock_get_clarifications.assert_called_once()


def test_portia_run_query_with_summary(portia: Portia, planning_model: MagicMock) -> None:
    """Test run_query sets both final output and summary correctly."""
    query = "What activities can I do in London based on weather?"

    # Mock planning_agent response
    weather_step = Step(
        task="Get weather in London",
        tool_id="add_tool",
        output="$weather",
    )
    activities_step = Step(
        task="Suggest activities based on weather",
        tool_id="add_tool",
        output="$activities",
    )
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[weather_step, activities_step],
        error=None,
    )

    # Mock agent responses
    weather_output = LocalOutput(value="Sunny and warm")
    activities_output = LocalOutput(value="Visit Hyde Park and have a picnic")
    expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"

    mock_step_agent = mock.MagicMock()
    mock_step_agent.execute_sync.side_effect = [weather_output, activities_output]

    mock_summarizer_agent = mock.MagicMock()
    mock_summarizer_agent.create_summary.side_effect = [expected_summary]

    with (
        mock.patch(
            "portia.portia.FinalOutputSummarizer",
            return_value=mock_summarizer_agent,
        ),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
    ):
        plan_run = portia.run(query)

        # Verify run completed successfully
        assert plan_run.state == PlanRunState.COMPLETE

        # Verify step outputs were stored correctly
        assert plan_run.outputs.step_outputs["$weather"] == weather_output
        assert plan_run.outputs.step_outputs["$activities"] == activities_output

        # Verify final output and summary
        assert plan_run.outputs.final_output is not None
        assert plan_run.outputs.final_output.get_value() == activities_output.get_value()
        assert plan_run.outputs.final_output.get_summary() == expected_summary

        # Verify create_summary was called with correct args
        mock_summarizer_agent.create_summary.assert_called_once_with(
            plan=mock.ANY,
            plan_run=mock.ANY,
        )


def test_portia_sets_final_output_with_summary(portia: Portia) -> None:
    """Test that final output is set with correct summary."""
    (plan, plan_run) = get_test_plan_run()
    plan.steps = [
        Step(
            task="Get weather in London",
            output="$london_weather",
        ),
        Step(
            task="Suggest activities based on weather",
            output="$activities",
        ),
    ]

    plan_run.outputs.step_outputs = {
        "$london_weather": LocalOutput(value="Sunny and warm"),
        "$activities": LocalOutput(value="Visit Hyde Park and have a picnic"),
    }

    expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"
    mock_summarizer = mock.MagicMock()
    mock_summarizer.create_summary.side_effect = [expected_summary]

    with mock.patch(
        "portia.portia.FinalOutputSummarizer",
        return_value=mock_summarizer,
    ):
        last_step_output = LocalOutput(value="Visit Hyde Park and have a picnic")
        output = portia._get_final_output(plan, plan_run, last_step_output)  # noqa: SLF001

        # Verify the final output
        assert output is not None
        assert output.get_value() == "Visit Hyde Park and have a picnic"
        assert output.get_summary() == expected_summary

        # Verify create_summary was called with correct args
        mock_summarizer.create_summary.assert_called_once()
        call_args = mock_summarizer.create_summary.call_args[1]
        assert isinstance(call_args["plan"], ReadOnlyPlan)
        assert isinstance(call_args["plan_run"], ReadOnlyPlanRun)
        assert call_args["plan"].id == plan.id
        assert call_args["plan_run"].id == plan_run.id


def test_portia_run_query_with_memory(
    portia_with_agent_memory: Portia,
    planning_model: MagicMock,
) -> None:
    """Test run_query sets both final output and summary correctly."""
    query = "What activities can I do in London based on weather?"

    # Mock planning_agent response
    weather_step = Step(
        task="Get weather in London",
        tool_id="add_tool",
        output="$weather",
    )
    activities_step = Step(
        task="Suggest activities based on weather",
        tool_id="add_tool",
        output="$activities",
    )
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[weather_step, activities_step],
        error=None,
    )

    # Mock agent responses
    weather_summary = "sunny"
    weather_output = LocalOutput(value="Sunny and warm", summary=weather_summary)
    activities_summary = "picnic"
    activities_output = LocalOutput(
        value="Visit Hyde Park and have a picnic",
        summary=activities_summary,
    )
    expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"

    mock_step_agent = mock.MagicMock()
    mock_step_agent.execute_sync.side_effect = [weather_output, activities_output]

    mock_summarizer_agent = mock.MagicMock()
    mock_summarizer_agent.create_summary.side_effect = [expected_summary]

    with (
        mock.patch(
            "portia.portia.FinalOutputSummarizer",
            return_value=mock_summarizer_agent,
        ),
        mock.patch.object(
            portia_with_agent_memory,
            "_get_agent_for_step",
            return_value=mock_step_agent,
        ),
    ):
        plan_run = portia_with_agent_memory.run(query)

        # Verify run completed successfully
        assert plan_run.state == PlanRunState.COMPLETE

        # Verify step outputs were stored correctly
        assert plan_run.outputs.step_outputs["$weather"] == AgentMemoryOutput(
            output_name="$weather",
            plan_run_id=plan_run.id,
            summary=weather_summary,
        )
        assert (
            portia_with_agent_memory.storage.get_plan_run_output("$weather", plan_run.id)
            == weather_output
        )
        assert plan_run.outputs.step_outputs["$activities"] == AgentMemoryOutput(
            output_name="$activities",
            plan_run_id=plan_run.id,
            summary=activities_summary,
        )
        assert (
            portia_with_agent_memory.storage.get_plan_run_output("$activities", plan_run.id)
            == activities_output
        )

        # Verify final output and summary
        assert plan_run.outputs.final_output is not None
        assert plan_run.outputs.final_output.get_value() == activities_output.value
        assert plan_run.outputs.final_output.get_summary() == expected_summary


def test_portia_get_final_output_handles_summary_error(portia: Portia) -> None:
    """Test that final output is set even if summary generation fails."""
    (plan, plan_run) = get_test_plan_run()

    # Mock the SummarizerAgent to raise an exception
    mock_agent = mock.MagicMock()
    mock_agent.create_summary.side_effect = Exception("Summary failed")

    with mock.patch(
        "portia.execution_agents.utils.final_output_summarizer.FinalOutputSummarizer",
        return_value=mock_agent,
    ):
        step_output = LocalOutput(value="Some output")
        final_output = portia._get_final_output(plan, plan_run, step_output)  # noqa: SLF001

        # Verify the final output is set without summary
        assert final_output is not None
        assert final_output.get_value() == "Some output"
        assert final_output.get_summary() is None


def test_portia_wait_for_ready_max_retries(portia: Portia) -> None:
    """Test wait for ready with max retries."""
    plan, plan_run = get_test_plan_run()
    plan_run.state = PlanRunState.NEED_CLARIFICATION
    portia.storage.save_plan(plan)
    portia.storage.save_plan_run(plan_run)
    with pytest.raises(InvalidPlanRunStateError):
        portia.wait_for_ready(plan_run, max_retries=0)


def test_portia_wait_for_ready_backoff_period(portia: Portia) -> None:
    """Test wait for ready with backoff period."""
    plan, plan_run = get_test_plan_run()
    plan_run.state = PlanRunState.NEED_CLARIFICATION
    portia.storage.save_plan(plan)
    portia.storage.get_plan_run = mock.MagicMock(return_value=plan_run)
    with pytest.raises(InvalidPlanRunStateError):
        portia.wait_for_ready(plan_run, max_retries=1, backoff_start_time_seconds=0)


def test_portia_resolve_clarification_error(portia: Portia) -> None:
    """Test resolve error."""
    plan, plan_run = get_test_plan_run()
    plan2, plan_run2 = get_test_plan_run()
    clarification = InputClarification(
        user_guidance="",
        argument_name="",
        plan_run_id=plan_run2.id,
    )
    portia.storage.save_plan(plan)
    portia.storage.save_plan_run(plan_run)
    portia.storage.save_plan(plan2)
    portia.storage.save_plan_run(plan_run2)
    with pytest.raises(InvalidPlanRunStateError):
        portia.resolve_clarification(clarification, "test")

    with pytest.raises(InvalidPlanRunStateError):
        portia.resolve_clarification(clarification, "test", plan_run)


def test_portia_resolve_clarification(portia: Portia) -> None:
    """Test resolve success."""
    plan, plan_run = get_test_plan_run()
    clarification = InputClarification(
        user_guidance="",
        argument_name="",
        plan_run_id=plan_run.id,
    )
    plan_run.outputs.clarifications = [clarification]
    portia.storage.save_plan(plan)
    portia.storage.save_plan_run(plan_run)

    plan_run = portia.resolve_clarification(clarification, "test", plan_run)
    assert plan_run.state == PlanRunState.READY_TO_RESUME


def test_portia_get_tool_for_step_none_tool_id() -> None:
    """Test that when step.tool_id is None, LLMTool is used as fallback."""
    portia = Portia(config=get_test_config(), tools=[AdditionTool()])
    plan, plan_run = get_test_plan_run()

    # Create a step with no tool_id
    step = Step(
        task="Some task",
        inputs=[],
        output="$output",
        tool_id=None,
    )

    tool = portia._get_tool_for_step(step, plan_run)  # noqa: SLF001
    assert tool is None


def test_get_llm_tool() -> None:
    """Test special case retrieval of LLMTool as it isn't explicitly in most tool registries."""
    portia = Portia(config=get_test_config(), tools=example_tool_registry)
    plan, plan_run = get_test_plan_run()

    # Create a step with no tool_id
    step = Step(
        task="Some task",
        inputs=[],
        output="$output",
        tool_id=LLMTool.LLM_TOOL_ID,
    )

    tool = portia._get_tool_for_step(step, plan_run)  # noqa: SLF001
    assert tool is not None
    assert isinstance(tool._child_tool, LLMTool)  # noqa: SLF001 # pyright: ignore[reportAttributeAccessIssue]


def test_portia_run_plan(portia: Portia, planning_model: MagicMock) -> None:
    """Test that run_plan calls create_plan_run and resume."""
    query = "example query"

    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )
    plan = portia.plan(query)

    # Mock the create_plan_run and resume methods
    with (
        mock.patch.object(portia, "create_plan_run") as mockcreate_plan_run,
        mock.patch.object(portia, "resume") as mock_resume,
    ):
        mock_plan_run = MagicMock()
        mock_resumed_plan_run = MagicMock()
        mockcreate_plan_run.return_value = mock_plan_run
        mock_resume.return_value = mock_resumed_plan_run

        result = portia.run_plan(plan)

        mockcreate_plan_run.assert_called_once_with(plan)

        mock_resume.assert_called_once_with(mock_plan_run)

        assert result == mock_resumed_plan_run


def test_portia_handle_clarification(planning_model: MagicMock) -> None:
    """Test that portia can handle a clarification."""
    clarification_handler = TestClarificationHandler()
    portia = Portia(
        config=get_test_config(custom_models={PLANNING_MODEL_KEY: planning_model}),
        tools=[ClarificationTool()],
        execution_hooks=ExecutionHooks(clarification_handler=clarification_handler),
    )
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[
            Step(
                task="Raise a clarification",
                tool_id="clarification_tool",
                output="$output",
            ),
        ],
        error=None,
    )
    mock_step_agent = mock.MagicMock()
    mock_summarizer_agent = mock.MagicMock()
    mock_summarizer_agent.create_summary.side_effect = "I caught the clarification"
    with (
        mock.patch(
            "portia.portia.FinalOutputSummarizer",
            return_value=mock_summarizer_agent,
        ),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
    ):
        plan = portia.plan("Raise a clarification")
        plan_run = portia.create_plan_run(plan)

        mock_step_agent.execute_sync.side_effect = [
            LocalOutput(
                value=InputClarification(
                    plan_run_id=plan_run.id,
                    user_guidance="Handle this clarification",
                    argument_name="raise_clarification",
                ),
            ),
            LocalOutput(value="I caught the clarification"),
        ]
        portia.resume(plan_run)
        assert plan_run.state == PlanRunState.COMPLETE

        # Check that the clarifications were handled correctly
        assert clarification_handler.received_clarification is not None
        assert (
            clarification_handler.received_clarification.user_guidance
            == "Handle this clarification"
        )


def test_portia_error_clarification(portia: Portia, planning_model: MagicMock) -> None:
    """Test that portia can handle an error clarification."""
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )
    plan_run = portia.run("test query")

    portia.error_clarification(
        ValueConfirmationClarification(
            plan_run_id=plan_run.id,
            user_guidance="Handle this clarification",
            argument_name="raise_clarification",
        ),
        error=ValueError("test error"),
    )
    assert plan_run.state == PlanRunState.FAILED


def test_portia_error_clarification_with_plan_run(
    portia: Portia,
    planning_model: MagicMock,
) -> None:
    """Test that portia can handle an error clarification."""
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[],
        error=None,
    )
    plan_run = portia.run("test query")

    portia.error_clarification(
        ValueConfirmationClarification(
            plan_run_id=plan_run.id,
            user_guidance="Handle this clarification",
            argument_name="raise_clarification",
        ),
        error=ValueError("test error"),
        plan_run=plan_run,
    )
    assert plan_run.state == PlanRunState.FAILED


def test_portia_run_with_introspection_skip(portia: Portia, planning_model: MagicMock) -> None:
    """Test run with introspection agent returning SKIP outcome."""
    # Setup mock plan and response
    step1 = Step(task="Step 1", inputs=[], output="$step1_result", condition="some_condition")
    step2 = Step(task="Step 2", inputs=[], output="$step2_result")
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[step1, step2],
        error=None,
    )

    # Mock introspection agent to return SKIP for first step
    mock_introspection = MagicMock()
    mock_introspection.pre_step_introspection.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.SKIP,
        reason="Condition not met",
    )

    # Mock step agent to return output for second step
    mock_step_agent = MagicMock()
    mock_step_agent.execute_sync.return_value = LocalOutput(value="Step 2 result")

    with (
        mock.patch.object(portia, "_get_introspection_agent", return_value=mock_introspection),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
    ):
        plan_run = portia.run("Test query with skipped step")

        # Verify result
        assert plan_run.state == PlanRunState.COMPLETE
        assert "$step1_result" in plan_run.outputs.step_outputs
        assert (
            plan_run.outputs.step_outputs["$step1_result"].get_value()
            == PreStepIntrospectionOutcome.SKIP
        )
        assert "$step2_result" in plan_run.outputs.step_outputs
        assert plan_run.outputs.step_outputs["$step2_result"].get_value() == "Step 2 result"
        assert plan_run.outputs.final_output is not None
        assert plan_run.outputs.final_output.get_value() == "Step 2 result"


def test_portia_run_with_introspection_complete(portia: Portia, planning_model: MagicMock) -> None:
    """Test run with introspection agent returning COMPLETE outcome."""
    # Setup mock plan and response
    step1 = Step(task="Step 1", inputs=[], output="$step1_result")
    step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="some_condition")
    step3 = Step(task="Step 3", inputs=[], output="$step3_result")
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[step1, step2, step3],
        error=None,
    )

    # Mock step agent for first step
    mock_step_agent = MagicMock()
    mock_step_agent.execute_sync.return_value = LocalOutput(value="Step 1 result")

    # Configure the COMPLETE outcome for the introspection agent
    mock_introspection_complete = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.COMPLETE,
        reason="Remaining steps cannot be executed",
    )

    def custom_handle_introspection(*args, **kwargs):  # noqa: ANN002, ANN003, ANN202, ARG001
        plan_run: PlanRun = kwargs.get("plan_run")  # type: ignore  # noqa: PGH003

        if plan_run.current_step_index == 1:
            plan_run.outputs.step_outputs["$step2_result"] = LocalOutput(
                value=PreStepIntrospectionOutcome.COMPLETE,
                summary="Remaining steps cannot be executed",
            )
            plan_run.outputs.final_output = LocalOutput(
                value="Step 1 result",
                summary="Execution completed early",
            )
            plan_run.state = PlanRunState.COMPLETE

            return (plan_run, mock_introspection_complete)

        # Otherwise continue normally
        return (
            plan_run,
            PreStepIntrospection(
                outcome=PreStepIntrospectionOutcome.CONTINUE,
                reason="Condition met",
            ),
        )

    with (
        mock.patch.object(portia, "_handle_introspection_outcome", custom_handle_introspection),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
    ):
        # Run the test
        plan_run = portia.run("Test query with early completed execution")

        # Verify result based on our simulated outcomes
        assert plan_run.state == PlanRunState.COMPLETE
        assert "$step2_result" in plan_run.outputs.step_outputs
        assert (
            plan_run.outputs.step_outputs["$step2_result"].get_value()
            == PreStepIntrospectionOutcome.COMPLETE
        )
        assert plan_run.outputs.final_output is not None
        assert plan_run.outputs.final_output.get_summary() == "Execution completed early"


def test_portia_run_with_introspection_fail(portia: Portia, planning_model: MagicMock) -> None:
    """Test run with introspection agent returning FAIL outcome."""
    # Setup mock plan and response
    step1 = Step(task="Step 1", inputs=[], output="$step1_result")
    step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="some_condition")
    planning_model.get_structured_response.return_value = StepsOrError(
        steps=[step1, step2],
        error=None,
    )

    # Mock step agent for first step
    mock_step_agent = MagicMock()
    mock_step_agent.execute_sync.return_value = LocalOutput(value="Step 1 result")

    # Configure the FAIL outcome
    mock_introspection_fail = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.FAIL,
        reason="Missing required data",
    )

    def custom_handle_introspection(*args, **kwargs):  # noqa: ANN002, ANN003, ANN202, ARG001
        plan_run: PlanRun = kwargs.get("plan_run")  # type: ignore  # noqa: PGH003
        # If this is step 1, simulate a FAIL outcome
        if plan_run.current_step_index == 1:
            # Modify the plan_run to look like it failed
            failed_output = LocalOutput(
                value=PreStepIntrospectionOutcome.FAIL,
                summary="Missing required data",
            )
            plan_run.outputs.step_outputs["$step2_result"] = failed_output
            plan_run.outputs.final_output = failed_output
            plan_run.state = PlanRunState.FAILED

            # Return FAIL outcome
            return (plan_run, mock_introspection_fail)

        # Otherwise continue normally
        return (
            plan_run,
            PreStepIntrospection(
                outcome=PreStepIntrospectionOutcome.CONTINUE,
                reason="Condition met",
            ),
        )

    with (
        mock.patch.object(portia, "_handle_introspection_outcome", custom_handle_introspection),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
    ):
        # Run the test
        plan_run = portia.run("Test query with failed execution")

        # Verify the expected outcome
        assert plan_run.state == PlanRunState.FAILED
        assert "$step2_result" in plan_run.outputs.step_outputs
        assert (
            plan_run.outputs.step_outputs["$step2_result"].get_value()
            == PreStepIntrospectionOutcome.FAIL
        )
        assert plan_run.outputs.final_output is not None
        assert plan_run.outputs.final_output.get_value() == PreStepIntrospectionOutcome.FAIL
        assert plan_run.outputs.final_output.get_summary() == "Missing required data"


def test_handle_introspection_outcome_complete(portia: Portia) -> None:
    """Test the actual implementation of _handle_introspection_outcome for COMPLETE outcome."""
    # Create a plan with conditions
    step = Step(task="Test step", inputs=[], output="$test_output", condition="some_condition")
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=[]),
        steps=[step],
    )
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=0,
        state=PlanRunState.IN_PROGRESS,
    )

    mock_introspection = MagicMock()
    mock_introspection.pre_step_introspection.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.COMPLETE,
        reason="Stopping execution",
    )

    # Mock the _get_final_output method to return a predefined output
    mock_final_output = LocalOutput(value="Final result", summary="Final summary")
    with mock.patch.object(portia, "_get_final_output", return_value=mock_final_output):
        # Call the actual method (not mocked)
        previous_output = LocalOutput(value="Previous step result")
        updated_plan_run, outcome = portia._handle_introspection_outcome(  # noqa: SLF001
            introspection_agent=mock_introspection,
            plan=plan,
            plan_run=plan_run,
            last_executed_step_output=previous_output,
        )

        # Verify the outcome
        assert outcome.outcome == PreStepIntrospectionOutcome.COMPLETE
        assert outcome.reason == "Stopping execution"

        # Verify plan_run was updated correctly
        assert (
            updated_plan_run.outputs.step_outputs["$test_output"].get_value()
            == PreStepIntrospectionOutcome.COMPLETE
        )
        assert (
            updated_plan_run.outputs.step_outputs["$test_output"].get_summary()
            == "Stopping execution"
        )
        assert updated_plan_run.outputs.final_output == mock_final_output
        assert updated_plan_run.state == PlanRunState.COMPLETE


def test_handle_introspection_outcome_fail(portia: Portia) -> None:
    """Test the actual implementation of _handle_introspection_outcome for FAIL outcome."""
    # Create a plan with conditions
    step = Step(task="Test step", inputs=[], output="$test_output", condition="some_condition")
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=[]),
        steps=[step],
    )
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=0,
        state=PlanRunState.IN_PROGRESS,
    )

    # Mock the introspection agent to return FAIL
    mock_introspection = MagicMock()
    mock_introspection.pre_step_introspection.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.FAIL,
        reason="Execution failed",
    )

    # Call the actual method (not mocked)
    previous_output = LocalOutput(value="Previous step result")
    updated_plan_run, outcome = portia._handle_introspection_outcome(  # noqa: SLF001
        introspection_agent=mock_introspection,
        plan=plan,
        plan_run=plan_run,
        last_executed_step_output=previous_output,
    )

    # Verify the outcome
    assert outcome.outcome == PreStepIntrospectionOutcome.FAIL
    assert outcome.reason == "Execution failed"

    # Verify plan_run was updated correctly
    assert (
        updated_plan_run.outputs.step_outputs["$test_output"].get_value()
        == PreStepIntrospectionOutcome.FAIL
    )
    assert updated_plan_run.outputs.step_outputs["$test_output"].get_summary() == "Execution failed"
    assert updated_plan_run.outputs.final_output is not None
    assert updated_plan_run.outputs.final_output.get_value() == PreStepIntrospectionOutcome.FAIL
    assert updated_plan_run.outputs.final_output.get_summary() == "Execution failed"
    assert updated_plan_run.state == PlanRunState.FAILED


def test_handle_introspection_outcome_skip(portia: Portia) -> None:
    """Test the actual implementation of _handle_introspection_outcome for SKIP outcome."""
    # Create a plan with conditions
    step = Step(task="Test step", inputs=[], output="$test_output", condition="some_condition")
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=[]),
        steps=[step],
    )
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=0,
        state=PlanRunState.IN_PROGRESS,
    )

    # Mock the introspection agent to return SKIP
    mock_introspection = MagicMock()
    mock_introspection.pre_step_introspection.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.SKIP,
        reason="Skipping step",
    )

    # Call the actual method (not mocked)
    previous_output = LocalOutput(value="Previous step result")
    updated_plan_run, outcome = portia._handle_introspection_outcome(  # noqa: SLF001
        introspection_agent=mock_introspection,
        plan=plan,
        plan_run=plan_run,
        last_executed_step_output=previous_output,
    )

    # Verify the outcome
    assert outcome.outcome == PreStepIntrospectionOutcome.SKIP
    assert outcome.reason == "Skipping step"

    # Verify plan_run was updated correctly
    assert (
        updated_plan_run.outputs.step_outputs["$test_output"].get_value()
        == PreStepIntrospectionOutcome.SKIP
    )
    assert updated_plan_run.outputs.step_outputs["$test_output"].get_summary() == "Skipping step"
    assert updated_plan_run.state == PlanRunState.IN_PROGRESS  # State should remain IN_PROGRESS


def test_handle_introspection_outcome_no_condition(portia: Portia) -> None:
    """Test _handle_introspection_outcome when step has no condition."""
    # Create a plan with a step that has no condition
    step = Step(task="Test step", inputs=[], output="$test_output")  # No condition
    plan = Plan(
        plan_context=PlanContext(query="test query", tool_ids=[]),
        steps=[step],
    )
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=0,
        state=PlanRunState.IN_PROGRESS,
    )

    # Mock the introspection agent (should not be called)
    mock_introspection = MagicMock()

    # Call the actual method
    previous_output = LocalOutput(value="Previous step result")
    updated_plan_run, outcome = portia._handle_introspection_outcome(  # noqa: SLF001
        introspection_agent=mock_introspection,
        plan=plan,
        plan_run=plan_run,
        last_executed_step_output=previous_output,
    )

    # Verify default outcome is CONTINUE
    assert outcome.outcome == PreStepIntrospectionOutcome.CONTINUE
    assert outcome.reason == "No condition to evaluate."

    # The introspection agent should not be called
    mock_introspection.pre_step_introspection.assert_not_called()

    # Plan run should be unchanged (no step outputs added)
    assert "$test_output" not in updated_plan_run.outputs.step_outputs
    assert updated_plan_run.state == PlanRunState.IN_PROGRESS


def test_portia_resume_with_skipped_steps(portia: Portia) -> None:
    """Test resuming a plan run with skipped steps and verifying final output.

    This test verifies:
    1. Resuming from a middle index works correctly
    2. Steps marked as SKIPPED are properly skipped during execution
    3. The final output is correctly computed from the last non-SKIPPED step
    """
    # Create a plan with multiple steps
    step1 = Step(task="Step 1", inputs=[], output="$step1_result")
    step2 = Step(task="Step 2", inputs=[], output="$step2_result", condition="true")
    step3 = Step(task="Step 3", inputs=[], output="$step3_result", condition="false")
    step4 = Step(task="Step 4", inputs=[], output="$step4_result", condition="false")
    plan = Plan(
        plan_context=PlanContext(query="Test query with skips", tool_ids=[]),
        steps=[step1, step2, step3, step4],
    )

    # Create a plan run that's partially completed (step1 is done)
    plan_run = PlanRun(
        plan_id=plan.id,
        current_step_index=1,  # Resume from step 2
        state=PlanRunState.IN_PROGRESS,
        outputs=PlanRunOutputs(
            step_outputs={
                "$step1_result": LocalOutput(value="Step 1 result", summary="Summary of step 1"),
            },
        ),
    )

    # Mock the storage to return our plan
    portia.storage.save_plan(plan)
    portia.storage.save_plan_run(plan_run)

    # Mock introspection agent to SKIP steps 3 and 4
    mock_introspection = MagicMock()

    def mock_introspection_outcome(*args, **kwargs):  # noqa: ANN002, ANN003, ANN202, ARG001
        plan_run = kwargs.get("plan_run")
        if plan_run.current_step_index in (2, 3):  # pyright: ignore[reportOptionalMemberAccess] # Skip both step3 and step4
            return PreStepIntrospection(
                outcome=PreStepIntrospectionOutcome.SKIP,
                reason="Condition is false",
            )
        return PreStepIntrospection(
            outcome=PreStepIntrospectionOutcome.CONTINUE,
            reason="Continue execution",
        )

    mock_introspection.pre_step_introspection.side_effect = mock_introspection_outcome

    # Mock step agent to return expected output for step 2 only (steps 3 and 4 will be skipped)
    mock_step_agent = MagicMock()
    mock_step_agent.execute_sync.return_value = LocalOutput(
        value="Step 2 result",
        summary="Summary of step 2",
    )

    # Mock the final output summarizer
    expected_summary = "Combined summary of steps 1 and 2"
    mock_summarizer = MagicMock()
    mock_summarizer.create_summary.return_value = expected_summary

    with (
        mock.patch.object(portia, "_get_introspection_agent", return_value=mock_introspection),
        mock.patch.object(portia, "_get_agent_for_step", return_value=mock_step_agent),
        mock.patch("portia.portia.FinalOutputSummarizer", return_value=mock_summarizer),
    ):
        result_plan_run = portia.resume(plan_run)

        assert result_plan_run.state == PlanRunState.COMPLETE

        assert result_plan_run.outputs.step_outputs["$step1_result"].get_value() == "Step 1 result"
        assert result_plan_run.outputs.step_outputs["$step2_result"].get_value() == "Step 2 result"
        assert (
            result_plan_run.outputs.step_outputs["$step3_result"].get_value()
            == PreStepIntrospectionOutcome.SKIP
        )
        assert (
            result_plan_run.outputs.step_outputs["$step4_result"].get_value()
            == PreStepIntrospectionOutcome.SKIP
        )
        assert result_plan_run.outputs.final_output is not None
        assert result_plan_run.outputs.final_output.get_value() == "Step 2 result"
        assert result_plan_run.outputs.final_output.get_summary() == expected_summary
        assert result_plan_run.current_step_index == 3

```

## File: tests/unit/test_storage.py

```python
"""Test simple agent."""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING
from unittest.mock import ANY, MagicMock, patch
from uuid import UUID

import pytest

from portia.errors import StorageError
from portia.execution_agents.output import AgentMemoryOutput, LocalOutput
from portia.plan import Plan, PlanContext, PlanUUID
from portia.plan_run import PlanRun, PlanRunState, PlanRunUUID
from portia.storage import (
    AdditionalStorage,
    DiskFileStorage,
    InMemoryStorage,
    PlanRunListResponse,
    PlanStorage,
    PortiaCloudStorage,
    RunStorage,
)
from tests.utils import get_test_config, get_test_plan_run, get_test_tool_call

if TYPE_CHECKING:
    from pytest_httpx import HTTPXMock

    from portia.tool_call import ToolCallRecord


def test_storage_base_classes() -> None:
    """Test PlanStorage raises."""

    class MyStorage(RunStorage, PlanStorage, AdditionalStorage):
        """Override to test base."""

        def save_plan(self, plan: Plan) -> None:
            return super().save_plan(plan)  # type: ignore  # noqa: PGH003

        def get_plan(self, plan_id: PlanUUID) -> Plan:
            return super().get_plan(plan_id)  # type: ignore  # noqa: PGH003

        def save_plan_run(self, plan_run: PlanRun) -> None:
            return super().save_plan_run(plan_run)  # type: ignore  # noqa: PGH003

        def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
            return super().get_plan_run(plan_run_id)  # type: ignore  # noqa: PGH003

        def get_plan_runs(
            self,
            run_state: PlanRunState | None = None,
            page: int | None = None,
        ) -> PlanRunListResponse:
            return super().get_plan_runs(run_state, page)  # type: ignore  # noqa: PGH003

        def save_tool_call(self, tool_call: ToolCallRecord) -> None:
            return super().save_tool_call(tool_call)  # type: ignore  # noqa: PGH003

    storage = MyStorage()
    plan = Plan(plan_context=PlanContext(query="", tool_ids=[]), steps=[])
    plan_run = PlanRun(
        plan_id=plan.id,
    )

    tool_call = get_test_tool_call(plan_run)

    with pytest.raises(NotImplementedError):
        storage.save_plan(plan)

    with pytest.raises(NotImplementedError):
        storage.get_plan(plan.id)

    with pytest.raises(NotImplementedError):
        storage.save_plan_run(plan_run)

    with pytest.raises(NotImplementedError):
        storage.get_plan_run(plan_run.id)

    with pytest.raises(NotImplementedError):
        storage.get_plan_runs()

    with pytest.raises(NotImplementedError):
        storage.save_tool_call(tool_call)


def test_in_memory_storage() -> None:
    """Test in memory storage."""
    storage = InMemoryStorage()
    (plan, plan_run) = get_test_plan_run()
    storage.save_plan(plan)
    assert storage.get_plan(plan.id) == plan
    storage.save_plan_run(plan_run)
    assert storage.get_plan_run(plan_run.id) == plan_run
    assert storage.get_plan_runs().results == [plan_run]
    assert storage.get_plan_runs(PlanRunState.FAILED).results == []
    storage.save_plan_run_output("test name", LocalOutput(value="test value"), plan_run.id)
    assert storage.get_plan_run_output("test name", plan_run.id) == LocalOutput(value="test value")
    # This just logs, but check it doesn't cause any issues
    tool_call = get_test_tool_call(plan_run)
    storage.save_tool_call(tool_call)
    # Check with a very large output too
    tool_call.output = "a" * 100000
    storage.save_tool_call(tool_call)


def test_disk_storage(tmp_path: Path) -> None:
    """Test disk storage."""
    storage = DiskFileStorage(storage_dir=str(tmp_path))
    (plan, plan_run) = get_test_plan_run()
    storage.save_plan(plan)
    assert storage.get_plan(plan.id) == plan
    storage.save_plan_run(plan_run)
    assert storage.get_plan_run(plan_run.id) == plan_run
    all_runs = storage.get_plan_runs()
    assert all_runs.results == [plan_run]
    assert storage.get_plan_runs(PlanRunState.FAILED).results == []
    storage.save_plan_run_output("test name", LocalOutput(value="test value"), plan_run.id)
    assert storage.get_plan_run_output("test name", plan_run.id) == LocalOutput(value="test value")


def test_portia_cloud_storage() -> None:
    """Test PortiaCloudStorage raises StorageError on failure responses."""
    config = get_test_config(portia_api_key="test_api_key")
    storage = PortiaCloudStorage(config)

    plan = Plan(
        id=PlanUUID(uuid=UUID("12345678-1234-5678-1234-567812345678")),
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(
        id=PlanRunUUID(uuid=UUID("87654321-4321-8765-4321-876543218765")),
        plan_id=plan.id,
    )
    tool_call = get_test_tool_call(plan_run)

    mock_response = MagicMock()
    mock_response.is_success = False
    mock_response.content = b"An error occurred."

    with (
        patch.object(storage.client, "post", return_value=mock_response) as mock_post,
        patch.object(storage.client, "get", return_value=mock_response) as mock_get,
    ):
        # Test save_plan failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.save_plan(plan)

        mock_post.assert_called_once_with(
            url="/api/v0/plans/",
            json={
                "id": str(plan.id),
                "steps": [],
                "query": plan.plan_context.query,
                "tool_ids": plan.plan_context.tool_ids,
            },
        )

    with (
        patch.object(storage.client, "get", return_value=mock_response) as mock_get,
    ):
        # Test get_plan failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.get_plan(plan.id)

        mock_get.assert_called_once_with(
            url=f"/api/v0/plans/{plan.id}/",
        )

    with (
        patch.object(storage.client, "put", return_value=mock_response) as mock_put,
    ):
        # Test save_run failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.save_plan_run(plan_run)

        mock_put.assert_called_once_with(
            url=f"/api/v0/plan-runs/{plan_run.id}/",
            json={
                "current_step_index": plan_run.current_step_index,
                "state": plan_run.state,
                "execution_context": plan_run.execution_context.model_dump(mode="json"),
                "outputs": plan_run.outputs.model_dump(mode="json"),
                "plan_id": str(plan_run.plan_id),
            },
        )

    with (
        patch.object(storage.client, "get", return_value=mock_response) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.get_plan_run(plan_run.id)

        mock_get.assert_called_once_with(
            url=f"/api/v0/plan-runs/{plan_run.id}/",
        )

    with (
        patch.object(storage.client, "get", return_value=mock_response) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.get_plan_runs()

        mock_get.assert_called_once_with(
            url="/api/v0/plan-runs/?",
        )

    with (
        patch.object(storage.client, "post", return_value=mock_response) as mock_post,
    ):
        # Test save_tool_call failure
        with pytest.raises(StorageError, match="An error occurred."):
            storage.save_tool_call(tool_call)

        mock_post.assert_called_once_with(
            url="/api/v0/tool-calls/",
            json={
                "plan_run_id": str(tool_call.plan_run_id),
                "tool_name": tool_call.tool_name,
                "step": tool_call.step,
                "end_user_id": tool_call.end_user_id or "",
                "additional_data": tool_call.additional_data,
                "input": tool_call.input,
                "output": tool_call.output,
                "status": tool_call.status,
                "latency_seconds": tool_call.latency_seconds,
            },
        )


def test_portia_cloud_storage_errors() -> None:
    """Test PortiaCloudStorage raises StorageError on failure responses."""
    config = get_test_config(portia_api_key="test_api_key")
    storage = PortiaCloudStorage(config)

    plan = Plan(
        id=PlanUUID(uuid=UUID("12345678-1234-5678-1234-567812345678")),
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(
        id=PlanRunUUID(uuid=UUID("87654321-4321-8765-4321-876543218765")),
        plan_id=plan.id,
    )

    tool_call = get_test_tool_call(plan_run)

    mock_exception = RuntimeError("An error occurred.")
    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test save_plan failure
        with pytest.raises(StorageError):
            storage.save_plan(plan)

        mock_post.assert_called_once_with(
            url="/api/v0/plans/",
            json={
                "id": str(plan.id),
                "steps": [],
                "query": plan.plan_context.query,
                "tool_ids": plan.plan_context.tool_ids,
            },
        )
    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test get_plan failure
        with pytest.raises(StorageError):
            storage.get_plan(plan.id)

        mock_get.assert_called_once_with(
            url=f"/api/v0/plans/{plan.id}/",
        )

    with (
        patch.object(storage.client, "put", side_effect=mock_exception) as mock_put,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test save_run failure
        with pytest.raises(StorageError):
            storage.save_plan_run(plan_run)

        mock_put.assert_called_once_with(
            url=f"/api/v0/plan-runs/{plan_run.id}/",
            json={
                "current_step_index": plan_run.current_step_index,
                "state": plan_run.state,
                "execution_context": plan_run.execution_context.model_dump(mode="json"),
                "outputs": plan_run.outputs.model_dump(mode="json"),
                "plan_id": str(plan_run.plan_id),
            },
        )

    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError):
            storage.get_plan_run(plan_run.id)

        mock_get.assert_called_once_with(
            url=f"/api/v0/plan-runs/{plan_run.id}/",
        )

    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError):
            storage.get_plan_runs()

        mock_get.assert_called_once_with(
            url="/api/v0/plan-runs/?",
        )

    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError):
            storage.get_plan_runs(run_state=PlanRunState.COMPLETE, page=10)

        mock_get.assert_called_once_with(
            url="/api/v0/plan-runs/?page=10&run_state=COMPLETE",
        )

    with (
        patch.object(storage.client, "post", side_effect=mock_exception) as mock_post,
        patch.object(storage.client, "get", side_effect=mock_exception) as mock_get,
    ):
        # Test get_run failure
        with pytest.raises(StorageError):
            storage.save_tool_call(tool_call)

        mock_post.assert_called_once_with(
            url="/api/v0/tool-calls/",
            json={
                "plan_run_id": str(tool_call.plan_run_id),
                "tool_name": tool_call.tool_name,
                "step": tool_call.step,
                "end_user_id": tool_call.end_user_id or "",
                "additional_data": tool_call.additional_data,
                "input": tool_call.input,
                "output": tool_call.output,
                "status": tool_call.status,
                "latency_seconds": tool_call.latency_seconds,
            },
        )


def test_portia_cloud_agent_memory(httpx_mock: HTTPXMock) -> None:
    """Test PortiaCloudStorage agent memory."""
    config = get_test_config(portia_api_key="test_api_key")
    agent_memory = PortiaCloudStorage(config)
    plan = Plan(
        id=PlanUUID(uuid=UUID("12345678-1234-5678-1234-567812345678")),
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(
        id=PlanRunUUID(uuid=UUID("87654321-4321-8765-4321-876543218765")),
        plan_id=plan.id,
    )
    output = LocalOutput(value="test value", summary="test summary")
    mock_success_response = MagicMock()
    mock_success_response.is_success = True

    # Test saving an output
    with (
        patch.object(
            agent_memory.form_client,
            "put",
            return_value=mock_success_response,
        ) as mock_put,
    ):
        result = agent_memory.save_plan_run_output("test_output", output, plan_run.id)

        mock_put.assert_called_once_with(
            url=f"/api/v0/agent-memory/plan-runs/{plan_run.id}/outputs/test_output/",
            files={
                "value": (
                    "output",
                    ANY,
                ),
            },
            data={
                "summary": output.get_summary(),
            },
        )
        assert isinstance(result, AgentMemoryOutput)
        assert result.output_name == "test_output"
        assert result.plan_run_id == plan_run.id
        assert result.summary == output.get_summary()
        assert Path(f".portia/cache/agent_memory/{plan_run.id}/test_output.json").is_file()

    # Test getting an output when it is cached locally
    with (
        patch.object(agent_memory.client, "get") as mock_get,
    ):
        result = agent_memory.get_plan_run_output("test_output", plan_run.id)

        # Verify that we didn't call Portia Cloud because we have a cached value
        mock_get.assert_not_called()

        # Verify the returned output
        assert result.get_summary() == output.get_summary()
        assert result.get_value() == output.get_value()

    # Test getting an output when it is not cached locally
    mock_output_response = MagicMock()
    mock_output_response.is_success = True
    mock_output_response.json.return_value = {
        "summary": "test summary",
        "url": "https://example.com/output",
    }
    httpx_mock.add_response(
        url="https://example.com/output",
        status_code=200,
        content=b"test value",
    )

    with (
        patch.object(agent_memory, "_read_from_cache", side_effect=FileNotFoundError),
        patch.object(agent_memory.client, "get", return_value=mock_output_response) as mock_get,
        patch.object(agent_memory, "_write_to_cache") as mock_write_cache,
    ):
        result = agent_memory.get_plan_run_output("test_output", plan_run.id)

        # Verify that it fetched from Portia Cloud
        mock_get.assert_called_once_with(
            url=f"/api/v0/agent-memory/plan-runs/{plan_run.id}/outputs/test_output/",
        )

        # Verify that it fetched the value from the URL using the httpx client
        assert len(httpx_mock.get_requests()) == 1

        # Verify that it wrote to the local cache
        mock_write_cache.assert_called_once()

        # Verify the returned output
        assert result.get_summary() == "test summary"
        assert result.get_value() == "test value"


def test_portia_cloud_agent_memory_local_cache_expiry() -> None:
    """Test PortiaCloudStorage agent memory."""
    config = get_test_config(portia_api_key="test_api_key")
    agent_memory = PortiaCloudStorage(config)
    plan = Plan(
        id=PlanUUID(uuid=UUID("12345678-1234-5678-1234-567812345678")),
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(
        id=PlanRunUUID(uuid=UUID("87654321-4321-8765-4321-876543218765")),
        plan_id=plan.id,
    )
    output = LocalOutput(value="test value", summary="test summary")
    mock_success_response = MagicMock()
    mock_success_response.is_success = True

    mock_success_response = MagicMock()
    mock_success_response.is_success = True

    with (
        patch.object(
            agent_memory.form_client,
            "put",
            return_value=mock_success_response,
        ),
        patch.object(agent_memory.client, "get"),
    ):
        # Write 21 outputs to the cache (cache size is 20)
        for i in range(21):
            agent_memory.save_plan_run_output(f"test_output_{i}", output, plan_run.id)

        # Check that the cache only stores 20 entries
        cache_files = list(Path(agent_memory.cache_dir).glob("**/*.json"))
        assert len(cache_files) == 20
        assert "test_output_20.json" in [file.name for file in cache_files]


def test_portia_cloud_agent_memory_errors() -> None:
    """Test PortiaCloudStorage raises StorageError on agent memory failure responses."""
    config = get_test_config(portia_api_key="test_api_key")
    agent_memory = PortiaCloudStorage(config)
    plan = Plan(
        id=PlanUUID(uuid=UUID("12345678-1234-5678-1234-567812345678")),
        plan_context=PlanContext(query="", tool_ids=[]),
        steps=[],
    )
    plan_run = PlanRun(
        id=PlanRunUUID(uuid=UUID("87654321-4321-8765-4321-876543218765")),
        plan_id=plan.id,
    )
    output = LocalOutput(value="test value", summary="test summary")

    mock_exception = RuntimeError("An error occurred.")
    with (
        patch.object(agent_memory.form_client, "put", side_effect=mock_exception) as mock_put,
    ):
        with pytest.raises(StorageError):
            agent_memory.save_plan_run_output("test_output", output, plan_run.id)

        mock_put.assert_called_once_with(
            url=f"/api/v0/agent-memory/plan-runs/{plan_run.id}/outputs/test_output/",
            files={
                "value": (
                    "output",
                    ANY,
                ),
            },
            data={
                "summary": output.get_summary(),
            },
        )

    with (
        patch.object(
            agent_memory,
            "_read_from_cache",
            side_effect=FileNotFoundError,
        ) as mock_read_cache,
        patch.object(agent_memory.client, "get", side_effect=mock_exception) as mock_get,
    ):
        with pytest.raises(StorageError):
            agent_memory.get_plan_run_output("test_output", plan_run.id)

        mock_read_cache.assert_called_once_with(f"{plan_run.id}/test_output.json", LocalOutput)
        mock_get.assert_called_once_with(
            url=f"/api/v0/agent-memory/plan-runs/{plan_run.id}/outputs/test_output/",
        )

```

## File: tests/unit/execution_agents/test_execution_utils.py

```python
"""Test execution utilities."""

from __future__ import annotations

import pytest
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.graph import END, MessagesState

from portia.clarification import InputClarification
from portia.config import FEATURE_FLAG_AGENT_MEMORY_ENABLED
from portia.errors import InvalidAgentOutputError, ToolFailedError, ToolRetryError
from portia.execution_agents.execution_utils import (
    MAX_RETRIES,
    AgentNode,
    next_state_after_tool_call,
    process_output,
    tool_call_or_end,
)
from portia.execution_agents.output import LocalOutput, Output
from portia.prefixed_uuid import PlanRunUUID
from tests.utils import AdditionTool, get_test_config


def test_next_state_after_tool_call_no_error() -> None:
    """Test next state when tool call succeeds."""
    messages: list[ToolMessage] = [
        ToolMessage(
            content="Success message",
            tool_call_id="123",
            name="test_tool",
        ),
    ]
    state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

    result = next_state_after_tool_call(get_test_config(), state)

    assert result == END


def test_next_state_after_tool_call_with_summarize() -> None:
    """Test next state when tool call succeeds and should summarize."""
    tool = AdditionTool()
    tool.should_summarize = True

    messages: list[ToolMessage] = [
        ToolMessage(
            content="Success message",
            tool_call_id="123",
            name="test_tool",
        ),
    ]
    state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

    result = next_state_after_tool_call(get_test_config(), state, tool)

    assert result == AgentNode.SUMMARIZER


def test_next_state_after_tool_call_with_large_output() -> None:
    """Test next state when tool call succeeds and should summarize."""
    tool = AdditionTool()
    messages: list[ToolMessage] = [
        ToolMessage(
            content="Test" * 1000,
            tool_call_id="123",
            name="test_tool",
        ),
    ]
    state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

    config = get_test_config(
        # Set a small threshold value so all outputs are stored in agent memory
        feature_flags={FEATURE_FLAG_AGENT_MEMORY_ENABLED: True},
        large_output_threshold_tokens=10,
    )
    result = next_state_after_tool_call(config, state, tool)
    assert result == AgentNode.SUMMARIZER


def test_next_state_after_tool_call_with_error_retry() -> None:
    """Test next state when tool call fails and max retries reached."""
    for i in range(1, MAX_RETRIES + 1):
        messages: list[ToolMessage] = [
            ToolMessage(
                content=f"ToolSoftError: Error {j}",
                tool_call_id=str(j),
                name="test_tool",
            )
            for j in range(1, i + 1)
        ]
        state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

        result = next_state_after_tool_call(get_test_config(), state)

        expected_state = END if i == MAX_RETRIES else AgentNode.TOOL_AGENT
        assert result == expected_state, f"Failed at retry {i}"


def test_tool_call_or_end() -> None:
    """Test tool_call_or_end state transitions."""
    message_with_calls = AIMessage(content="test")
    state_with_calls: MessagesState = {"messages": [message_with_calls]}  # type: ignore  # noqa: PGH003

    message_without_calls = HumanMessage(content="test")
    state_without_calls: MessagesState = {"messages": [message_without_calls]}  # type: ignore  # noqa: PGH003

    assert tool_call_or_end(state_with_calls) == AgentNode.TOOLS
    assert tool_call_or_end(state_without_calls) == END


def test_process_output_with_clarifications() -> None:
    """Test process_output with clarifications."""
    clarifications = [
        InputClarification(
            argument_name="test",
            user_guidance="test",
            plan_run_id=PlanRunUUID(),
        ),
    ]
    message = HumanMessage(content="test")

    result = process_output([message], clarifications=clarifications)  # type: ignore  # noqa: PGH003

    assert isinstance(result, Output)
    assert result.get_value() == clarifications


def test_process_output_with_tool_errors() -> None:
    """Test process_output with tool errors."""
    tool = AdditionTool()

    soft_error = ToolMessage(content="ToolSoftError: test", tool_call_id="1", name="test")
    hard_error = ToolMessage(content="ToolHardError: test", tool_call_id="1", name="test")

    with pytest.raises(ToolRetryError):
        process_output([soft_error], tool)

    with pytest.raises(ToolFailedError):
        process_output([hard_error], tool)


def test_process_output_with_invalid_message() -> None:
    """Test process_output with invalid message."""
    invalid_message = AIMessage(content="test")

    with pytest.raises(InvalidAgentOutputError):
        process_output([invalid_message])


def test_process_output_with_output_artifacts() -> None:
    """Test process_output with outpu artifacts."""
    message = ToolMessage(tool_call_id="1", content="", artifact=LocalOutput(value="test"))
    message2 = ToolMessage(tool_call_id="2", content="", artifact=LocalOutput(value="bar"))

    result = process_output([message, message2], clarifications=[])

    assert isinstance(result, Output)
    assert result.get_value() == ["test", "bar"]
    assert result.get_summary() == "test, bar"


def test_process_output_with_artifacts() -> None:
    """Test process_output with artifacts."""
    message = ToolMessage(tool_call_id="1", content="", artifact="test")

    result = process_output([message], clarifications=[])

    assert isinstance(result, Output)
    assert result.get_value() == "test"


def test_process_output_with_content() -> None:
    """Test process_output with content."""
    message = ToolMessage(tool_call_id="1", content="test")

    result = process_output([message], clarifications=[])

    assert isinstance(result, Output)
    assert result.get_value() == "test"


def test_process_output_summary_matches_serialized_value() -> None:
    """Test process_output summary matches serialized value."""
    dict_value = {"key1": "value1", "key2": "value2"}
    message = ToolMessage(tool_call_id="1", content="test", artifact=LocalOutput(value=dict_value))

    result = process_output([message], clarifications=[])

    assert isinstance(result, Output)
    assert result.get_value() == dict_value
    assert result.get_summary() == result.serialize_value()


def test_process_output_summary_not_updated_if_provided() -> None:
    """Test process_output does not update summary if already provided."""
    dict_value = {"key1": "value1", "key2": "value2"}
    provided_summary = "This is a provided summary."
    message = ToolMessage(
        tool_call_id="1",
        content="test",
        artifact=LocalOutput(value=dict_value, summary=provided_summary),
    )

    result = process_output([message], clarifications=[])

    assert isinstance(result, Output)
    assert result.get_value() == dict_value
    assert result.get_summary() == provided_summary


def test_next_state_after_tool_call_with_clarification_artifact() -> None:
    """Test next state when tool call succeeds with clarification artifact."""
    tool = AdditionTool()
    tool.should_summarize = True

    clarification = InputClarification(
        argument_name="test",
        user_guidance="test",
        plan_run_id=PlanRunUUID(),
    )

    messages: list[ToolMessage] = [
        ToolMessage(
            content="Success message",
            tool_call_id="123",
            name="test_tool",
            artifact=clarification,
        ),
    ]
    state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

    result = next_state_after_tool_call(get_test_config(), state, tool)

    # Should return END even though tool.should_summarize is True
    # because the message contains a clarification artifact
    assert result == END


def test_next_state_after_tool_call_with_list_of_clarifications() -> None:
    """Test next state when tool call succeeds with a list of clarifications as artifact."""
    tool = AdditionTool()
    tool.should_summarize = True

    clarifications = [
        InputClarification(
            argument_name="test1",
            user_guidance="guidance1",
            plan_run_id=PlanRunUUID(),
        ),
        InputClarification(
            argument_name="test2",
            user_guidance="guidance2",
            plan_run_id=PlanRunUUID(),
        ),
    ]

    messages: list[ToolMessage] = [
        ToolMessage(
            content="Success message",
            tool_call_id="123",
            name="test_tool",
            artifact=clarifications,
        ),
    ]
    state: MessagesState = {"messages": messages}  # type: ignore  # noqa: PGH003

    result = next_state_after_tool_call(get_test_config(), state, tool)

    # Should return END even though tool.should_summarize is True
    # because the message contains a list of clarifications as artifact
    assert result == END

```

## File: tests/unit/execution_agents/test_oneshot_agent.py

```python
"""Test simple agent."""

from __future__ import annotations

from typing import Any

import pytest
from langchain_core.messages import AIMessage, ToolMessage
from langgraph.prebuilt import ToolNode

from portia.errors import InvalidAgentError
from portia.execution_agents.one_shot_agent import OneShotAgent, OneShotToolCallingModel
from portia.execution_agents.output import LocalOutput, Output
from tests.utils import AdditionTool, get_test_config, get_test_plan_run


def test_oneshot_agent_task(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test running an agent without a tool.

    Note: This tests mocks almost everything, but allows us to make sure things
    are running in order and being called correctly and passed out correctly.
    """

    def tool_calling_model(self, state) -> dict[str, Any]:  # noqa: ANN001, ARG001
        response = AIMessage(content="")
        response.tool_calls = [
            {
                "name": "Send_Email_Tool",
                "type": "tool_call",
                "id": "call_3z9rYHY6Rui7rTW0O7N7Wz51",
                "args": {
                    "recipients": ["test@example.com"],
                    "email_title": "Hi",
                    "email_body": "Hi",
                },
            },
        ]
        return {"messages": [response]}

    monkeypatch.setattr(OneShotToolCallingModel, "invoke", tool_calling_model)

    def tool_call(self, input, config) -> dict[str, Any]:  # noqa: A002, ANN001, ARG001
        return {
            "messages": ToolMessage(
                content="Sent email",
                artifact=LocalOutput(value="Sent email with id: 0"),
                tool_call_id="call_3z9rYHY6Rui7rTW0O7N7Wz51",
            ),
        }

    monkeypatch.setattr(ToolNode, "invoke", tool_call)

    (plan, plan_run) = get_test_plan_run()
    agent = OneShotAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=AdditionTool(),
    )

    output = agent.execute_sync()
    assert isinstance(output, Output)
    assert output.get_value() == "Sent email with id: 0"


def test_oneshot_agent_without_tool_raises() -> None:
    """Test oneshot agent without tool raises."""
    (plan, plan_run) = get_test_plan_run()
    with pytest.raises(InvalidAgentError):
        OneShotAgent(
            step=plan.steps[0],
            plan_run=plan_run,
            config=get_test_config(),
            tool=None,
        ).execute_sync()

```

## File: tests/unit/execution_agents/test_context.py

```python
"""test context."""

from datetime import UTC, datetime

import pytest
from pydantic import HttpUrl

from portia.clarification import ActionClarification, InputClarification
from portia.execution_agents.context import build_context
from portia.execution_agents.output import LocalOutput, Output
from portia.execution_context import ExecutionContext, get_execution_context
from portia.plan import Step, Variable
from tests.utils import get_test_plan_run


@pytest.fixture
def inputs() -> list[Variable]:
    """Return a list of inputs for pytest fixtures."""
    return [
        Variable(
            name="$email_address",
            description="Target recipient for email",
        ),
        Variable(name="$email_body", description="Content for email"),
        Variable(name="$email_title", description="Title for email"),
    ]


@pytest.fixture
def outputs() -> dict[str, Output]:
    """Return a dictionary of outputs for pytest fixtures."""
    return {
        "$email_body": LocalOutput(value="The body of the email"),
        "$email_title": LocalOutput(value="Example email"),
        "$email_address": LocalOutput(value="test@example.com"),
        "$london_weather": LocalOutput(value="rainy"),
    }


def test_context_empty() -> None:
    """Test that the context is set up correctly."""
    (_, plan_run) = get_test_plan_run()
    plan_run.outputs.step_outputs = {}
    context = build_context(
        ExecutionContext(),
        Step(inputs=[], output="", task=""),
        plan_run,
    )
    assert "System Context:" in context
    assert len(context) == 42  # length should always be the same


def test_context_execution_context() -> None:
    """Test that the context is set up correctly."""
    (plan, plan_run) = get_test_plan_run()

    context = build_context(
        ExecutionContext(additional_data={"user_id": "123"}),
        plan.steps[0],
        plan_run,
    )
    assert "System Context:" in context
    assert "user_id" in context
    assert "123" in context


def test_context_inputs_and_outputs(inputs: list[Variable], outputs: dict[str, Output]) -> None:
    """Test that the context is set up correctly with inputs and outputs."""
    (plan, plan_run) = get_test_plan_run()
    plan.steps[0].inputs = inputs
    plan_run.outputs.step_outputs = outputs
    context = build_context(
        ExecutionContext(),
        plan.steps[0],
        plan_run,
    )
    for variable in inputs:
        assert variable.name in context
    for name, output in outputs.items():
        assert name in context
        if output.get_value():
            val = output.get_value()
            assert isinstance(val, str)
            assert val in context


def test_all_contexts(inputs: list[Variable], outputs: dict[str, Output]) -> None:
    """Test that the context is set up correctly with all contexts."""
    (plan, plan_run) = get_test_plan_run()
    plan.steps[0].inputs = inputs
    plan_run.outputs.step_outputs = outputs
    clarifications = [
        InputClarification(
            plan_run_id=plan_run.id,
            argument_name="$email_cc",
            user_guidance="email cc list",
            response="bob@bla.com",
            step=0,
        ),
        InputClarification(
            plan_run_id=plan_run.id,
            argument_name="$email_cc",
            user_guidance="email cc list",
            response="bob@bla.com",
            step=1,
        ),
        ActionClarification(
            plan_run_id=plan_run.id,
            action_url=HttpUrl("http://example.com"),
            user_guidance="click on the link",
        ),
    ]
    plan_run.outputs.clarifications = clarifications
    context = build_context(
        ExecutionContext(
            end_user_id="123",
            additional_data={"email": "hello@world.com"},
        ),
        plan.steps[0],
        plan_run,
    )
    # as LLMs are sensitive even to white space formatting we do a complete match here
    assert (
        context
        == f"""Additional context: You MUST use this information to complete your task.
Inputs: the original inputs provided by the planning_agent
input_name: $email_address
input_value: test@example.com
input_description: Target recipient for email
----------
input_name: $email_body
input_value: The body of the email
input_description: Content for email
----------
input_name: $email_title
input_value: Example email
input_description: Title for email
----------
Broader context: This may be useful information from previous steps that can indirectly help you.
output_name: $london_weather
output_value: rainy
----------
Clarifications:
This section contains the user provided response to previous clarifications
for the current step. They should take priority over any other context given.
input_name: $email_cc
clarification_reason: email cc list
input_value: bob@bla.com
----------
Metadata: This section contains general context about this execution.
end_user_id: 123
context_key_name: email context_key_value: hello@world.com
----------
System Context:
Today's date is {datetime.now(UTC).strftime('%Y-%m-%d')}"""
    )


def test_context_inputs_outputs_clarifications(
    inputs: list[Variable],
    outputs: dict[str, Output],
) -> None:
    """Test that the context is set up correctly with inputs, outputs, and missing args."""
    (plan, plan_run) = get_test_plan_run()
    clarifications = [
        InputClarification(
            plan_run_id=plan_run.id,
            argument_name="$email_cc",
            user_guidance="email cc list",
            response="bob@bla.com",
            step=0,
        ),
        ActionClarification(
            plan_run_id=plan_run.id,
            action_url=HttpUrl("http://example.com"),
            user_guidance="click on the link",
            step=1,
        ),
    ]
    plan.steps[0].inputs = inputs
    plan_run.outputs.step_outputs = outputs
    plan_run.outputs.clarifications = clarifications
    context = build_context(
        get_execution_context(),
        plan.steps[0],
        plan_run,
    )
    for variable in inputs:
        assert variable.name in context
    for name, output in outputs.items():
        assert name in context
        if output.get_value():
            val = output.get_value()
            assert isinstance(val, str)
            assert val in context
    assert "email cc list" in context
    assert "bob@bla.com" in context

```

## File: tests/unit/execution_agents/test_output.py

```python
"""Test output."""

from __future__ import annotations

import json
from datetime import UTC, datetime
from typing import Any
from unittest.mock import MagicMock

import pytest
from openai import BaseModel
from pydantic import HttpUrl

from portia.clarification import ActionClarification
from portia.config import LLMModel
from portia.execution_agents.output import AgentMemoryOutput, LocalOutput
from portia.prefixed_uuid import PlanRunUUID
from portia.storage import AgentMemory


class MyModel(BaseModel):
    """Test BaseModel."""

    id: str


class NotAModel:
    """Test class that's not a BaseModel."""

    id: str

    def __init__(self, id: str) -> None:  # noqa: A002
        """Init an instance."""
        self.id = id


not_a_model = NotAModel(id="123")
now = datetime.now(tz=UTC)
clarification = ActionClarification(
    plan_run_id=PlanRunUUID(),
    user_guidance="",
    action_url=HttpUrl("https://example.com"),
)


@pytest.mark.parametrize(
    ("input_value", "expected"),
    [
        pytest.param("Hello World!", "Hello World!", id="string"),
        pytest.param(None, "", id="none"),
        pytest.param({"hello": "world"}, json.dumps({"hello": "world"}), id="dict"),
        pytest.param([{"hello": "world"}], json.dumps([{"hello": "world"}]), id="list"),
        pytest.param(("hello", "world"), json.dumps(["hello", "world"]), id="tuple"),
        pytest.param({"hello"}, json.dumps(["hello"]), id="set"),  # sets don't have ordering
        pytest.param(1, "1", id="int"),
        pytest.param(1.23, "1.23", id="float"),
        pytest.param(False, "false", id="bool"),
        pytest.param(LLMModel.GPT_4_O, str(LLMModel.GPT_4_O.value), id="enum"),
        pytest.param(MyModel(id="123"), MyModel(id="123").model_dump_json(), id="model"),
        pytest.param(b"Hello World!", "Hello World!", id="bytes"),
        pytest.param(now, now.isoformat(), id="datetime"),
        pytest.param(not_a_model, str(not_a_model), id="not_a_model"),
        pytest.param(
            [clarification],
            json.dumps([clarification.model_dump(mode="json")]),
            id="list_of_clarification",
        ),
    ],
)
def test_output_serialize(input_value: Any, expected: Any) -> None:  # noqa: ANN401
    """Test output serialize."""
    output = LocalOutput(value=input_value).serialize_value()
    assert output == expected


def test_local_output() -> None:
    """Test value is held locally."""
    output = LocalOutput(value="test value")
    assert output.get_value() == "test value"

    mock_agent_memory = MagicMock(spec=AgentMemory)
    assert output.full_value(mock_agent_memory) == "test value"
    mock_agent_memory.get_plan_run_output.assert_not_called()


def test_agent_memory_output() -> None:
    """Test value is stored in agent memory."""
    output = AgentMemoryOutput(
        output_name="test_value",
        plan_run_id=PlanRunUUID(),
        summary="test summary",
    )
    assert output.get_value() == "test summary"
    assert output.get_summary() == "test summary"
    assert output.serialize_value() == "test summary"

    mock_agent_memory = MagicMock()
    mock_agent_memory.get_plan_run_output.return_value = LocalOutput(value="retrieved value")

    result = output.full_value(mock_agent_memory)
    assert result == "retrieved value"
    mock_agent_memory.get_plan_run_output.assert_called_once_with(
        output.output_name,
        output.plan_run_id,
    )

```

## File: tests/unit/execution_agents/test_base_execution_agent.py

```python
"""Test simple agent."""

from __future__ import annotations

import json
from datetime import UTC, datetime
from typing import Any

from openai import BaseModel
from pydantic import HttpUrl

from portia.clarification import ActionClarification
from portia.config import LLMModel
from portia.execution_agents.base_execution_agent import BaseExecutionAgent
from portia.execution_agents.output import LocalOutput
from portia.prefixed_uuid import PlanRunUUID
from tests.utils import get_test_config, get_test_plan_run


def test_base_agent_default_context() -> None:
    """Test default context."""
    plan, plan_run = get_test_plan_run()
    agent = BaseExecutionAgent(
        plan.steps[0],
        plan_run,
        get_test_config(),
        None,
    )
    context = agent.get_system_context()
    assert context is not None


def test_output_serialize() -> None:
    """Test output serialize."""

    class MyModel(BaseModel):
        id: str

    class NotAModel:
        id: str

        def __init__(self, id: str) -> None:  # noqa: A002
            self.id = id

    not_a_model = NotAModel(id="123")
    now = datetime.now(tz=UTC)
    clarification = ActionClarification(
        plan_run_id=PlanRunUUID(),
        user_guidance="",
        action_url=HttpUrl("https://example.com"),
    )

    tcs: list[tuple[Any, Any]] = [
        ("Hello World!", "Hello World!"),
        (None, ""),
        ({"hello": "world"}, json.dumps({"hello": "world"})),
        ([{"hello": "world"}], json.dumps([{"hello": "world"}])),
        (("hello", "world"), json.dumps(["hello", "world"])),
        ({"hello"}, json.dumps(["hello"])),  # sets don't have ordering
        (1, "1"),
        (1.23, "1.23"),
        (False, "false"),
        (LLMModel.GPT_4_O, str(LLMModel.GPT_4_O.value)),
        (MyModel(id="123"), MyModel(id="123").model_dump_json()),
        (b"Hello World!", "Hello World!"),
        (now, now.isoformat()),
        (not_a_model, str(not_a_model)),
        ([clarification], json.dumps([clarification.model_dump(mode="json")])),
    ]

    for tc in tcs:
        output = LocalOutput(value=tc[0]).serialize_value()
        assert output == tc[1]

```

## File: tests/unit/execution_agents/test_default_execution_agent.py

```python
"""test default execution agent."""

from __future__ import annotations

import logging
from types import SimpleNamespace
from typing import Any
from unittest import mock

import pytest
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.graph import END
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, Field

from portia.clarification import InputClarification
from portia.errors import InvalidAgentError, InvalidPlanRunStateError
from portia.execution_agents.default_execution_agent import (
    MAX_RETRIES,
    DefaultExecutionAgent,
    ParserModel,
    ToolArgument,
    ToolCallingModel,
    ToolInputs,
    VerifiedToolArgument,
    VerifiedToolInputs,
    VerifierModel,
)
from portia.execution_agents.output import LocalOutput, Output
from portia.model import LangChainGenerativeModel
from portia.plan import Step
from portia.tool import Tool
from tests.utils import (
    AdditionTool,
    get_mock_base_chat_model,
    get_mock_langchain_generative_model,
    get_test_config,
    get_test_plan_run,
    get_test_tool_context,
)


@pytest.fixture(scope="session", autouse=True)
def _setup() -> None:
    logging.basicConfig(level=logging.INFO)


class _TestToolSchema(BaseModel):
    """Input for TestTool."""

    content: str = Field(..., description="INPUT_DESCRIPTION")


def test_parser_model() -> None:
    """Test the parser model."""
    tool_inputs = ToolInputs(
        args=[
            ToolArgument(
                name="content",
                value="CONTENT_STRING",
                valid=True,
                explanation="EXPLANATION_STRING",
            ),
        ],
    )
    mock_model = get_mock_base_chat_model(response=tool_inputs)

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=_TestToolSchema.model_json_schema,
        args_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
    )
    parser_model = ParserModel(
        model=LangChainGenerativeModel(client=mock_model, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    parser_model.invoke({})  # type: ignore  # noqa: PGH003

    assert mock_model.invoke.called
    messages = mock_model.invoke.call_args[0][0]
    assert messages
    assert "You are a highly capable assistant" in messages[0].content  # type: ignore  # noqa: PGH003
    assert "CONTEXT_STRING" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "DESCRIPTION_STRING" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_NAME" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_DESCRIPTION" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "INPUT_DESCRIPTION" in messages[1].content  # type: ignore  # noqa: PGH003
    assert mock_model.with_structured_output.called
    assert mock_model.with_structured_output.call_args[0][0] == ToolInputs


def test_parser_model_with_retries() -> None:
    """Test the parser model with retries."""
    tool_inputs = ToolInputs(
        args=[],
    )
    mock_invoker = get_mock_base_chat_model(response=tool_inputs)

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=_TestToolSchema.model_json_schema,
        args_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
    )
    parser_model = ParserModel(
        model=LangChainGenerativeModel(client=mock_invoker, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )

    with mock.patch.object(parser_model, "invoke", side_effect=parser_model.invoke) as mock_invoke:
        parser_model.invoke({})  # type: ignore  # noqa: PGH003

    assert mock_invoke.call_count == MAX_RETRIES + 1


def test_parser_model_with_retries_invalid_structured_response() -> None:
    """Test the parser model handling of invalid JSON and retries."""
    mock_model = get_mock_base_chat_model(
        response="NOT_A_PYDANTIC_MODEL_INSTANCE",
    )

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=_TestToolSchema.model_json_schema,
        args_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
    )
    parser_model = ParserModel(
        model=LangChainGenerativeModel(client=mock_model, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )

    with mock.patch.object(parser_model, "invoke", side_effect=parser_model.invoke) as mock_invoke:
        parser_model.invoke({"messages": []})  # type: ignore  # noqa: PGH003

    assert mock_invoke.call_count == MAX_RETRIES + 1


def test_parser_model_with_invalid_args() -> None:
    """Test the parser model handling of invalid arguments and retries."""
    # First response contains one valid and one invalid argument
    invalid_tool_inputs = ToolInputs(
        args=[
            ToolArgument(
                name="content",
                value="VALID_CONTENT",
                valid=True,
                explanation="Valid content string",
            ),
            ToolArgument(
                name="number",
                value=42,
                valid=False,
                explanation="The number should be more than 42",
            ),
        ],
    )

    # Second response contains all valid arguments
    valid_tool_inputs = ToolInputs(
        args=[
            ToolArgument(
                name="content",
                value="VALID_CONTENT",
                valid=True,
                explanation="Valid content string",
            ),
            ToolArgument(
                name="number",
                value=43,
                valid=True,
                explanation="Valid number value",
            ),
        ],
    )

    responses = [invalid_tool_inputs, valid_tool_inputs]
    current_response_index = 0

    def mock_invoke(*_, **__):  # noqa: ANN002, ANN003, ANN202
        nonlocal current_response_index
        response = responses[current_response_index]
        current_response_index += 1
        return response

    mock_model = get_mock_base_chat_model(response=None)
    mock_model.invoke.side_effect = mock_invoke

    class TestSchema(BaseModel):
        content: str
        number: int

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=TestSchema.model_json_schema,
        args_schema=TestSchema,
        description="TOOL_DESCRIPTION",
    )

    parser_model = ParserModel(
        model=LangChainGenerativeModel(client=mock_model, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )

    # First call should store the error and retry
    result = parser_model.invoke({"messages": []})

    # Verify that the error was stored
    assert len(parser_model.previous_errors) == 1
    assert (
        parser_model.previous_errors[0]
        == "Error in argument number: The number should be more than 42\n"
    )

    # Verify that we got the valid response after retry
    result_inputs = ToolInputs.model_validate_json(result["messages"][0])
    assert len(result_inputs.args) == 2

    # Check both arguments in final result
    content_arg = next(arg for arg in result_inputs.args if arg.name == "content")
    number_arg = next(arg for arg in result_inputs.args if arg.name == "number")

    assert content_arg.valid
    assert content_arg.value == "VALID_CONTENT"
    assert number_arg.valid
    assert number_arg.value == 43


def test_verifier_model() -> None:
    """Test the verifier model."""
    tool_inputs = ToolInputs(
        args=[
            ToolArgument(
                name="content",
                value="CONTENT_STRING",
                valid=True,
                explanation="EXPLANATION_STRING",
            ),
        ],
    )
    verified_tool_inputs = VerifiedToolInputs(
        args=[VerifiedToolArgument(name="content", value="CONTENT_STRING", made_up=False)],
    )
    mock_model = get_mock_base_chat_model(response=verified_tool_inputs)

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
        args_json_schema=_TestToolSchema.model_json_schema,
    )
    verifier_model = VerifierModel(
        model=LangChainGenerativeModel(client=mock_model, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    verifier_model.invoke({"messages": [AIMessage(content=tool_inputs.model_dump_json(indent=2))]})

    assert mock_model.invoke.called  # type: ignore[reportFunctionMemberAccess]
    messages = mock_model.invoke.call_args[0][0]  # type: ignore[reportFunctionMemberAccess]
    assert "You are an expert reviewer" in messages[0].content  # type: ignore  # noqa: PGH003
    assert "CONTEXT_STRING" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "DESCRIPTION_STRING" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_NAME" in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_DESCRIPTION" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "INPUT_DESCRIPTION" in messages[1].content  # type: ignore  # noqa: PGH003
    assert mock_model.with_structured_output.called
    assert mock_model.with_structured_output.call_args[0][0] == VerifiedToolInputs


def test_verifier_model_schema_validation() -> None:
    """Test the verifier model schema validation."""

    class TestSchema(BaseModel):
        required_field1: str
        required_field2: int
        optional_field: str | None = None

    verified_tool_inputs = VerifiedToolInputs(
        args=[
            VerifiedToolArgument(name="required_field1", value=None, schema_invalid=True),
            VerifiedToolArgument(name="required_field2", value=None, schema_invalid=True),
            VerifiedToolArgument(name="optional_field", value=None, schema_invalid=False),
        ],
    )
    mock_model = get_mock_base_chat_model(response=verified_tool_inputs)

    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_schema=TestSchema,
        description="TOOL_DESCRIPTION",
        args_json_schema=_TestToolSchema.model_json_schema,
    )
    verifier_model = VerifierModel(
        model=LangChainGenerativeModel(client=mock_model, model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )

    result = verifier_model.invoke(
        {"messages": [AIMessage(content=verified_tool_inputs.model_dump_json(indent=2))]},
    )

    result_inputs = VerifiedToolInputs.model_validate_json(result["messages"][0])

    required_field1 = next(arg for arg in result_inputs.args if arg.name == "required_field1")
    required_field2 = next(arg for arg in result_inputs.args if arg.name == "required_field2")
    assert (
        required_field1.schema_invalid
    ), "required_field1 should be marked as missing when validation fails"
    assert (
        required_field2.schema_invalid
    ), "required_field2 should be marked as missing when validation fails"

    optional_field = next(arg for arg in result_inputs.args if arg.name == "optional_field")
    assert (
        not optional_field.schema_invalid
    ), "optional_field should not be marked as missing when validation fails"


def test_tool_calling_model_no_hallucinations() -> None:
    """Test the tool calling model."""
    verified_tool_inputs = VerifiedToolInputs(
        args=[VerifiedToolArgument(name="content", value="CONTENT_STRING", made_up=False)],
    )
    mock_model = get_mock_langchain_generative_model(
        SimpleNamespace(tool_calls=[{"name": "add_tool", "args": "CALL_ARGS"}]),
    )

    (_, plan_run) = get_test_plan_run()
    agent = SimpleNamespace(
        verified_args=verified_tool_inputs,
        clarifications=[],
    )
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.plan_run = plan_run
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
    )
    tool_calling_model = ToolCallingModel(
        model=mock_model,
        context="CONTEXT_STRING",
        tools=[AdditionTool().to_langchain_with_artifact(ctx=get_test_tool_context())],
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    tool_calling_model.invoke({"messages": []})

    base_chat_model = mock_model.to_langchain()
    assert base_chat_model.invoke.called  # type: ignore[reportFunctionMemberAccess]
    messages = base_chat_model.invoke.call_args[0][0]  # type: ignore[reportFunctionMemberAccess]
    assert "You are very powerful assistant" in messages[0].content  # type: ignore  # noqa: PGH003
    assert "CONTEXT_STRING" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "DESCRIPTION_STRING" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_NAME" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_DESCRIPTION" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "INPUT_DESCRIPTION" not in messages[1].content  # type: ignore  # noqa: PGH003


def test_tool_calling_model_with_hallucinations() -> None:
    """Test the tool calling model."""
    verified_tool_inputs = VerifiedToolInputs(
        args=[VerifiedToolArgument(name="content", value="CONTENT_STRING", made_up=True)],
    )
    mock_model = get_mock_langchain_generative_model(
        SimpleNamespace(tool_calls=[{"name": "add_tool", "args": "CALL_ARGS"}]),
    )

    (_, plan_run) = get_test_plan_run()

    clarification = InputClarification(
        plan_run_id=plan_run.id,
        user_guidance="USER_GUIDANCE",
        response="CLARIFICATION_RESPONSE",
        argument_name="content",
        resolved=True,
    )

    failed_clarification = InputClarification(
        plan_run_id=plan_run.id,
        user_guidance="USER_GUIDANCE_FAILED",
        response="FAILED",
        argument_name="content",
        resolved=True,
    )

    plan_run.outputs.clarifications = [clarification]
    agent = SimpleNamespace(
        verified_args=verified_tool_inputs,
        clarifications=[failed_clarification, clarification],
        missing_args={"content": clarification},
        get_last_resolved_clarification=lambda arg_name: clarification
        if arg_name == "content"
        else None,
    )
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.plan_run = plan_run
    agent.tool = SimpleNamespace(
        id="TOOL_ID",
        name="TOOL_NAME",
        args_json_schema=_TestToolSchema,
        description="TOOL_DESCRIPTION",
    )
    tool_calling_model = ToolCallingModel(
        model=mock_model,
        context="CONTEXT_STRING",
        tools=[AdditionTool().to_langchain_with_artifact(ctx=get_test_tool_context())],
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    tool_calling_model.invoke({"messages": []})

    base_chat_model = mock_model.to_langchain()
    assert base_chat_model.invoke.called  # type: ignore[reportFunctionMemberAccess]
    messages = base_chat_model.invoke.call_args[0][0]  # type: ignore[reportFunctionMemberAccess]
    assert "You are very powerful assistant" in messages[0].content  # type: ignore  # noqa: PGH003
    assert "CONTEXT_STRING" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "DESCRIPTION_STRING" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_NAME" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "TOOL_DESCRIPTION" not in messages[1].content  # type: ignore  # noqa: PGH003
    assert "INPUT_DESCRIPTION" not in messages[1].content  # type: ignore  # noqa: PGH003


def test_basic_agent_task(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test running an agent without a tool.

    Note: This tests mocks almost everything, but allows us to make sure things
    are running in order and being called correctly and passed out correctly.
    """
    tool_inputs = ToolInputs(
        args=[
            ToolArgument(
                name="email_address",
                valid=True,
                value="test@example.com",
                explanation="It's an email address.",
            ),
        ],
    )
    verified_tool_inputs = VerifiedToolInputs(
        args=[
            VerifiedToolArgument(name="email_address", value="test@example.com", made_up=False),
        ],
    )

    tool = AdditionTool()

    def parser_model(self, state):  # noqa: ANN001, ANN202, ARG001
        return {"messages": [tool_inputs.model_dump_json(indent=2)]}

    monkeypatch.setattr(ParserModel, "invoke", parser_model)

    def verifier_model(self, state):  # noqa: ANN001, ANN202, ARG001
        self.agent.verified_args = verified_tool_inputs
        return {"messages": [verified_tool_inputs.model_dump_json(indent=2)]}

    monkeypatch.setattr(VerifierModel, "invoke", verifier_model)

    def tool_calling_model(self, state):  # noqa: ANN001, ANN202, ARG001
        response = AIMessage(content="")
        response.tool_calls = [
            {
                "name": "add_tool",
                "type": "tool_call",
                "id": "call_3z9rYHY6Rui7rTW0O7N7Wz51",
                "args": {
                    "recipients": ["test@example.com"],
                    "email_title": "Hi",
                    "email_body": "Hi",
                },
            },
        ]
        return {"messages": [response]}

    monkeypatch.setattr(ToolCallingModel, "invoke", tool_calling_model)

    def tool_call(self, input, config):  # noqa: A002, ANN001, ANN202, ARG001
        return {
            "messages": ToolMessage(
                content="Sent email",
                artifact=LocalOutput(value="Sent email with id: 0"),
                tool_call_id="call_3z9rYHY6Rui7rTW0O7N7Wz51",
            ),
        }

    monkeypatch.setattr(ToolNode, "invoke", tool_call)

    (plan, plan_run) = get_test_plan_run()
    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=tool,
    )

    output = agent.execute_sync()
    assert isinstance(output, Output)
    assert output.get_value() == "Sent email with id: 0"


def test_basic_agent_task_with_verified_args(monkeypatch: pytest.MonkeyPatch) -> None:
    """Test running an agent with verified args.

    Note: This tests mocks almost everything, but allows us to make sure things
    are running in order and being called correctly and passed out correctly.
    """
    verified_tool_inputs = VerifiedToolInputs(
        args=[
            VerifiedToolArgument(name="email_address", value="test@example.com", made_up=False),
        ],
    )

    tool = AdditionTool()

    def tool_calling_model(self, state):  # noqa: ANN001, ANN202, ARG001
        response = AIMessage(content="")
        response.tool_calls = [
            {
                "name": "add_tool",
                "type": "tool_call",
                "id": "call_3z9rYHY6Rui7rTW0O7N7Wz51",
                "args": {
                    "recipients": ["test@example.com"],
                    "email_title": "Hi",
                    "email_body": "Hi",
                },
            },
        ]
        return {"messages": [response]}

    monkeypatch.setattr(ToolCallingModel, "invoke", tool_calling_model)

    def tool_call(self, input, config):  # noqa: A002, ANN001, ANN202, ARG001
        return {
            "messages": ToolMessage(
                content="Sent email",
                artifact=LocalOutput(value="Sent email with id: 0"),
                tool_call_id="call_3z9rYHY6Rui7rTW0O7N7Wz51",
            ),
        }

    monkeypatch.setattr(ToolNode, "invoke", tool_call)

    (plan, plan_run) = get_test_plan_run()
    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=tool,
    )
    agent.verified_args = verified_tool_inputs

    output = agent.execute_sync()
    assert isinstance(output, Output)
    assert output.get_value() == "Sent email with id: 0"


def test_default_execution_agent_edge_cases() -> None:
    """Tests edge cases are handled."""
    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    agent.tool = None
    parser_model = ParserModel(
        model=get_mock_langchain_generative_model(get_mock_base_chat_model()),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    with pytest.raises(InvalidPlanRunStateError):
        parser_model.invoke({"messages": []})

    agent.verified_args = None
    tool_calling_model = ToolCallingModel(
        model=get_mock_langchain_generative_model(get_mock_base_chat_model()),
        context="CONTEXT_STRING",
        tools=[AdditionTool().to_langchain_with_artifact(ctx=get_test_tool_context())],
        agent=agent,  # type: ignore  # noqa: PGH003
    )
    with pytest.raises(InvalidPlanRunStateError):
        tool_calling_model.invoke({"messages": []})


def test_get_last_resolved_clarification() -> None:
    """Test get_last_resolved_clarification."""
    (plan, plan_run) = get_test_plan_run()
    resolved_clarification1 = InputClarification(
        plan_run_id=plan_run.id,
        argument_name="arg",
        response="2",
        user_guidance="FAILED",
        resolved=True,
        step=0,
    )
    resolved_clarification2 = InputClarification(
        plan_run_id=plan_run.id,
        argument_name="arg",
        response="2",
        user_guidance="SUCCESS",
        resolved=True,
        step=0,
    )
    unresolved_clarification = InputClarification(
        plan_run_id=plan_run.id,
        argument_name="arg",
        response="2",
        user_guidance="",
        resolved=False,
        step=0,
    )
    plan_run.outputs.clarifications = [
        resolved_clarification1,
        resolved_clarification2,
        unresolved_clarification,
    ]
    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=None,
    )
    assert agent.get_last_resolved_clarification("arg") == resolved_clarification2


def test_clarifications_or_continue() -> None:
    """Test clarifications_or_continue."""
    (plan, plan_run) = get_test_plan_run()
    clarification = InputClarification(
        plan_run_id=plan_run.id,
        argument_name="arg",
        response="2",
        user_guidance="",
        resolved=True,
    )

    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=None,
    )
    inputs = VerifiedToolInputs(
        args=[
            VerifiedToolArgument(name="arg", value="1", made_up=True),
        ],
    )

    # when clarifications don't match expect a new one
    output = agent.clarifications_or_continue(
        {
            "messages": [
                HumanMessage(
                    content=inputs.model_dump_json(indent=2),
                ),
            ],
        },
    )
    assert output == END
    assert isinstance(agent.new_clarifications, list)
    assert isinstance(agent.new_clarifications[0], InputClarification)

    # when clarifications match expect to call tools
    clarification = InputClarification(
        plan_run_id=plan_run.id,
        argument_name="arg",
        response="1",
        user_guidance="",
        resolved=True,
        step=0,
    )

    (plan, plan_run) = get_test_plan_run()
    plan_run.outputs.clarifications = [clarification]
    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=None,
    )

    inputs = VerifiedToolInputs(
        args=[
            VerifiedToolArgument(name="arg", value="1", made_up=True),
        ],
    )

    output = agent.clarifications_or_continue(
        {
            "messages": [
                HumanMessage(
                    content=inputs.model_dump_json(indent=2),
                ),
            ],
        },
    )
    assert output == "tool_agent"
    assert isinstance(agent.new_clarifications, list)
    assert len(agent.new_clarifications) == 0


def test_default_execution_agent_none_tool_execute_sync() -> None:
    """Test that executing DefaultExecutionAgent with None tool raises an exception."""
    (plan, plan_run) = get_test_plan_run()

    agent = DefaultExecutionAgent(
        step=plan.steps[0],
        plan_run=plan_run,
        config=get_test_config(),
        tool=None,
    )

    with pytest.raises(InvalidAgentError) as exc_info:
        agent.execute_sync()

    assert "Tool is required for DefaultExecutionAgent" in str(exc_info.value)


class MockToolSchema(BaseModel):
    """Mock tool schema."""

    optional_arg: str | None = Field(default=None, description="An optional argument")


class MockAgent:
    """Mock agent."""

    def __init__(self) -> None:
        """Init mock agent."""
        self.tool = MockTool()


class MockTool(Tool):
    """Mock tool."""

    def __init__(self) -> None:
        """Init mock tool."""
        super().__init__(
            name="Mock Tool",
            id="mock_tool",
            description="Mock tool description",
            args_schema=MockToolSchema,
            output_schema=("type", "A description of the output"),
        )

    def run(self, **kwargs: Any) -> Any:  # noqa: ANN401, ARG002
        """Run mock tool."""
        return "RUN_RESULT"


def test_optional_args_with_none_values() -> None:
    """Test that optional args with None values are handled correctly.

    Required args with None values should always be marked made_up.
    Optional args with None values should be marked not made_up.
    """
    agent = DefaultExecutionAgent(
        step=Step(task="TASK_STRING", output="$out"),
        plan_run=get_test_plan_run()[1],
        config=get_test_config(),
        tool=MockTool(),
    )
    model = VerifierModel(
        model=LangChainGenerativeModel(client=get_mock_base_chat_model(), model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,
    )

    #  Optional arg and made_up is True == not made_up
    updated_tool_inputs = model._validate_args_against_schema(  # noqa: SLF001
        VerifiedToolInputs(
            args=[VerifiedToolArgument(name="optional_arg", value=None, made_up=True)],
        ),
    )
    assert updated_tool_inputs.args[0].made_up is False

    #  Optional arg and made_up is False == mnot ade_up
    updated_tool_inputs = model._validate_args_against_schema(  # noqa: SLF001
        VerifiedToolInputs(
            args=[VerifiedToolArgument(name="optional_arg", value=None, made_up=False)],
        ),
    )
    assert updated_tool_inputs.args[0].made_up is False


def test_verifier_model_edge_cases() -> None:
    """Tests edge cases are handled."""
    agent = SimpleNamespace()
    agent.step = Step(task="DESCRIPTION_STRING", output="$out")
    verifier_model = VerifierModel(
        model=LangChainGenerativeModel(client=get_mock_base_chat_model(), model_name="test"),
        context="CONTEXT_STRING",
        agent=agent,  # type: ignore  # noqa: PGH003
    )

    # Check error with no tool specified
    agent.tool = None
    with pytest.raises(InvalidPlanRunStateError):
        verifier_model.invoke({"messages": []})

```

## File: tests/unit/execution_agents/utils/test_final_output_summarizer.py

```python
"""Tests for the SummarizerAgent."""

from unittest import mock

import pytest

from portia.config import SUMMARISER_MODEL_KEY, Config
from portia.execution_agents.output import LocalOutput
from portia.execution_agents.utils.final_output_summarizer import FinalOutputSummarizer
from portia.introspection_agents.introspection_agent import PreStepIntrospectionOutcome
from portia.model import GenerativeModel, Message
from portia.plan import Step
from tests.utils import get_test_config, get_test_plan_run


@pytest.fixture
def mock_summarizer_model() -> mock.MagicMock:
    """Mock the summarizer model."""
    return mock.MagicMock(spec=GenerativeModel)


@pytest.fixture
def summarizer_config(mock_summarizer_model: mock.MagicMock) -> Config:
    """Create a summarizer config with a mocked model."""
    return get_test_config(custom_models={SUMMARISER_MODEL_KEY: mock_summarizer_model})


def test_summarizer_agent_execute_sync(
    summarizer_config: Config,
    mock_summarizer_model: mock.MagicMock,
) -> None:
    """Test that the summarizer agent correctly executes and returns a summary."""
    # Set up test data
    (plan, plan_run) = get_test_plan_run()
    plan.plan_context.query = "What's the weather in London and what can I do?"
    plan.steps = [
        Step(
            task="Get weather in London",
            output="$london_weather",
        ),
        Step(
            task="Suggest activities based on weather",
            output="$activities",
        ),
    ]

    plan_run.outputs.step_outputs = {
        "$london_weather": LocalOutput(value="Sunny and warm"),
        "$activities": LocalOutput(value="Visit Hyde Park and have a picnic"),
    }

    # Mock LLM response
    expected_summary = "Weather is sunny and warm in London, visit to Hyde Park for a picnic"
    mock_summarizer_model.get_response.return_value = Message(
        content=expected_summary,
        role="assistant",
    )

    summarizer = FinalOutputSummarizer(config=summarizer_config)
    output = summarizer.create_summary(plan=plan, plan_run=plan_run)

    assert output == expected_summary

    # Verify LLM was called with correct prompt
    expected_context = (
        "Query: What's the weather in London and what can I do?\n"
        "----------\n"
        "Task: Get weather in London\n"
        "Output: Sunny and warm\n"
        "----------\n"
        "Task: Suggest activities based on weather\n"
        "Output: Visit Hyde Park and have a picnic\n"
        "----------"
    )
    expected_prompt = FinalOutputSummarizer.SUMMARIZE_TASK + expected_context
    mock_summarizer_model.get_response.assert_called_once_with(
        [Message(content=expected_prompt, role="user")],
    )


def test_summarizer_agent_empty_plan_run(
    summarizer_config: Config,
    mock_summarizer_model: mock.MagicMock,
) -> None:
    """Test summarizer agent with empty plan run."""
    (plan, plan_run) = get_test_plan_run()
    plan.plan_context.query = "Empty query"
    plan.steps = []
    plan_run.outputs.step_outputs = {}

    mock_summarizer_model.get_response.return_value = Message(
        content="Empty summary",
        role="assistant",
    )

    summarizer = FinalOutputSummarizer(config=summarizer_config)

    output = summarizer.create_summary(plan=plan, plan_run=plan_run)

    # Verify empty context case
    assert output == "Empty summary"
    expected_prompt = FinalOutputSummarizer.SUMMARIZE_TASK + ("Query: Empty query\n----------")
    mock_summarizer_model.get_response.assert_called_once_with(
        [Message(content=expected_prompt, role="user")],
    )


def test_summarizer_agent_handles_empty_response(
    summarizer_config: Config,
    mock_summarizer_model: mock.MagicMock,
) -> None:
    """Test that the agent handles None response from LLM."""
    (plan, plan_run) = get_test_plan_run()
    plan.plan_context.query = "Test query"

    mock_summarizer_model.get_response.return_value = Message(content="", role="assistant")

    summarizer = FinalOutputSummarizer(config=summarizer_config)
    output = summarizer.create_summary(plan=plan, plan_run=plan_run)

    # Verify None handling
    assert output is None


def test_build_tasks_and_outputs_context(
    summarizer_config: Config,
) -> None:
    """Test that the tasks and outputs context is built correctly."""
    (plan, plan_run) = get_test_plan_run()

    # Set up test data
    plan.plan_context.query = "What's the weather in London and what can I do?"
    plan.steps = [
        Step(
            task="Get weather in London",
            output="$london_weather",
        ),
        Step(
            task="Suggest activities based on weather",
            output="$activities",
        ),
    ]

    plan_run.outputs.step_outputs = {
        "$london_weather": LocalOutput(value="Sunny and warm"),
        "$activities": LocalOutput(value="Visit Hyde Park and have a picnic"),
    }

    summarizer = FinalOutputSummarizer(config=summarizer_config)
    context = summarizer._build_tasks_and_outputs_context(  # noqa: SLF001
        plan=plan,
        plan_run=plan_run,
    )

    # Verify exact output format including query
    assert context == (
        "Query: What's the weather in London and what can I do?\n"
        "----------\n"
        "Task: Get weather in London\n"
        "Output: Sunny and warm\n"
        "----------\n"
        "Task: Suggest activities based on weather\n"
        "Output: Visit Hyde Park and have a picnic\n"
        "----------"
    )


def test_build_tasks_and_outputs_context_empty() -> None:
    """Test that the tasks and outputs context handles empty steps and outputs."""
    (plan, plan_run) = get_test_plan_run()

    # Empty plan and run
    plan.plan_context.query = "Empty query"
    plan.steps = []
    plan_run.outputs.step_outputs = {}

    summarizer = FinalOutputSummarizer(config=get_test_config())
    context = summarizer._build_tasks_and_outputs_context(  # noqa: SLF001
        plan=plan,
        plan_run=plan_run,
    )

    # Should still include query even if no steps/outputs
    assert context == ("Query: Empty query\n----------")


def test_build_tasks_and_outputs_context_partial_outputs() -> None:
    """Test that the context builder handles steps with missing outputs."""
    (plan, plan_run) = get_test_plan_run()

    # Set up test data with missing output
    plan.plan_context.query = "What's the weather in London?"
    plan.steps = [
        Step(
            task="Get weather in London",
            output="$london_weather",
        ),
        Step(
            task="Suggest activities based on weather",
            output="$activities",
        ),
    ]

    # Only provide output for first step
    plan_run.outputs.step_outputs = {
        "$london_weather": LocalOutput(value="Sunny and warm"),
    }

    summarizer = FinalOutputSummarizer(config=get_test_config())
    context = summarizer._build_tasks_and_outputs_context(  # noqa: SLF001
        plan=plan,
        plan_run=plan_run,
    )

    # Verify only step with output is included, but query is always present
    assert context == (
        "Query: What's the weather in London?\n"
        "----------\n"
        "Task: Get weather in London\n"
        "Output: Sunny and warm\n"
        "----------"
    )


def test_build_tasks_and_outputs_context_with_conditional_outcomes() -> None:
    """Test that the context builder correctly uses summary for conditional outcomes."""
    (plan, plan_run) = get_test_plan_run()

    plan.plan_context.query = "Test query with conditional outcomes"
    plan.steps = [
        Step(
            task="Regular task",
            output="$regular_output",
        ),
        Step(
            task="Failed task",
            output="$failed_output",
        ),
        Step(
            task="Skipped task",
            output="$skipped_output",
        ),
        Step(
            task="Complete task",
            output="$complete_output",
        ),
    ]

    plan_run.outputs.step_outputs = {
        "$regular_output": LocalOutput(value="Regular result", summary="Not used"),
        "$failed_output": LocalOutput(
            value=PreStepIntrospectionOutcome.FAIL,
            summary="This task failed due to an error",
        ),
        "$skipped_output": LocalOutput(
            value=PreStepIntrospectionOutcome.SKIP,
            summary="This task was skipped as it was unnecessary",
        ),
        "$complete_output": LocalOutput(
            value=PreStepIntrospectionOutcome.COMPLETE,
            summary="The plan execution was completed early",
        ),
    }

    summarizer = FinalOutputSummarizer(config=get_test_config())
    context = summarizer._build_tasks_and_outputs_context(  # noqa: SLF001
        plan=plan,
        plan_run=plan_run,
    )

    assert context == (
        "Query: Test query with conditional outcomes\n"
        "----------\n"
        "Task: Regular task\n"
        "Output: Regular result\n"
        "----------\n"
        "Task: Failed task\n"
        "Output: This task failed due to an error\n"
        "----------\n"
        "Task: Skipped task\n"
        "Output: This task was skipped as it was unnecessary\n"
        "----------\n"
        "Task: Complete task\n"
        "Output: The plan execution was completed early\n"
        "----------"
    )

```

## File: tests/unit/execution_agents/utils/test_step_summarizer.py

```python
"""Test summarizer model."""

from __future__ import annotations

from langchain_core.messages import AIMessage, ToolMessage

from portia.config import FEATURE_FLAG_AGENT_MEMORY_ENABLED
from portia.execution_agents.output import LocalOutput
from portia.execution_agents.utils.step_summarizer import StepSummarizer
from portia.plan import Step
from tests.utils import (
    AdditionTool,
    get_mock_langchain_generative_model,
    get_test_config,
)


def test_summarizer_model_normal_output() -> None:
    """Test the summarizer model with valid tool message."""
    summary = AIMessage(content="Short summary")
    tool = AdditionTool()
    mock_model = get_mock_langchain_generative_model(response=summary)
    tool_message = ToolMessage(
        content="Tool output content",
        tool_call_id="123",
        name=tool.name,
        artifact=LocalOutput(value="Tool output value"),
    )

    summarizer_model = StepSummarizer(
        config=get_test_config(),
        model=mock_model,
        tool=tool,
        step=Step(task="Test task", output="$output"),
    )
    base_chat_model = mock_model.to_langchain()
    result = summarizer_model.invoke({"messages": [tool_message]})

    assert base_chat_model.invoke.called  # type: ignore[reportFunctionMemberAccess]
    messages = base_chat_model.invoke.call_args[0][0]  # type: ignore[reportFunctionMemberAccess]
    assert messages
    assert "You are a highly skilled summarizer" in messages[0].content
    assert "Tool output content" in messages[1].content

    # Check that summaries were added to the artifact
    output_message = result["messages"][0]
    assert isinstance(output_message, ToolMessage)
    assert output_message.artifact.summary == "Short summary"


def test_summarizer_model_non_tool_message() -> None:
    """Test the summarizer model with non-tool message should not invoke the LLM."""
    mock_model = get_mock_langchain_generative_model()
    ai_message = AIMessage(content="AI message content")

    summarizer_model = StepSummarizer(
        config=get_test_config(),
        model=mock_model,
        tool=AdditionTool(),
        step=Step(task="Test task", output="$output"),
    )
    result = summarizer_model.invoke({"messages": [ai_message]})

    assert not mock_model.to_langchain().invoke.called  # type: ignore[reportFunctionMemberAccess]
    assert result["messages"][0] == ai_message


def test_summarizer_model_no_messages() -> None:
    """Test the summarizer model with empty message list should not invoke the LLM."""
    mock_model = get_mock_langchain_generative_model()

    summarizer_model = StepSummarizer(
        config=get_test_config(),
        model=mock_model,
        tool=AdditionTool(),
        step=Step(task="Test task", output="$output"),
    )
    result = summarizer_model.invoke({"messages": []})

    assert not mock_model.to_langchain().invoke.called  # type: ignore[reportFunctionMemberAccess]
    assert result["messages"] == [None]


def test_summarizer_model_large_output() -> None:
    """Test the summarizer model with large output."""
    summary = AIMessage(content="Short summary")
    mock_model = get_mock_langchain_generative_model(response=summary)
    tool_message = ToolMessage(
        content="Test " * 1000,
        tool_call_id="123",
        name="test_tool",
        artifact=LocalOutput(value="Test " * 1000),
    )

    summarizer_model = StepSummarizer(
        # Set a low threshold so the above output is considered large
        config=get_test_config(
            large_output_threshold_tokens=100,
            feature_flags={
                FEATURE_FLAG_AGENT_MEMORY_ENABLED: True,
            },
        ),
        model=mock_model,
        tool=AdditionTool(),
        step=Step(task="Test task", output="$output"),
    )
    base_chat_model = mock_model.to_langchain()
    result = summarizer_model.invoke({"messages": [tool_message]})

    assert base_chat_model.invoke.called  # type: ignore[reportFunctionMemberAccess]
    messages = base_chat_model.invoke.call_args[0][0]  # type: ignore[reportFunctionMemberAccess]
    assert messages
    assert "You are a highly skilled summarizer" in messages[0].content
    assert "This is a large value" in messages[1].content
    # Check that the content has been truncated
    assert messages[1].content.count("Test") < 1000

    # Check that summaries were added to the artifact
    output_message = result["messages"][0]
    assert isinstance(output_message, ToolMessage)
    assert output_message.artifact.summary == "Short summary"


def test_summarizer_model_error_handling() -> None:
    """Test the summarizer model error handling."""

    class TestError(Exception):
        """Test error."""

    mock_model = get_mock_langchain_generative_model()
    mock_model.to_langchain().invoke.side_effect = TestError("Test error")  # type: ignore[reportFunctionMemberAccess]

    tool_message = ToolMessage(
        content="Tool output content",
        tool_call_id="123",
        name="test_tool",
        artifact=LocalOutput(value="Tool output value"),
    )

    summarizer_model = StepSummarizer(
        config=get_test_config(),
        model=mock_model,
        tool=AdditionTool(),
        step=Step(task="Test task", output="$output"),
    )
    result = summarizer_model.invoke({"messages": [tool_message]})

    # Should return original message without summaries when error occurs
    output_message = result["messages"][0]
    assert isinstance(output_message, ToolMessage)
    assert output_message.artifact.summary is None

```

## File: tests/unit/introspection_agents/test_default_introspection_agent.py

```python
"""Tests for the DefaultIntrospectionAgent module."""

from __future__ import annotations

from unittest.mock import MagicMock, patch

import pytest
from langchain_core.messages import HumanMessage

from portia.config import INTROSPECTION_MODEL_KEY
from portia.execution_agents.output import LocalOutput
from portia.introspection_agents.default_introspection_agent import DefaultIntrospectionAgent
from portia.introspection_agents.introspection_agent import (
    BaseIntrospectionAgent,
    PreStepIntrospection,
    PreStepIntrospectionOutcome,
)
from portia.model import GenerativeModel, Message
from portia.plan import Plan, PlanContext, Step, Variable
from portia.plan_run import PlanRun, PlanRunOutputs, PlanRunState
from portia.prefixed_uuid import PlanUUID
from tests.utils import get_test_config


@pytest.fixture
def mock_introspection_model() -> MagicMock:
    """Mock GenerativeModel object for testing."""
    return MagicMock(spec=GenerativeModel)


@pytest.fixture
def introspection_agent(mock_introspection_model: MagicMock) -> DefaultIntrospectionAgent:
    """Create an instance of the DefaultIntrospectionAgent with mocked config."""
    mock_config = get_test_config(
        custom_models={
            INTROSPECTION_MODEL_KEY: mock_introspection_model,
        },
    )
    return DefaultIntrospectionAgent(config=mock_config)


@pytest.fixture
def mock_plan() -> Plan:
    """Create a mock Plan for testing."""
    return Plan(
        plan_context=PlanContext(
            query="test query",
            tool_ids=["test_tool_1", "test_tool_2", "test_tool_3"],
        ),
        steps=[
            Step(
                task="Task 1",
                tool_id="test_tool_1",
                inputs=[],
                output="$result1",
            ),
            Step(
                task="Task 2",
                tool_id="test_tool_2",
                inputs=[
                    Variable(name="$result1", description="Result of task 1"),
                ],
                output="$result2",
                condition="$result1 != 'SKIPPED'",
            ),
            Step(
                task="Task 3",
                tool_id="test_tool_3",
                inputs=[
                    Variable(name="$result2", description="Result of task 2"),
                ],
                output="$final_result",
                condition="$result2 != 'SKIPPED'",
            ),
        ],
    )


@pytest.fixture
def mock_plan_run() -> PlanRun:
    """Create a mock PlanRun for testing."""
    return PlanRun(
        plan_id=PlanUUID(),
        current_step_index=1,
        state=PlanRunState.IN_PROGRESS,
        outputs=PlanRunOutputs(
            step_outputs={
                "$result1": LocalOutput(value="Task 1 result", summary="Task 1 summary"),
            },
            final_output=None,
        ),
    )


def test_base_introspection_agent_initialization() -> None:
    """Test BaseIntrospectionAgent initialization and default behavior."""

    # Create a minimal implementation of BaseIntrospectionAgent for testing
    class TestIntrospectionAgent(BaseIntrospectionAgent):
        """Test implementation of BaseIntrospectionAgent."""

        def pre_step_introspection(
            self,
            plan: Plan,  # noqa: ARG002
            plan_run: PlanRun,  # noqa: ARG002
        ) -> PreStepIntrospection:
            """Implement required method to test the base class."""
            return PreStepIntrospection(
                outcome=PreStepIntrospectionOutcome.CONTINUE,
                reason="Default implementation test",
            )

    config = get_test_config()
    agent = TestIntrospectionAgent(config)

    assert agent.config == config

    empty_plan = Plan(
        plan_context=PlanContext(query="test", tool_ids=[]),
        steps=[],
    )
    empty_plan_run = PlanRun(plan_id=empty_plan.id)

    result = agent.pre_step_introspection(empty_plan, empty_plan_run)

    assert isinstance(result, PreStepIntrospection)
    assert result.outcome == PreStepIntrospectionOutcome.CONTINUE
    assert result.reason == "Default implementation test"


def test_base_introspection_agent_abstract_method_raises_error() -> None:
    """Test that non-implemented pre_step_introspection raises NotImplementedError."""

    class IncompleteIntrospectionAgent(BaseIntrospectionAgent):
        """Test implementation that doesn't override the abstract method."""

        # Implement the method but have it call the parent's implementation
        def pre_step_introspection(
            self,
            plan: Plan,
            plan_run: PlanRun,
        ) -> PreStepIntrospection:
            """Call the parent's implementation which should raise NotImplementedError."""
            return super().pre_step_introspection(plan, plan_run)  # type: ignore  # noqa: PGH003

    config = get_test_config()
    agent = IncompleteIntrospectionAgent(config)

    empty_plan = Plan(
        plan_context=PlanContext(query="test", tool_ids=[]),
        steps=[],
    )
    empty_plan_run = PlanRun(plan_id=empty_plan.id)

    with pytest.raises(NotImplementedError, match="pre_step_introspection is not implemented"):
        agent.pre_step_introspection(empty_plan, empty_plan_run)


def test_pre_step_introspection_continue(
    introspection_agent: DefaultIntrospectionAgent,
    mock_introspection_model: MagicMock,
    mock_plan: Plan,
    mock_plan_run: PlanRun,
) -> None:
    """Test pre_step_introspection returns CONTINUE when conditions are met."""
    # Mock the Model response response to simulate a CONTINUE outcome
    mock_introspection_model.get_structured_response.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.CONTINUE,
        reason="All conditions are met.",
    )
    result = introspection_agent.pre_step_introspection(
        plan=mock_plan,
        plan_run=mock_plan_run,
    )

    assert result.outcome == PreStepIntrospectionOutcome.CONTINUE
    assert result.reason == "All conditions are met."


def test_pre_step_introspection_skip(
    introspection_agent: DefaultIntrospectionAgent,
    mock_plan: Plan,
    mock_plan_run: PlanRun,
    mock_introspection_model: MagicMock,
) -> None:
    """Test pre_step_introspection returns SKIP when condition is false."""
    mock_introspection_model.get_structured_response.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.SKIP,
        reason="Condition is false.",
    )

    result = introspection_agent.pre_step_introspection(
        plan=mock_plan,
        plan_run=mock_plan_run,
    )

    assert result.outcome == PreStepIntrospectionOutcome.SKIP
    assert result.reason == "Condition is false."


def test_pre_step_introspection_fail(
    introspection_agent: DefaultIntrospectionAgent,
    mock_plan: Plan,
    mock_plan_run: PlanRun,
    mock_introspection_model: MagicMock,
) -> None:
    """Test pre_step_introspection returns FAIL when missing required data."""
    mock_introspection_model.get_structured_response.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.FAIL,
        reason="Missing required data.",
    )

    result = introspection_agent.pre_step_introspection(
        plan=mock_plan,
        plan_run=mock_plan_run,
    )

    assert result.outcome == PreStepIntrospectionOutcome.FAIL
    assert result.reason == "Missing required data."


def test_pre_step_introspection_stop(
    introspection_agent: DefaultIntrospectionAgent,
    mock_plan: Plan,
    mock_plan_run: PlanRun,
    mock_introspection_model: MagicMock,
) -> None:
    """Test pre_step_introspection returns STOP when remaining steps cannot be executed."""
    mock_introspection_model.get_structured_response.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.COMPLETE,
        reason="Remaining steps cannot be executed.",
    )

    result = introspection_agent.pre_step_introspection(
        plan=mock_plan,
        plan_run=mock_plan_run,
    )

    assert result.outcome == PreStepIntrospectionOutcome.COMPLETE
    assert result.reason == "Remaining steps cannot be executed."


def test_pre_step_introspection_passes_correct_data(
    introspection_agent: DefaultIntrospectionAgent,
    mock_plan: Plan,
    mock_plan_run: PlanRun,
    mock_introspection_model: MagicMock,
) -> None:
    """Test pre_step_introspection passes correct data to LLM."""
    mock_messages = [HumanMessage(content="Test message")]

    mock_introspection_model.get_structured_response.return_value = PreStepIntrospection(
        outcome=PreStepIntrospectionOutcome.CONTINUE,
        reason="Test reason",
    )

    with patch(
        "langchain.prompts.ChatPromptTemplate.format_messages",
        return_value=mock_messages,
    ):
        result = introspection_agent.pre_step_introspection(
            plan=mock_plan,
            plan_run=mock_plan_run,
        )

        mock_introspection_model.get_structured_response.assert_called_once_with(
            schema=PreStepIntrospection,
            messages=[Message(role="user", content="Test message")],
        )

        assert result.outcome == PreStepIntrospectionOutcome.CONTINUE
        assert result.reason == "Test reason"

```

## File: tests/unit/introspection_agents/__init__.py

```python
"""Tests for introspection agents."""

```

## File: tests/unit/planning_agents/test_default_planning_agent.py

```python
"""Tests for the PlanningAgent module."""

from __future__ import annotations

import re
from typing import TYPE_CHECKING
from unittest.mock import MagicMock

import pytest

from portia.open_source_tools.llm_tool import LLMTool
from portia.plan import Plan, PlanContext, Step, Variable
from portia.planning_agents.base_planning_agent import BasePlanningAgent, StepsOrError
from portia.planning_agents.context import (
    render_prompt_insert_defaults,
)
from portia.planning_agents.default_planning_agent import DefaultPlanningAgent
from tests.utils import AdditionTool, get_mock_langchain_generative_model, get_test_config

if TYPE_CHECKING:
    from portia.config import Config
    from portia.tool import Tool


@pytest.fixture
def mock_config() -> Config:
    """Mock Config object for testing."""
    return MagicMock()


def test_generate_steps_or_error_success(mock_config: Config) -> None:
    """Test successful plan generation with valid inputs."""
    query = "Send hello@portialabs.ai an email with a summary of the latest news on AI"

    # Mock the Model response to simulate a successful plan generation
    mock_model = get_mock_langchain_generative_model(
        response=StepsOrError(
            steps=[],
            error=None,
        ),
    )
    mock_config.resolve_model.return_value = mock_model  # type: ignore[reportFunctionMemberAccess]
    planning_agent = DefaultPlanningAgent(mock_config)

    result = planning_agent.generate_steps_or_error(
        query=query,
        tool_list=[],
    )

    assert result.steps == []
    assert result.error is None


def test_base_classes() -> None:
    """Test PlanStorage raises."""

    class MyPlanningAgent(BasePlanningAgent):
        """Override to test base."""

        def generate_steps_or_error(
            self,
            query: str,
            tool_list: list[Tool],
            examples: list[Plan] | None = None,
        ) -> StepsOrError:
            return super().generate_steps_or_error(query, tool_list, examples)  # type: ignore  # noqa: PGH003

    wrapper = MyPlanningAgent(get_test_config())

    with pytest.raises(NotImplementedError):
        wrapper.generate_steps_or_error("", [], [])


def test_generate_steps_or_error_failure(mock_config: Config) -> None:
    """Test handling of error when generating a plan fails."""
    query = "Send hello@portialabs.ai an email with a summary of the latest news on AI"

    # Mock the Model response to simulate an error in plan generation
    mock_model = get_mock_langchain_generative_model(
        response=StepsOrError(
            steps=[],
            error="Unable to generate a plan",
        ),
    )
    mock_config.resolve_model.return_value = mock_model  # type: ignore[reportFunctionMemberAccess]
    planning_agent = DefaultPlanningAgent(mock_config)
    result = planning_agent.generate_steps_or_error(
        query=query,
        tool_list=[],
    )

    assert result.error == "Unable to generate a plan"


def test_render_prompt() -> None:
    """Test render prompt."""
    plans = [
        Plan(
            plan_context=PlanContext(
                query="plan query 1",
                tool_ids=["plan_tool1a", "plan_tool1b"],
            ),
            steps=[
                Step(
                    task="plan task 1",
                    tool_id="plan_tool1a",
                    inputs=[Variable(name="$plan_input1", description="plan description 1")],
                    output="$plan_output1",
                ),
            ],
        ),
    ]
    rendered_prompt = render_prompt_insert_defaults(
        query="test query",
        tool_list=[AdditionTool()],
        examples=plans,
    )
    overall_pattern = re.compile(
        r"<Example>(.*?)</Example>.*?<Tools>(.*?)</Tools>.*?<Request>(.*?)</Request>.*?",
        re.DOTALL,
    )
    example_match, tools_content, request_content = overall_pattern.findall(
        rendered_prompt,
    )[0]

    tool_pattern = re.compile(r"<Tools>(.*?)</Tools>", re.DOTALL)
    tool_match = tool_pattern.findall(example_match)[0]

    assert "plan_tool1a" in tool_match
    assert "plan_tool1b" in tool_match

    query_pattern = re.compile(r"<Query>(.*?)</Query>", re.DOTALL)
    query_match = query_pattern.findall(example_match)[0]

    assert "plan query 1" in query_match

    response_pattern = re.compile(r"<Response>(.*?)</Response>", re.DOTALL)
    response_match = response_pattern.findall(example_match)[0]

    assert "plan task 1" in response_match
    assert "plan_tool1a" in response_match
    assert "$plan_input1" in response_match
    assert "$plan_output1" in response_match

    assert "Use this tool to add two numbers together" in tools_content
    assert "add_tool" in tools_content

    assert "test query" in request_content


def test_generate_steps_or_error_invalid_tool_id(mock_config: Config) -> None:
    """Test handling of invalid tool ID in generated steps."""
    query = "Calculate something"

    mock_response = StepsOrError(
        steps=[
            Step(
                task="Calculate sum",
                tool_id="no_tool_1",
                inputs=[],
                output="$result",
            ),
            Step(
                task="Calculate sum2",
                tool_id="no_tool_2",
                inputs=[],
                output="$result2",
            ),
        ],
        error=None,
    )
    mock_model = get_mock_langchain_generative_model(
        response=mock_response,
    )
    mock_config.resolve_model.return_value = mock_model  # type: ignore[reportFunctionMemberAccess]
    planning_agent = DefaultPlanningAgent(mock_config)
    result = planning_agent.generate_steps_or_error(
        query=query,
        tool_list=[AdditionTool()],
    )

    assert result.error == "Missing tools no_tool_1, no_tool_2 from the provided tool_list"
    assert result.steps == mock_response.steps


def test_generate_steps_assigns_llm_tool_id(mock_config: Config) -> None:
    """Test that steps without tool_id get assigned to LLMTool."""
    query = "Generate a creative story"

    # Mock response with steps that have no tool_id
    mock_response = StepsOrError(
        steps=[
            Step(
                task="Write a story opening",
                tool_id=None,
                inputs=[],
                output="$story_opening",
            ),
            Step(
                task="Write story conclusion",
                tool_id=None,
                inputs=[],
                output="$story_conclusion",
            ),
        ],
        error=None,
    )
    mock_model = get_mock_langchain_generative_model(
        response=mock_response,
    )
    mock_config.resolve_model.return_value = mock_model  # type: ignore[reportFunctionMemberAccess]
    planning_agent = DefaultPlanningAgent(mock_config)
    result = planning_agent.generate_steps_or_error(
        query=query,
        tool_list=[AdditionTool()],
    )

    assert all(step.tool_id == LLMTool.LLM_TOOL_ID for step in result.steps)
    assert len(result.steps) == 2
    assert result.error is None

```

## File: tests/unit/open_source_tools/test_weather_tool.py

```python
"""Weather tool tests."""

from unittest.mock import Mock, patch

import pytest

from portia.errors import ToolHardError, ToolSoftError
from portia.open_source_tools.weather import WeatherTool
from tests.utils import get_test_tool_context


def test_weather_tool_missing_api_key() -> None:
    """Test that WeatherTool raises ToolHardError if API key is missing."""
    tool = WeatherTool()
    with patch("os.getenv", return_value=""):
        ctx = get_test_tool_context()
        with pytest.raises(ToolHardError):
            tool.run(ctx, "paris")


def test_weather_tool_successful_response() -> None:
    """Test that WeatherTool successfully processes a valid response."""
    tool = WeatherTool()
    mock_api_key = "mock-api-key"
    mock_response = {"main": {"temp": 10}, "weather": [{"description": "sunny"}]}

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.get") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            result = tool.run(ctx, "paris")
            assert result == "The current weather in paris is sunny with a temperature of 10°C."


def test_weather_tool_no_answer_in_response() -> None:
    """Test that WeatherTool raises ToolSoftError if no answer is found in the response."""
    tool = WeatherTool()
    mock_api_key = "mock-api-key"
    mock_response = {"no_answer": "No relevant information found."}

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.get") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            with pytest.raises(ToolSoftError, match="No data found for: Paris"):
                tool.run(ctx, "Paris")


def test_weather_tool_no_main_answer_in_response() -> None:
    """Test that WeatherTool raises ToolSoftError if no answer is found in the response."""
    tool = WeatherTool()
    mock_api_key = "mock-api-key"
    mock_response = {
        "no_answer": "No relevant information found.",
        "weather": [{"description": "sunny"}],
    }

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.get") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            with pytest.raises(ToolSoftError, match="No main data found for city: Paris"):
                tool.run(ctx, "Paris")


def test_weather_tool_http_error() -> None:
    """Test that WeatherTool handles HTTP errors correctly."""
    tool = WeatherTool()
    mock_api_key = "mock-api-key"

    with patch("os.getenv", return_value=mock_api_key):  # noqa: SIM117
        with patch("httpx.get", side_effect=Exception("HTTP Error")):
            ctx = get_test_tool_context()
            with pytest.raises(Exception, match="HTTP Error"):
                tool.run(ctx, "Paris")

```

## File: tests/unit/open_source_tools/test_image_understanding_tool.py

```python
"""Tests for image understanding tool."""

from unittest.mock import MagicMock

import pytest
from langchain.schema import HumanMessage
from pydantic import ValidationError

from portia.open_source_tools.image_understanding_tool import (
    ImageUnderstandingTool,
    ImageUnderstandingToolSchema,
)
from portia.tool import ToolRunContext


@pytest.fixture
def mock_image_understanding_tool() -> ImageUnderstandingTool:
    """Fixture to create an instance of ImageUnderstandingTool."""
    return ImageUnderstandingTool(id="test_tool", name="Test Image Understanding Tool")


def test_image_understanding_tool_run_url(
    mock_tool_run_context: ToolRunContext,
    mock_image_understanding_tool: ImageUnderstandingTool,
    mock_model: MagicMock,
) -> None:
    """Test that ImageUnderstandingTool runs successfully and returns a response."""
    # Setup mock responses
    mock_response = MagicMock()
    mock_response.content = "Test response content"
    mock_model.to_langchain.return_value.invoke.return_value = mock_response

    # Define task input
    schema_data = {
        "task": "What is the capital of France?",
        "image_url": "https://example.com/image.png",
    }

    # Run the tool
    result = mock_image_understanding_tool.run(mock_tool_run_context, **schema_data)

    assert mock_model.to_langchain.called
    mock_model.to_langchain.return_value.invoke.assert_called_once_with(
        [
            HumanMessage(content=mock_image_understanding_tool.prompt),
            HumanMessage(
                content=[
                    {"type": "text", "text": schema_data["task"]},
                    {
                        "type": "image_url",
                        "image_url": {"url": schema_data["image_url"]},
                    },
                ],
            ),
        ],
    )

    # Assert the result is the expected response
    assert result == "Test response content"


def test_image_understanding_tool_schema_valid_input() -> None:
    """Test that the LLMToolSchema correctly validates the input."""
    schema_data = {
        "task": "Solve a math problem in this image",
        "image_url": "https://example.com/image.png",
    }
    schema = ImageUnderstandingToolSchema(**schema_data)

    assert schema.task == "Solve a math problem in this image"
    assert schema.image_url == "https://example.com/image.png"


def test_image_understanding_tool_schema_missing_task() -> None:
    """Test that LLMToolSchema raises an error if 'task' is missing."""
    with pytest.raises(ValidationError):
        ImageUnderstandingToolSchema(image_url="https://example.com/image.png")  # type: ignore  # noqa: PGH003


def test_image_understanding_tool_schema_missing_image_url_and_file() -> None:
    """Test that LLMToolSchema raises an error if 'image_url' and 'image_file' are missing."""
    with pytest.raises(ValidationError):
        ImageUnderstandingToolSchema(task="Solve a math problem in this image")  # type: ignore  # noqa: PGH003


def test_image_understanding_tool_schema_both_image_url_and_file() -> None:
    """Test that LLMToolSchema raises an error if 'image_url' and 'image_file' are provided."""
    with pytest.raises(ValidationError):
        ImageUnderstandingToolSchema(
            task="Solve a math problem in this image",
            image_url="https://example.com/image.png",
            image_file="image.png",
        )  # type: ignore  # noqa: PGH003


def test_image_understanding_tool_initialization(
    mock_image_understanding_tool: ImageUnderstandingTool,
) -> None:
    """Test that LLMTool is correctly initialized."""
    assert mock_image_understanding_tool.id == "test_tool"
    assert mock_image_understanding_tool.name == "Test Image Understanding Tool"


def test_image_understanding_tool_run_with_context(
    mock_model: MagicMock,
    mock_tool_run_context: ToolRunContext,
    mock_image_understanding_tool: ImageUnderstandingTool,
) -> None:
    """Test that ImageUnderstandingTool runs successfully when a context is provided."""
    # Setup mock responses
    mock_response = MagicMock()
    mock_response.content = "Test response content"
    mock_model.to_langchain.return_value.invoke.return_value = mock_response
    # Define task and context
    mock_image_understanding_tool.tool_context = "Context for task"
    schema_data = {
        "task": "What is the capital of France?",
        "image_url": "https://example.com/map.png",
    }

    # Run the tool
    result = mock_image_understanding_tool.run(mock_tool_run_context, **schema_data)

    # Verify that the Models's to_langchain().invoke method is called
    called_with = mock_model.to_langchain.return_value.invoke.call_args_list[0].args[0]
    assert len(called_with) == 2
    assert isinstance(called_with[0], HumanMessage)
    assert isinstance(called_with[1], HumanMessage)
    assert mock_image_understanding_tool.tool_context in called_with[1].content[0]["text"]
    # Assert the result is the expected response
    assert result == "Test response content"

```

## File: tests/unit/open_source_tools/test_file_writer_tool.py

```python
"""FileWriterTool tests."""

from pathlib import Path

import pytest

from portia.open_source_tools.local_file_writer_tool import FileWriterTool
from tests.utils import get_test_tool_context


def test_file_writer_tool_successful_write(tmp_path: Path) -> None:
    """Test that FileWriterTool successfully writes content to a file."""
    tool = FileWriterTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test_file.txt"
    content = "Hello, world!"

    result = tool.run(ctx, str(filename), content)
    assert filename.read_text() == content
    assert result == f"Content written to {filename}"


def test_file_writer_tool_overwrite_existing_file(tmp_path: Path) -> None:
    """Test that FileWriterTool overwrites an existing file."""
    tool = FileWriterTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "existing_file.txt"
    filename.write_text("Old content")
    content = "New content!"

    result = tool.run(ctx, str(filename), content)
    assert filename.read_text() == content
    assert result == f"Content written to {filename}"


def test_file_writer_tool_handles_file_creation_error(tmp_path: Path) -> None:
    """Test that FileWriterTool raises an error if file creation fails."""
    tool = FileWriterTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "error_file.txt"

    # Make the directory read-only to simulate permission error
    tmp_path.chmod(0o400)
    with pytest.raises(OSError, match="Permission denied"):
        tool.run(ctx, str(filename), "This should fail")

```

## File: tests/unit/open_source_tools/test_file_reader_tool.py

```python
"""FileReaderTool tests."""

import json
from pathlib import Path

import pandas as pd
import pytest

from portia.clarification import MultipleChoiceClarification
from portia.errors import ToolHardError
from portia.open_source_tools.local_file_reader_tool import FileReaderTool
from tests.utils import get_test_tool_context


def test_file_reader_tool_read_txt(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .txt file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.txt"
    content = "Hello, world!"
    filename.write_text(content, encoding="utf-8")

    result = tool.run(ctx, str(filename))
    assert result == content


def test_file_reader_tool_read_log(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .log file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.log"
    content = "Hello, world!"
    filename.write_text(content, encoding="utf-8")

    result = tool.run(ctx, str(filename))
    assert result == content


def test_file_reader_tool_read_json(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .json file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.json"
    content = {"key": "value"}
    filename.write_text(json.dumps(content), encoding="utf-8")

    result = tool.run(ctx, str(filename))
    assert isinstance(result, str)
    assert json.loads(result) == content


def test_file_reader_tool_read_csv(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .csv file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.csv"
    frame = pd.DataFrame({"col1": [1, 2], "col2": [3, 4]})
    frame.to_csv(filename, index=False)

    result = tool.run(ctx, str(filename))
    assert isinstance(result, str)
    assert "col1" in result
    assert "col2" in result


def test_file_reader_tool_read_xlsx(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .xlsx file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.xlsx"
    frame = pd.DataFrame({"col1": [1, 2], "col2": [3, 4]})
    frame.to_excel(filename, index=False)

    result = tool.run(ctx, str(filename))
    assert isinstance(result, str)
    assert "col1" in result
    assert "col2" in result


def test_file_reader_tool_read_xls(tmp_path: Path) -> None:
    """Test that FileReaderTool reads content from a .xls file."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.xls"
    frame = pd.DataFrame({"col1": [1, 2], "col2": [3, 4]})
    frame.to_excel(filename, index=False)

    result = tool.run(ctx, str(filename))
    assert isinstance(result, str)
    assert "col1" in result
    assert "col2" in result


def test_file_reader_tool_unsupported_format(tmp_path: Path) -> None:
    """Test that FileReaderTool raises an error for unsupported file formats."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "test.unsupported"
    filename.write_text("Some content", encoding="utf-8")

    with pytest.raises(ToolHardError, match="Unsupported file format"):
        tool.run(ctx, str(filename))


def test_file_reader_tool_file_alt_files(tmp_path: Path) -> None:
    """Test that FileReaderTool raises an error when file is not found."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "non_existent.txt"

    subfolder = tmp_path / "test"
    subfolder.mkdir()

    alt_filename = subfolder / "non_existent.txt"
    content = "Hello, world!"
    alt_filename.write_text(content, encoding="utf-8")

    output = tool.run(ctx, str(filename))
    assert isinstance(output, MultipleChoiceClarification)
    assert isinstance(output.options, list)
    assert len(output.options) == 1
    assert output.options[0] == str(alt_filename)
    assert str(filename) in output.user_guidance
    assert str(alt_filename) in output.user_guidance


def test_file_reader_tool_file_no_files(tmp_path: Path) -> None:
    """Test that FileReaderTool raises an error when file is not found."""
    tool = FileReaderTool()
    ctx = get_test_tool_context()
    filename = tmp_path / "non_existent.txt"

    with pytest.raises(ToolHardError):
        tool.run(ctx, str(filename))

```

## File: tests/unit/open_source_tools/test_calculator_tool.py

```python
"""Tests for Calculator Tool."""

from unittest.mock import patch

import pytest

from portia.errors import ToolHardError
from portia.open_source_tools.calculator_tool import CalculatorTool
from tests.utils import get_test_tool_context


@pytest.fixture
def calculator_tool() -> CalculatorTool:
    """Return Calculator Tool."""
    return CalculatorTool()


def test_math_expression_conversion(calculator_tool: CalculatorTool) -> None:
    """Test expression conversion."""
    assert calculator_tool.math_expression("What is 3 added to 5?") == "3 + 5"
    assert calculator_tool.math_expression("12 multiplied by 4") == "12 * 4"
    assert calculator_tool.math_expression("20 divided by 5") == "20 / 5"
    assert calculator_tool.math_expression("divide 20 by 5") == "20 / 5"
    assert calculator_tool.math_expression("8 subtracted from 15") == "15 - 8"
    assert calculator_tool.math_expression("subtract 7 from 14") == "14 - 7"
    assert calculator_tool.math_expression("multiply 6 by 3") == "6 * 3"


def test_run_valid_expressions(calculator_tool: CalculatorTool) -> None:
    """Test valid expressions."""
    context = get_test_tool_context()
    assert calculator_tool.run(context, "3 plus 5") == 8.0
    assert calculator_tool.run(context, "10 divided by 2") == 5.0
    assert calculator_tool.run(context, "6 times 3") == 18.0
    assert calculator_tool.run(context, "15 minus 4") == 11.0
    assert calculator_tool.run(context, "212 + 14") == 226
    assert calculator_tool.run(context, "300 - 14") == 286
    assert calculator_tool.run(context, "3 x 2") == 6
    assert calculator_tool.run(context, "What is the sum of 17.42 and 16.72") == 34.14


def test_run_invalid_expressions(calculator_tool: CalculatorTool) -> None:
    """Test invalid expressions."""
    context = get_test_tool_context()
    with pytest.raises(ToolHardError):
        calculator_tool.run(context, "what is the meaning of life?")

    with pytest.raises(ToolHardError):
        calculator_tool.run(context, "")

    with pytest.raises(ToolHardError):
        calculator_tool.run(context, "")

    with patch.object(CalculatorTool, "math_expression", return_value=None):
        patched_tool = CalculatorTool()
        with pytest.raises(ToolHardError):
            patched_tool.run(context, " ")

    with pytest.raises(ToolHardError):
        calculator_tool.run(context, "5 + 3 * x")

    with pytest.raises(ToolHardError, match="Error evaluating expression."):
        calculator_tool.run(context, "subtract (def myclass) from 10")


def test_run_division_by_zero(calculator_tool: CalculatorTool) -> None:
    """Test divide by zero."""
    context = get_test_tool_context()
    with pytest.raises(ToolHardError, match="Error: Division by zero"):
        calculator_tool.run(context, "10 divided by 0")


def test_run_complex_expressions(calculator_tool: CalculatorTool) -> None:
    """Test complex."""
    context = get_test_tool_context()
    assert calculator_tool.run(context, "(3 plus 5) times 2") == 16.0
    assert calculator_tool.run(context, "(10 minus 3) divided by 2") == 3.5


def test_run_decimal_numbers(calculator_tool: CalculatorTool) -> None:
    """Test decimals."""
    context = get_test_tool_context()
    assert calculator_tool.run(context, "3.5 plus 2.5") == 6.0
    assert calculator_tool.run(context, "7.2 divided by 3.6") == 2.0

```

## File: tests/unit/open_source_tools/conftest.py

```python
"""Fixtures for open source tools."""

import uuid
from unittest.mock import MagicMock

import pytest

from portia.config import Config
from portia.execution_context import ExecutionContext
from portia.model import LangChainGenerativeModel
from portia.prefixed_uuid import PlanRunUUID
from portia.tool import ToolRunContext


@pytest.fixture
def mock_tool_run_context(mock_model: MagicMock) -> ToolRunContext:
    """Fixture to mock ExecutionContext."""
    mock_config = MagicMock(spec=Config)
    mock_config.resolve_model.return_value = mock_model
    mock_config.resolve_langchain_model.return_value = mock_model
    mock_execution_context = MagicMock(spec=ExecutionContext)
    mock_execution_context.plan_run_context = None
    return ToolRunContext.model_construct(
        execution_context=mock_execution_context,
        plan_run_id=PlanRunUUID(uuid=uuid.uuid4()),
        config=mock_config,
        clarifications=[],
    )


@pytest.fixture(autouse=True)
def mock_openai_api_key_env(monkeypatch: pytest.MonkeyPatch) -> None:
    """Fixture to set the OPENAI_API_KEY environment variable."""
    monkeypatch.setenv("OPENAI_API_KEY", "123")


@pytest.fixture
def mock_model() -> MagicMock:
    """Fixture to mock a GenerativeModel."""
    return MagicMock(spec=LangChainGenerativeModel)

```

## File: tests/unit/open_source_tools/test_search_tool.py

```python
"""Search tool tests."""

from unittest.mock import Mock, patch

import pytest

from portia.errors import ToolHardError, ToolSoftError
from portia.open_source_tools.search_tool import SearchTool
from tests.utils import get_test_tool_context


def test_search_tool_missing_api_key() -> None:
    """Test that SearchTool raises ToolHardError if API key is missing."""
    tool = SearchTool()
    with patch("os.getenv", return_value=""):
        ctx = get_test_tool_context()
        with pytest.raises(ToolHardError):
            tool.run(ctx, "What is the capital of France?")


def test_search_tool_successful_response() -> None:
    """Test that SearchTool successfully processes a valid response."""
    tool = SearchTool()
    mock_api_key = "mock-api-key"
    mock_response = {
        "query": "What is the capital of France?",
        "follow_up_questions": "",
        "answer": "The capital of France is Paris.",
        "images": [],
        "results": ["result1", "result2", "result3", "result4", "result5"],
        "response_time": 2.43,
    }

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.post") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            result = tool.run(ctx, "What is the capital of France?")
            assert result == ["result1", "result2", "result3"]


def test_search_tool_fewer_results_than_max() -> None:
    """Test that SearchTool successfully processes a valid response."""
    tool = SearchTool()
    mock_api_key = "mock-api-key"
    mock_response = {
        "query": "What is the capital of France?",
        "follow_up_questions": "",
        "answer": "The capital of France is Paris.",
        "images": [],
        "results": ["result1", "result2"],
        "response_time": 2.43,
    }

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.post") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            result = tool.run(ctx, "What is the capital of France?")
            assert result == ["result1", "result2"]


def test_search_tool_no_answer_in_response() -> None:
    """Test that SearchTool raises ToolSoftError if no answer is found in the response."""
    tool = SearchTool()
    mock_api_key = "mock-api-key"
    mock_response = {"no_answer": "No relevant information found."}

    with patch("os.getenv", return_value=mock_api_key):
        ctx = get_test_tool_context()
        with patch("httpx.post") as mock_post:
            mock_post.return_value = Mock(status_code=200, json=lambda: mock_response)

            with pytest.raises(ToolSoftError, match="Failed to get answer to search:.*"):
                tool.run(ctx, "What is the capital of France?")


def test_search_tool_http_error() -> None:
    """Test that SearchTool handles HTTP errors correctly."""
    tool = SearchTool()
    mock_api_key = "mock-api-key"

    with patch("os.getenv", return_value=mock_api_key):  # noqa: SIM117
        with patch("httpx.post", side_effect=Exception("HTTP Error")):
            ctx = get_test_tool_context()
            with pytest.raises(Exception, match="HTTP Error"):
                tool.run(ctx, "What is the capital of France?")

```

## File: tests/unit/open_source_tools/test_llm_tool.py

```python
"""tests for llm tool."""

from unittest.mock import MagicMock

import pytest

from portia.model import Message
from portia.open_source_tools.llm_tool import LLMTool, LLMToolSchema
from portia.tool import ToolRunContext


@pytest.fixture
def mock_llm_tool() -> LLMTool:
    """Fixture to create an instance of LLMTool."""
    return LLMTool(id="test_tool", name="Test LLM Tool")


def test_llm_tool_plan_run(
    mock_llm_tool: LLMTool,
    mock_tool_run_context: ToolRunContext,
    mock_model: MagicMock,
) -> None:
    """Test that LLMTool runs successfully and returns a response."""
    # Setup mock responses
    mock_model.get_response.return_value = Message(role="user", content="Test response content")
    # Define task input
    task = "What is the capital of France?"

    # Run the tool
    result = mock_llm_tool.run(mock_tool_run_context, task)

    mock_model.get_response.assert_called_once_with(
        [Message(role="user", content=mock_llm_tool.prompt), Message(role="user", content=task)],
    )

    # Assert the result is the expected response
    assert result == "Test response content"


def test_llm_tool_schema_valid_input() -> None:
    """Test that the LLMToolSchema correctly validates the input."""
    schema_data = {"task": "Solve a math problem"}
    schema = LLMToolSchema(**schema_data)

    assert schema.task == "Solve a math problem"


def test_llm_tool_schema_missing_task() -> None:
    """Test that LLMToolSchema raises an error if 'task' is missing."""
    with pytest.raises(ValueError):  # noqa: PT011
        LLMToolSchema()  # type: ignore  # noqa: PGH003


def test_llm_tool_initialization(mock_llm_tool: LLMTool) -> None:
    """Test that LLMTool is correctly initialized."""
    assert mock_llm_tool.id == "test_tool"
    assert mock_llm_tool.name == "Test LLM Tool"


def test_llm_tool_run_with_context(
    mock_llm_tool: LLMTool,
    mock_tool_run_context: ToolRunContext,
    mock_model: MagicMock,
) -> None:
    """Test that LLMTool runs successfully when a context is provided."""
    # Setup mock responses
    mock_model.get_response.return_value = Message(role="user", content="Test response content")

    # Define task and context
    mock_llm_tool.tool_context = "Context for task"
    task = "What is the capital of France?"

    # Run the tool
    result = mock_llm_tool.run(mock_tool_run_context, task)

    # Verify that the Model's get_response method is called
    called_with = mock_model.get_response.call_args_list[0].args[0]
    assert len(called_with) == 2
    assert isinstance(called_with[0], Message)
    assert isinstance(called_with[1], Message)
    assert mock_llm_tool.tool_context in called_with[1].content
    assert task in called_with[1].content
    # Assert the result is the expected response
    assert result == "Test response content"

```

## File: tests/integration/test_mcp_session.py

```python
"""MCP Session Tests."""

import socket
import subprocess
import time
from collections.abc import Iterator
from pathlib import Path

import mcp
import pytest

from portia.mcp_session import SseMcpClientConfig, StdioMcpClientConfig, get_mcp_session

SERVER_FILE_PATH = Path(__file__).parent / "mcp_server.py"


def is_port_in_use(port: int) -> bool:
    """Check if a port is in use."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(("localhost", port)) == 0


@pytest.mark.asyncio
async def test_mcp_session_stdio() -> None:
    """Test the MCP session with stdio."""
    async with get_mcp_session(
        StdioMcpClientConfig(
            server_name="test_server",
            command="poetry",
            args=["run", "python", str(SERVER_FILE_PATH.absolute()), "stdio"],
        ),
    ) as session:
        tools = await session.list_tools()
        assert isinstance(tools, mcp.ListToolsResult)
        assert len(tools.tools) == 1


@pytest.fixture
def sse_background_server() -> Iterator[None]:
    """Start the MCP server in the background."""
    process = subprocess.Popen(["poetry", "run", "python", str(SERVER_FILE_PATH.absolute()), "sse"])  # noqa: S607, S603
    try:
        # Wait for server to start
        time.sleep(3)

        # Check if process is still running
        if process.poll() is not None:
            raise Exception(f"Server process exited with code {process.poll()}")  # noqa: TRY002

        yield
    finally:
        process.terminate()
        try:
            process.wait(timeout=2)
        except subprocess.TimeoutExpired:
            process.kill()


@pytest.mark.asyncio
@pytest.mark.usefixtures("sse_background_server")
async def test_mcp_session_sse() -> None:
    """Test the MCP session with SSE."""
    async with get_mcp_session(
        SseMcpClientConfig(
            server_name="test_server",
            url="http://localhost:11385/sse",
            sse_read_timeout=5,
            timeout=5,
        ),
    ) as session:
        tools = await session.list_tools()
        assert isinstance(tools, mcp.ListToolsResult)
        assert len(tools.tools) == 1

```

## File: tests/integration/test_runner_context.py

```python
"""Tests for execution context."""

from __future__ import annotations

from portia.config import StorageClass, default_config
from portia.execution_context import ExecutionContext, execution_context
from portia.plan import Plan, PlanContext, Step
from portia.plan_run import PlanRun, PlanRunState
from portia.portia import Portia
from portia.tool import Tool, ToolRunContext
from portia.tool_registry import ToolRegistry


class ExecutionContextTrackerTool(Tool):
    """Tracks Execution Context."""

    id: str = "execution_tracker_tool"
    name: str = "Execution Tracker Tool"
    description: str = "Tracks tool execution context"
    output_schema: tuple[str, str] = (
        "None",
        "Nothing",
    )
    tool_context: ToolRunContext | None = None

    def run(
        self,
        ctx: ToolRunContext,
    ) -> None:
        """Save the context."""
        self.tool_context = ctx


def get_test_plan_run() -> tuple[Plan, PlanRun]:
    """Return test plan_run."""
    step1 = Step(
        task="Save Context",
        inputs=[],
        output="$ctx",
        tool_id="execution_tracker_tool",
    )
    plan = Plan(
        plan_context=PlanContext(
            query="Add 1 + 2",
            tool_ids=["add_tool"],
        ),
        steps=[step1],
    )
    return plan, PlanRun(plan_id=plan.id, current_step_index=0)


def test_portia_no_execution_context_new() -> None:
    """Test running a query."""
    tool = ExecutionContextTrackerTool()
    tool_registry = ToolRegistry([tool])
    portia = Portia(tools=tool_registry, config=default_config(storage_class=StorageClass.MEMORY))
    (plan, plan_run) = get_test_plan_run()
    portia.storage.save_plan(plan)
    plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert tool.tool_context
    assert tool.tool_context.plan_run_id == plan_run.id


def test_portia_no_execution_context_existing() -> None:
    """Test running a query."""
    tool = ExecutionContextTrackerTool()
    tool_registry = ToolRegistry([tool])
    portia = Portia(tools=tool_registry, config=default_config(storage_class=StorageClass.MEMORY))
    (plan, plan_run) = get_test_plan_run()
    plan_run.execution_context = ExecutionContext(end_user_id="123")
    portia.storage.save_plan(plan)
    plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert tool.tool_context
    assert tool.tool_context.plan_run_id == plan_run.id
    assert tool.tool_context.execution_context.end_user_id == "123"


def test_portia_with_execution_context_new() -> None:
    """Test running a query."""
    tool = ExecutionContextTrackerTool()
    tool_registry = ToolRegistry([tool])
    portia = Portia(tools=tool_registry, config=default_config(storage_class=StorageClass.MEMORY))
    (plan, plan_run) = get_test_plan_run()
    portia.storage.save_plan(plan)

    with execution_context(end_user_id="123"):
        plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert tool.tool_context
    assert tool.tool_context.plan_run_id == plan_run.id
    assert tool.tool_context.execution_context.end_user_id == "123"


def test_portia_with_execution_context_existing() -> None:
    """Test running a query."""
    tool = ExecutionContextTrackerTool()
    tool_registry = ToolRegistry([tool])
    portia = Portia(tools=tool_registry, config=default_config(storage_class=StorageClass.MEMORY))
    (plan, plan_run) = get_test_plan_run()
    plan_run.execution_context = ExecutionContext()
    portia.storage.save_plan(plan)

    with execution_context(end_user_id="123"):
        plan_run = portia.resume(plan_run)

    assert plan_run.state == PlanRunState.COMPLETE
    assert tool.tool_context
    assert tool.tool_context.plan_run_id == plan_run.id
    assert tool.tool_context.execution_context.end_user_id == "123"

```

## File: tests/integration/mcp_server.py

```python
"""Mock MCP server for testing."""

from __future__ import annotations

import sys
from logging import getLogger

from mcp.server import FastMCP

logger = getLogger(__name__)


server = FastMCP("server", port=11385, log_level="DEBUG")


@server.tool()
def add_one(input_number: float) -> str:
    """Add one to the input.

    Args:
        input_number: The input to add one to.

    Returns:
        The input plus one.

    """
    return str(input_number + 1)


if __name__ == "__main__":
    logger.info("Starting MCP server with args: %s", sys.argv)
    server.run(
        transport=sys.argv[1]  # type: ignore[arg-type]
        if len(sys.argv) > 1 and sys.argv[1] in ["stdio", "sse"]
        else "stdio",
    )

```

## File: tests/integration/test_e2e.py

```python
"""E2E Tests."""

from __future__ import annotations

from typing import TYPE_CHECKING, Callable
from unittest.mock import MagicMock, patch

import pytest
from pydantic import HttpUrl

from portia.clarification import ActionClarification, Clarification, InputClarification
from portia.clarification_handler import ClarificationHandler
from portia.config import (
    Config,
    ExecutionAgentType,
    LLMModel,
    LLMProvider,
    LogLevel,
    StorageClass,
)
from portia.errors import PlanError, ToolSoftError
from portia.open_source_tools.registry import example_tool_registry
from portia.plan import Plan, PlanContext, Step, Variable
from portia.plan_run import PlanRunState
from portia.portia import ExecutionHooks, Portia
from portia.tool_registry import ToolRegistry
from tests.utils import AdditionTool, ClarificationTool, ErrorTool, TestClarificationHandler

if TYPE_CHECKING:
    from portia.tool import ToolRunContext


CORE_MODELS = [
    (
        LLMProvider.OPENAI,
        LLMModel.GPT_4_O_MINI,
    ),
    (
        LLMProvider.ANTHROPIC,
        LLMModel.CLAUDE_3_OPUS,
    ),
]


PROVIDER_MODELS = [
    *CORE_MODELS,
    (
        LLMProvider.MISTRALAI,
        LLMModel.MISTRAL_LARGE,
    ),
    (
        LLMProvider.GOOGLE_GENERATIVE_AI,
        LLMModel.GEMINI_2_0_FLASH,
    ),
]

AGENTS = [
    ExecutionAgentType.DEFAULT,
    ExecutionAgentType.ONE_SHOT,
]


@pytest.mark.parametrize(("llm_provider", "llm_model_name"), PROVIDER_MODELS)
@pytest.mark.flaky(reruns=4)
def test_portia_run_query(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
) -> None:
    """Test running a simple query."""
    config = Config.from_default(
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        storage_class=StorageClass.MEMORY,
    )

    addition_tool = AdditionTool()
    addition_tool.should_summarize = True

    tool_registry = ToolRegistry([addition_tool])
    portia = Portia(config=config, tools=tool_registry)
    query = "Add 1 + 2"

    plan_run = portia.run(query)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.outputs.final_output
    assert plan_run.outputs.final_output.get_value() == 3
    for output in plan_run.outputs.step_outputs.values():
        assert output.get_summary() is not None


@pytest.mark.parametrize(("llm_provider", "llm_model_name"), PROVIDER_MODELS)
@pytest.mark.flaky(reruns=4)
def test_portia_generate_plan(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
) -> None:
    """Test planning a simple query."""
    config = Config.from_default(
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        storage_class=StorageClass.MEMORY,
    )

    tool_registry = ToolRegistry([AdditionTool()])
    portia = Portia(config=config, tools=tool_registry)
    query = "Add 1 + 2"

    plan = portia.plan(query)

    assert len(plan.steps) == 1
    assert plan.steps[0].tool_id == "add_tool"


@pytest.mark.parametrize(("llm_provider", "llm_model_name"), PROVIDER_MODELS)
@pytest.mark.parametrize("agent", AGENTS)
@pytest.mark.flaky(reruns=3)
def test_portia_run_query_with_clarifications(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
    agent: ExecutionAgentType,
) -> None:
    """Test running a query with clarification."""
    config = Config.from_default(
        default_log_level=LogLevel.DEBUG,
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        execution_agent_type=agent,
        storage_class=StorageClass.MEMORY,
    )

    test_clarification_handler = TestClarificationHandler()
    tool_registry = ToolRegistry([ClarificationTool()])
    portia = Portia(
        config=config,
        tools=tool_registry,
        execution_hooks=ExecutionHooks(clarification_handler=test_clarification_handler),
    )
    clarification_step = Step(
        tool_id="clarification_tool",
        task="Raise a clarification with user guidance 'Return a clarification'",
        output="",
        inputs=[],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="raise a clarification",
            tool_ids=["clarification_tool"],
        ),
        steps=[clarification_step],
    )
    portia.storage.save_plan(plan)

    plan_run = portia.run_plan(plan)
    assert plan_run.state == PlanRunState.COMPLETE
    assert test_clarification_handler.received_clarification is not None
    assert (
        test_clarification_handler.received_clarification.user_guidance == "Return a clarification"
    )


def test_portia_run_query_with_clarifications_no_handler() -> None:
    """Test running a query with clarification using Portia."""
    config = Config.from_default(
        default_log_level=LogLevel.DEBUG,
        llm_provider=LLMProvider.OPENAI,
        llm_model_name=LLMModel.GPT_4_O_MINI,
        execution_agent_type=ExecutionAgentType.DEFAULT,
        storage_class=StorageClass.MEMORY,
    )

    tool_registry = ToolRegistry([ClarificationTool()])
    portia = Portia(config=config, tools=tool_registry)
    clarification_step = Step(
        tool_id="clarification_tool",
        task="raise a clarification with a user guidance 'Return a clarification'",
        output="",
        inputs=[],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="Raise a clarification",
            tool_ids=["clarification_tool"],
        ),
        steps=[clarification_step],
    )
    portia.storage.save_plan(plan)

    plan_run = portia.run_plan(plan)

    assert plan_run.state == PlanRunState.NEED_CLARIFICATION
    assert plan_run.get_outstanding_clarifications()[0].user_guidance == "Return a clarification"

    plan_run = portia.resolve_clarification(
        plan_run.get_outstanding_clarifications()[0],
        "False",
    )

    portia.resume(plan_run)
    assert plan_run.state == PlanRunState.COMPLETE


@pytest.mark.parametrize(("llm_provider", "llm_model_name"), CORE_MODELS)
@pytest.mark.parametrize("agent", AGENTS)
def test_portia_run_query_with_hard_error(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
    agent: ExecutionAgentType,
) -> None:
    """Test running a query with error."""
    config = Config.from_default(
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        execution_agent_type=agent,
        storage_class=StorageClass.MEMORY,
    )
    tool_registry = ToolRegistry([ErrorTool()])
    portia = Portia(config=config, tools=tool_registry)
    clarification_step = Step(
        tool_id="error_tool",
        task="Use error tool with string 'Something went wrong' and \
        do not return a soft error or uncaught error",
        output="",
        inputs=[],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="raise an error",
            tool_ids=["error_tool"],
        ),
        steps=[clarification_step],
    )
    portia.storage.save_plan(plan)
    plan_run = portia.run_plan(plan)

    assert plan_run.state == PlanRunState.FAILED
    assert plan_run.outputs.final_output
    final_output = plan_run.outputs.final_output.get_value()
    assert isinstance(final_output, str)
    assert "Something went wrong" in final_output


@pytest.mark.parametrize("agent", AGENTS)
@pytest.mark.parametrize(("llm_provider", "llm_model_name"), CORE_MODELS)
@pytest.mark.flaky(reruns=3)
def test_portia_run_query_with_soft_error(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
    agent: ExecutionAgentType,
) -> None:
    """Test running a query with error."""
    config = Config.from_default(
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        execution_agent_type=agent,
        storage_class=StorageClass.MEMORY,
    )

    class MyAdditionTool(AdditionTool):
        def run(self, _: ToolRunContext, a: int, b: int) -> int:  # noqa: ARG002
            raise ToolSoftError("Server Timeout")

    tool_registry = ToolRegistry([MyAdditionTool()])
    portia = Portia(config=config, tools=tool_registry)
    clarification_step = Step(
        tool_id="add_tool",
        task="Add 1 + 2",
        output="",
        inputs=[],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="raise an error",
            tool_ids=["add_tool"],
        ),
        steps=[clarification_step],
    )
    portia.storage.save_plan(plan)
    plan_run = portia.run_plan(plan)

    assert plan_run.state == PlanRunState.FAILED
    assert plan_run.outputs.final_output
    final_output = plan_run.outputs.final_output.get_value()
    assert isinstance(final_output, str)
    assert "Tool add_tool failed after retries" in final_output


@pytest.mark.parametrize(("llm_provider", "llm_model_name"), CORE_MODELS)
@pytest.mark.parametrize("agent", AGENTS)
@pytest.mark.flaky(reruns=3)
def test_portia_run_query_with_multiple_clarifications(
    llm_provider: LLMProvider,
    llm_model_name: LLMModel,
    agent: ExecutionAgentType,
) -> None:
    """Test running a query with multiple clarification."""
    config = Config.from_default(
        default_log_level=LogLevel.DEBUG,
        llm_provider=llm_provider,
        llm_model_name=llm_model_name,
        execution_agent_type=agent,
        storage_class=StorageClass.MEMORY,
    )

    class MyAdditionTool(AdditionTool):
        def run(self, ctx: ToolRunContext, a: int, b: int) -> int | Clarification:  # type: ignore  # noqa: PGH003
            if a == 1:
                return InputClarification(
                    plan_run_id=ctx.plan_run_id,
                    argument_name="a",
                    user_guidance="please try again",
                )
            return a + b

    test_clarification_handler = TestClarificationHandler()
    test_clarification_handler.clarification_response = 456
    tool_registry = ToolRegistry([MyAdditionTool()])
    portia = Portia(
        config=config,
        tools=tool_registry,
        execution_hooks=ExecutionHooks(clarification_handler=test_clarification_handler),
    )

    step_one = Step(
        tool_id="add_tool",
        task="Add 1 + 2",
        output="$step_one",
        inputs=[],
    )
    step_two = Step(
        tool_id="add_tool",
        task="Add $step_one + 40",
        output="",
        inputs=[
            Variable(
                name="$step_one",
                description="value for step one",
            ),
        ],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="raise a clarification",
            tool_ids=["clarification_tool"],
        ),
        steps=[step_one, step_two],
    )
    portia.storage.save_plan(plan)

    plan_run = portia.run_plan(plan)

    assert plan_run.state == PlanRunState.COMPLETE
    # 498 = 456 (clarification for value a in step 1) + 2 (value b in step 1)
    #  + 40 (value b in step 2)
    assert plan_run.outputs.final_output is not None
    assert plan_run.outputs.final_output.get_value() == 498
    assert plan_run.outputs.final_output.get_summary() is not None

    assert test_clarification_handler.received_clarification is not None
    assert test_clarification_handler.received_clarification.user_guidance == "please try again"


@patch("time.sleep")
def test_portia_run_query_with_multiple_async_clarifications(
    sleep_mock: MagicMock,
) -> None:
    """Test running a query with multiple clarification."""
    config = Config.from_default(
        default_log_level=LogLevel.DEBUG,
        storage_class=StorageClass.CLOUD,
    )

    resolved = False

    class MyAdditionTool(AdditionTool):
        def run(self, ctx: ToolRunContext, a: int, b: int) -> int | Clarification:  # type: ignore  # noqa: PGH003
            nonlocal resolved
            if not resolved:
                return ActionClarification(
                    plan_run_id=ctx.plan_run_id,
                    user_guidance="please try again",
                    action_url=HttpUrl("https://www.test.com"),
                )
            resolved = False
            return a + b

    class ActionClarificationHandler(ClarificationHandler):
        def handle_action_clarification(
            self,
            clarification: ActionClarification,
            on_resolution: Callable[[Clarification, object], None],
            on_error: Callable[[Clarification, object], None],  # noqa: ARG002
        ) -> None:
            self.received_clarification = clarification

            # Call on_resolution and set the tool to return correctly after 2 sleeps in the
            # wait_for_ready loop
            def on_sleep_called(_: float) -> None:
                nonlocal resolved
                if sleep_mock.call_count >= 2:
                    sleep_mock.reset_mock()
                    on_resolution(clarification, 1)
                    resolved = True

            sleep_mock.side_effect = on_sleep_called

    test_clarification_handler = ActionClarificationHandler()
    portia = Portia(
        config=config,
        tools=ToolRegistry([MyAdditionTool()]),
        execution_hooks=ExecutionHooks(clarification_handler=test_clarification_handler),
    )

    step_one = Step(
        tool_id="add_tool",
        task="Add 1 + 2",
        output="$step_one",
        inputs=[],
    )
    step_two = Step(
        tool_id="add_tool",
        task="Add $step_one + 1",
        output="",
        inputs=[
            Variable(
                name="$step_one",
                description="value for step one",
            ),
        ],
    )
    plan = Plan(
        plan_context=PlanContext(
            query="raise a clarification",
            tool_ids=["clarification_tool"],
        ),
        steps=[step_one, step_two],
    )
    portia.storage.save_plan(plan)

    plan_run = portia.run_plan(plan)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.outputs.final_output is not None
    assert plan_run.outputs.final_output.get_value() == 4
    assert plan_run.outputs.final_output.get_summary() is not None

    assert test_clarification_handler.received_clarification is not None
    assert test_clarification_handler.received_clarification.user_guidance == "please try again"


@pytest.mark.flaky(reruns=3)
def test_portia_run_query_with_conditional_steps() -> None:
    """Test running a query with conditional steps."""
    config = Config.from_default(storage_class=StorageClass.MEMORY)
    portia = Portia(config=config, tools=example_tool_registry)
    query = (
        "If the sum of 5 and 6 is greater than 10, then sum 4 + 5 and give me the answer, "
        "otherwise sum 1 + 2 and give me that as the answer"
    )

    plan_run = portia.run(query)
    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.outputs.final_output is not None
    assert "9" in str(plan_run.outputs.final_output.get_value())
    assert "3" not in str(plan_run.outputs.final_output.get_value())


def test_portia_run_query_with_example_registry() -> None:
    """Test we can run a query using the example registry."""
    config = Config.from_default()

    portia = Portia(config=config, tools=example_tool_registry)
    query = "Add 1 + 2 together and then write a haiku about the answer"

    plan_run = portia.run(query)
    assert plan_run.state == PlanRunState.COMPLETE


def test_portia_run_query_requiring_cloud_tools_not_authenticated() -> None:
    """Test that running a query requiring cloud tools fails but points user to sign up."""
    config = Config.from_default(portia_api_key=None, storage_class=StorageClass.MEMORY)

    portia = Portia(config=config)
    query = "Send an email to John Doe using the Gmail tool"

    with pytest.raises(PlanError) as e:
        portia.plan(query)
    assert "PORTIA_API_KEY is required to use Portia cloud tools." in str(e.value)

```

## File: tests/integration/test_portia_cloud.py

```python
"""Portia Cloud Tests."""

import uuid

import pytest

from portia.clarification import ActionClarification
from portia.cloud import PortiaCloudClient
from portia.config import Config, StorageClass
from portia.errors import ToolNotFoundError
from portia.execution_context import execution_context
from portia.plan_run import PlanRunState
from portia.portia import Portia
from portia.storage import PortiaCloudStorage
from portia.tool import PortiaRemoteTool, ToolHardError
from portia.tool_registry import (
    PortiaToolRegistry,
    ToolRegistry,
)
from tests.utils import AdditionTool, get_test_plan_run, get_test_tool_context


def test_portia_run_query_with_cloud() -> None:
    """Test running a simple query."""
    config = Config.from_default(storage_class=StorageClass.CLOUD)
    portia = Portia(config=config)
    query = "Where is the next Olympics being hosted?"

    plan_run = portia.run(query)

    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.outputs.final_output

    storage = portia.storage
    # check we can get items back
    storage.get_plan(plan_run.plan_id)
    storage.get_plan_run(plan_run.id)


def test_run_tool_error() -> None:
    """Test running a simple query."""
    config = Config.from_default(storage_class=StorageClass.CLOUD)

    registry = PortiaToolRegistry(
        config=config,
    )
    with pytest.raises(ToolNotFoundError):
        registry.get_tool("Not a Tool")

    tool = registry.get_tool("portia:tavily::search")
    assert isinstance(tool, PortiaRemoteTool)
    tool.client = PortiaCloudClient().get_client(config)
    ctx = get_test_tool_context()
    with pytest.raises(ToolHardError):
        tool.run(ctx)


def test_portia_run_query_with_cloud_and_local() -> None:
    """Test running a simple query."""
    config = Config.from_default(storage_class=StorageClass.CLOUD)

    registry = ToolRegistry([AdditionTool()]) + PortiaToolRegistry(
        config=config,
    )

    portia = Portia(config=config, tools=registry)
    query = "Get the temperature in London and Sydney and then add the two temperatures together."

    plan_run = portia.run(query)
    assert plan_run.state == PlanRunState.COMPLETE
    assert plan_run.outputs.final_output


def test_portia_run_query_with_oauth() -> None:
    """Test running a simple query."""
    portia = Portia()
    query = "Star the portiaai/portia-sdk-repo"

    with execution_context(end_user_id=str(uuid.uuid4())):
        plan_run = portia.run(query)

    assert plan_run.state == PlanRunState.NEED_CLARIFICATION
    assert len(plan_run.outputs.clarifications) == 1
    assert isinstance(plan_run.outputs.clarifications[0], ActionClarification)


def test_portia_cloud_storage() -> None:
    """Test cloud storage."""
    config = Config.from_default()
    storage = PortiaCloudStorage(config)
    (plan, plan_run) = get_test_plan_run()
    storage.save_plan(plan)
    assert storage.get_plan(plan.id) == plan
    storage.save_plan_run(plan_run)
    assert storage.get_plan_run(plan_run.id) == plan_run
    assert isinstance(storage.get_plan_runs(PlanRunState.IN_PROGRESS).results, list)


def test_default_portia_has_correct_tools() -> None:
    """Test that the default portia has the correct tools."""
    portia = Portia()
    tools = portia.tool_registry.get_tools()
    assert len(tools) > 0
    assert any(tool.id == "portia:google:gmail:search_email" for tool in tools)
    assert not any(tool.id == "portia:microsoft:outlook:draft_email" for tool in tools)


def test_portia_with_microsoft_tools() -> None:
    """Test that the default portia has the correct tools."""
    portia_registry = PortiaToolRegistry(config=Config.from_default())
    ms_tools = [tool for tool in portia_registry.get_tools() if "microsoft:outlook" in tool.id]
    assert len(ms_tools) > 0

    filtered_registry = portia_registry.with_default_tool_filter()
    filtered_tools = filtered_registry.get_tools()
    assert len(filtered_tools) > 0
    assert not any("portia:microsoft:outlook" in tool.id for tool in filtered_tools)

```

## File: tests/integration/test_model.py

```python
"""Integration tests for Model subclasses."""

from __future__ import annotations

from typing import TYPE_CHECKING, Any
from unittest import mock

import instructor
import pytest
from langchain_openai import ChatOpenAI
from openai import OpenAI
from pydantic import BaseModel, SecretStr

from portia.config import Config
from portia.model import (
    AnthropicGenerativeModel,
    AzureOpenAIGenerativeModel,
    GenerativeModel,
    GoogleGenAiGenerativeModel,
    Message,
    MistralAIGenerativeModel,
    OpenAIGenerativeModel,
)
from portia.planning_agents.base_planning_agent import StepsOrError

if TYPE_CHECKING:
    from collections.abc import Iterator


class Response(BaseModel):
    """Test response model."""

    message: str


CONFIG = Config.from_default()
MODELS: list[GenerativeModel] = [
    OpenAIGenerativeModel(model_name="gpt-4o-mini", api_key=CONFIG.openai_api_key),
    AnthropicGenerativeModel(
        model_name="claude-3-5-sonnet-latest",
        api_key=CONFIG.anthropic_api_key,
    ),
    MistralAIGenerativeModel(model_name="mistral-small-latest", api_key=CONFIG.mistralai_api_key),
    GoogleGenAiGenerativeModel(model_name="gemini-2.0-flash", api_key=CONFIG.google_api_key),
    AZURE_MODEL := AzureOpenAIGenerativeModel(
        model_name="gpt-4o-mini",
        api_key=SecretStr("dummy"),
        azure_endpoint="https://dummy.openai.azure.com",
    ),
]


@pytest.fixture(autouse=True)
def patch_azure_model() -> Iterator[None]:
    """Patch the Azure model to use the OpenAI client under the hood.

    When we have Azure access we can remove this patch.
    """

    class AzureOpenAIWrapper(OpenAI):
        """Mock the AzureOpenAI client."""

        def __init__(self, *args: Any, **kwargs: Any) -> None:
            new_kwargs = kwargs.copy()
            new_kwargs.pop("api_version")
            new_kwargs.pop("azure_endpoint")
            new_kwargs["api_key"] = CONFIG.openai_api_key.get_secret_value()
            super().__init__(*args, **new_kwargs)

    with (
        mock.patch.object(
            AZURE_MODEL,
            "_client",
            ChatOpenAI(model="gpt-4o-mini", api_key=CONFIG.openai_api_key),
        ),
        mock.patch.object(
            AZURE_MODEL,
            "_instructor_client",
            instructor.from_openai(
                OpenAI(api_key=CONFIG.openai_api_key.get_secret_value()),
                mode=instructor.Mode.JSON,
            ),
        ),
    ):
        yield


@pytest.fixture
def messages() -> list[Message]:
    """Create test messages."""
    return [
        Message(role="system", content="You are a helpful assistant."),
        Message(role="user", content="Generate me a random output."),
    ]


@pytest.mark.parametrize("model", MODELS)
def test_get_response(model: GenerativeModel, messages: list[Message]) -> None:
    """Test get_response for each model type."""
    response = model.get_response(messages)
    assert isinstance(response, Message)
    assert response.role is not None
    assert response.content is not None


@pytest.mark.parametrize("model", MODELS)
def test_get_structured_response(model: GenerativeModel, messages: list[Message]) -> None:
    """Test get_structured_response for each model type."""
    response = model.get_structured_response(messages, Response)
    assert isinstance(response, Response)
    assert response.message is not None


@pytest.mark.parametrize("model", MODELS)
def test_get_structured_response_steps_or_error(
    model: GenerativeModel,
    messages: list[Message],
) -> None:
    """Test get_structured_response with StepsOrError for each model type."""
    response = model.get_structured_response(messages, StepsOrError)
    assert isinstance(response, StepsOrError)

```

## File: portia/prefixed_uuid.py

```python
"""Prefixed UUIDs.

Support for various prefixed UUIDs that append the type of UUID to the ID.
"""

from __future__ import annotations

from typing import ClassVar, Self
from uuid import UUID, uuid4

from pydantic import BaseModel, Field, model_serializer, model_validator

PLAN_UUID_PREFIX = "plan"
PLAN_RUN_UUID_PREFIX = "prun"
CLARIFICATION_UUID_PREFIX = "clar"


class PrefixedUUID(BaseModel):
    """A UUID with an optional prefix.

    Attributes:
        prefix (str): A string prefix to prepend to the UUID. Empty by default.
        uuid (UUID): The UUID value.
        id (str): Computed property that combines the prefix and UUID.

    """

    prefix: ClassVar[str] = ""
    uuid: UUID = Field(default_factory=uuid4)

    def __str__(self) -> str:
        """Return the string representation of the PrefixedUUID.

        Returns:
            str: The prefixed UUID string.

        """
        return str(self.uuid) if self.prefix == "" else f"{self.prefix}-{self.uuid}"

    @model_serializer
    def serialize_model(self) -> str:
        """Serialize the PrefixedUUID to a string using the id property.

        Returns:
            str: The prefixed UUID string.

        """
        return str(self)

    @classmethod
    def from_string(cls, prefixed_uuid: str) -> Self:
        """Create a PrefixedUUID from a string in the format 'prefix-uuid'.

        Args:
            prefixed_uuid (str): A string in the format 'prefix-uuid'.

        Returns:
            Self: A new instance of PrefixedUUID.

        Raises:
            ValueError: If the string format is invalid or the prefix doesn't match.

        """
        if cls.prefix == "":
            return cls(uuid=UUID(prefixed_uuid))
        prefix, uuid_str = prefixed_uuid.split("-", maxsplit=1)
        if prefix != cls.prefix:
            raise ValueError(f"Prefix {prefix} does not match expected prefix {cls.prefix}")
        return cls(uuid=UUID(uuid_str))

    @model_validator(mode="before")
    @classmethod
    def validate_model(cls, v: str | dict) -> dict:
        """Validate the ID field."""
        if isinstance(v, dict):
            return v
        if cls.prefix == "":
            return {
                "uuid": UUID(v),
            }
        prefix, uuid_str = v.split("-", maxsplit=1)
        if prefix != cls.prefix:
            raise ValueError(f"Prefix {prefix} does not match expected prefix {cls.prefix}")
        return {
            "uuid": UUID(uuid_str),
        }

    def __hash__(self) -> int:
        """Make PrefixedUUID hashable by using the UUID's hash.

        Returns:
            int: Hash value of the UUID.

        """
        return hash(self.uuid)


class PlanUUID(PrefixedUUID):
    """A UUID for a plan."""

    prefix: ClassVar[str] = PLAN_UUID_PREFIX


class PlanRunUUID(PrefixedUUID):
    """A UUID for a PlanRun."""

    prefix: ClassVar[str] = PLAN_RUN_UUID_PREFIX


class ClarificationUUID(PrefixedUUID):
    """A UUID for a clarification."""

    prefix: ClassVar[str] = CLARIFICATION_UUID_PREFIX

```

## File: portia/tool_call.py

```python
"""Tool Call module contains classes that record the outcome of a single tool call.

The `ToolCallStatus` enum defines the various states a tool call can be in, such
as in progress, successful, requiring clarification, or failing.

The `ToolCallRecord` class is a Pydantic model used to capture details about a
specific tool call, including its status, input, output, and associated metadata.
"""

from typing import Any

from pydantic import BaseModel, ConfigDict

from portia.common import PortiaEnum
from portia.plan_run import PlanRunUUID


class ToolCallStatus(PortiaEnum):
    """The status of the tool call.

    Attributes:
        IN_PROGRESS: The tool is currently in progress.
        NEED_CLARIFICATION: The tool raise a clarification.
        SUCCESS: The tool executed successfully.
        FAILED: The tool raised an error.

    """

    IN_PROGRESS = "IN_PROGRESS"
    SUCCESS = "SUCCESS"
    NEED_CLARIFICATION = "NEED_CLARIFICATION"
    FAILED = "FAILED"


class ToolCallRecord(BaseModel):
    """Model that records the details of an individual tool call.

    This class captures all relevant information about a single tool call
    within a PlanRun including metadata, input and output data, and status.

    Attributes:
        tool_name (str): The name of the tool being called.
        plan_run_id (RunUUID): The unique identifier of the run to which this tool call
            belongs.
        step (int): The step number of the tool call in the PlanRun.
        end_user_id (str | None): The ID of the end user, if applicable. Can be None.
        additional_data (dict[str, str]): Additional data from the execution context.
        status (ToolCallStatus): The current status of the tool call (e.g., IN_PROGRESS, SUCCESS).
        input (Any): The input data passed to the tool call.
        output (Any): The output data returned from the tool call.
        latency_seconds (float): The latency in seconds for the tool call to complete.

    """

    model_config = ConfigDict(extra="forbid")

    tool_name: str
    plan_run_id: PlanRunUUID
    step: int
    # execution context is tracked here so we get a snapshot if its updated
    end_user_id: str | None
    additional_data: dict[str, str]
    # details of the tool call are below
    status: ToolCallStatus
    input: Any
    output: Any
    latency_seconds: float

```

## File: portia/clarification.py

```python
"""Clarification Primitives.

This module defines base classes and utilities for handling clarifications in the Portia system.
Clarifications represent questions or actions requiring user input to resolve, with different types
of clarifications for various use cases such as arguments, actions, inputs, multiple choices,
and value confirmations.
"""

from __future__ import annotations

from abc import ABC
from typing import Any, Self, Union

from pydantic import (
    BaseModel,
    Field,
    HttpUrl,
    field_serializer,
    model_validator,
)

from portia.common import PortiaEnum, Serializable
from portia.prefixed_uuid import ClarificationUUID, PlanRunUUID


class ClarificationCategory(PortiaEnum):
    """The category of a clarification.

    This enum defines the different categories of clarifications that can exist, such as arguments,
    actions, inputs, and more. It helps to categorize clarifications for easier
    handling and processing.
    """

    ACTION = "Action"
    INPUT = "Input"
    MULTIPLE_CHOICE = "Multiple Choice"
    VALUE_CONFIRMATION = "Value Confirmation"
    CUSTOM = "Custom"


class Clarification(BaseModel, ABC):
    """Base Model for Clarifications.

    A Clarification represents a question or action that requires user input to resolve. For example
    it could indicate the need for OAuth authentication, missing arguments for a tool
    or a user choice from a list.

    Attributes:
        id (ClarificationUUID): A unique identifier for this clarification.
        category (ClarificationCategory): The category of this clarification, indicating its type.
        response (SERIALIZABLE_TYPE_VAR | None): The user's response to this clarification, if any.
        step (int | None): The step this clarification is associated with, if applicable.
        user_guidance (str): Guidance provided to the user to assist with the clarification.
        resolved (bool): Whether the clarification has been resolved by the user.

    """

    id: ClarificationUUID = Field(
        default_factory=ClarificationUUID,
        description="A unique ID for this clarification",
    )
    plan_run_id: PlanRunUUID = Field(
        description="The run this clarification is for",
    )
    category: ClarificationCategory = Field(
        description="The category of this clarification",
    )
    response: Serializable | None = Field(
        default=None,
        description="The response from the user to this clarification.",
    )
    step: int | None = Field(default=None, description="The step this clarification is linked to.")
    user_guidance: str = Field(
        description="Guidance that is provided to the user to help clarification.",
    )
    resolved: bool = Field(
        default=False,
        description="Whether this clarification has been resolved.",
    )


class ActionClarification(Clarification):
    """Action-based clarification.

    Represents a clarification that involves an action, such as clicking a link. The response is set
    to `True` once the user has completed the action associated with the link.

    Attributes:
        category (ClarificationCategory): The category for this clarification, 'Action'.
        action_url (HttpUrl): The URL for the action that the user needs to complete.

    """

    category: ClarificationCategory = Field(
        default=ClarificationCategory.ACTION,
        description="The category of this clarification",
    )
    action_url: HttpUrl

    @field_serializer("action_url")
    def serialize_action_url(self, action_url: HttpUrl) -> str:
        """Serialize the action URL to a string.

        Args:
            action_url (HttpUrl): The URL to be serialized.

        Returns:
            str: The serialized string representation of the URL.

        """
        return str(action_url)


class InputClarification(Clarification):
    """Input-based clarification.

    Represents a clarification where the user needs to provide a value for a specific argument.
    This type of clarification is used when the user is prompted to enter a value.

    Attributes:
        category (ClarificationCategory): The category for this clarification, 'Input'.

    """

    argument_name: str = Field(
        description="The name of the argument that a value is needed for.",
    )
    category: ClarificationCategory = Field(
        default=ClarificationCategory.INPUT,
        description="The category of this clarification",
    )


class MultipleChoiceClarification(Clarification):
    """Multiple choice-based clarification.

    Represents a clarification where the user needs to select an option for a specific argument.
    The available options are provided, and the user must select one.

    Attributes:
        category (ClarificationCategory): The category for this clarification 'Multiple Choice'.
        options (list[Serializable]): The available options for the user to choose from.

    Methods:
        validate_response: Ensures that the user's response is one of the available options.

    """

    argument_name: str = Field(
        description="The name of the argument that a value is needed for.",
    )
    category: ClarificationCategory = Field(
        default=ClarificationCategory.MULTIPLE_CHOICE,
        description="The category of this clarification",
    )
    options: list[Serializable]

    @model_validator(mode="after")
    def validate_response(self) -> Self:
        """Ensure the provided response is an option.

        This method checks that the response provided by the user is one of the options. If not,
        it raises an error.

        Returns:
            Self: The validated instance.

        Raises:
            ValueError: If the response is not one of the available options.

        """
        if self.resolved and self.response not in self.options:
            raise ValueError(f"{self.response} is not a supported option")
        return self


class ValueConfirmationClarification(Clarification):
    """Value confirmation clarification.

    Represents a clarification where the user is presented with a value and must confirm or deny it.
    The clarification should be created with the response field already set, and the user indicates
    acceptance by setting the resolved flag to `True`.

    Attributes:
        category (ClarificationCategory): The category for this clarification, 'Value Confirmation'.

    """

    argument_name: str = Field(
        description="The name of the argument that whose value needs confirmation.",
    )
    category: ClarificationCategory = Field(
        default=ClarificationCategory.VALUE_CONFIRMATION,
        description="The category of this clarification",
    )


class CustomClarification(Clarification):
    """Custom clarifications.

    Allows the user to extend clarifications with arbitrary data.
    The user is responsible for handling this clarification type.

    Attributes:
        category (ClarificationCategory): The category for this clarification, 'Custom'.

    """

    category: ClarificationCategory = Field(
        default=ClarificationCategory.CUSTOM,
        description="The category of this clarification",
    )
    name: str = Field(
        description="The name of this clarification."
        "Used to differentiate between different types of custom clarifications.",
    )
    data: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional data for this clarification. Can include any serializable type.",
    )


"""Type that encompasses all possible clarification types."""
ClarificationType = Union[
    Clarification,
    InputClarification,
    ActionClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
    CustomClarification,
]

"""A list of clarifications of any type."""
ClarificationListType = list[ClarificationType]

```

## File: portia/plan.py

```python
"""Plan primitives used to define and execute runs.

This module defines the core objects that represent the plan for executing a PlanRun.
The `Plan` class is the main structure that holds a series of steps (`Step`) to be executed by an
agent in response to a query. Each step can have inputs, an associated tool, and an output.
Variables can be used within steps to reference other parts of the plan or constants.

Classes in this file include:

- `Variable`: A variable used in the plan, referencing outputs of previous steps or constants.
- `Step`: Defines a single task that an agent will execute, including inputs and outputs.
- `ReadOnlyStep`: A read-only version of a `Step` used for passing steps to agents.
- `PlanContext`: Provides context about the plan, including the original query and available tools.
- `Plan`: Represents the entire series of steps required to execute a query.

These classes facilitate the definition of runs that can be dynamically adjusted based on the
tools, inputs, and outputs defined in the plan.

"""

from __future__ import annotations

from pydantic import BaseModel, ConfigDict, Field, field_serializer, model_validator

from portia.prefixed_uuid import PlanUUID


class PlanBuilder:
    """A builder for creating plans.

    This class provides an interface for constructing plans step by step. Requires a step to be
    added to the plan before building it.

    Example:
    plan = PlanBuilder() \
                .step("Step 1", "tool_id_1", "output_1") \
                .step("Step 2", "tool_id_2", "output_2") \
                .input("input_1", "value_1") \
                .build()

    """

    query: str
    steps: list[Step]

    def __init__(self, query: str | None = None) -> None:
        """Initialize the builder with the plan query.

        Args:
            query (str): The original query given by the user.

        """
        self.query = query if query is not None else ""
        self.steps = []

    def step(
        self,
        task: str,
        tool_id: str | None = None,
        output: str | None = None,
        inputs: list[Variable] | None = None,
        condition: str | None = None,
    ) -> PlanBuilder:
        """Add a step to the plan.

        Args:
            task (str): The task to be completed by the step.
            tool_id (str | None): The ID of the tool used in this step, if applicable.
            output (str | None): The unique output ID for the result of this step.
            inputs (list[Variable] | None): The inputs to the step
            condition (str | None): A human readable condition which controls if the step should run
              or not.

        Returns:
            PlanBuilder: The builder instance with the new step added.

        """
        if inputs is None:
            inputs = []
        if output is None:
            output = f"$output_{len(self.steps)}"
        self.steps.append(
            Step(
                task=task,
                output=output,
                inputs=inputs,
                tool_id=tool_id,
                condition=condition,
            ),
        )
        return self

    def input(
        self,
        name: str,
        description: str | None = None,
        step_index: int | None = None,
    ) -> PlanBuilder:
        """Add an input variable to the chosen step in the plan (default is the last step).

        Inputs are outputs from previous steps.

        Args:
            name (str): The name of the input.
            description (str | None): The description of the input.
            step_index (int | None): The index of the step to add the input to. If not provided,
                                    the input will be added to the last step.

        Returns:
            PlanBuilder: The builder instance with the new input added.

        """
        step_index = self._get_step_index_or_raise(step_index)
        if description is None:
            description = ""
        self.steps[step_index].inputs.append(
            Variable(name=name, description=description),
        )
        return self

    def condition(
        self,
        condition: str,
        step_index: int | None = None,
    ) -> PlanBuilder:
        """Add a condition to the chosen step in the plan (default is the last step).

        Args:
            condition (str): The condition to be added to the chosen step.
            step_index (int | None): The index of the step to add the condition to.
                If not provided, the condition will be added to the last step.

        Returns:
            PlanBuilder: The builder instance with the new condition added.

        """
        step_index = self._get_step_index_or_raise(step_index)
        self.steps[step_index].condition = condition
        return self

    def build(self) -> Plan:
        """Build the plan.

        Returns:
            Plan: The built plan.

        """
        tool_ids = list({step.tool_id for step in self.steps if step.tool_id is not None})
        return Plan(
            plan_context=PlanContext(query=self.query, tool_ids=tool_ids),
            steps=self.steps,
        )

    def _get_step_index_or_raise(self, step_index: int | None) -> int:
        """Get the index of the step to add the condition to.

        Args:
            step_index (int | None): The index of the step to add the condition to. If not provided,
                                    it will default to the last step.

        Returns:
            int: The index of the step to add the condition to.

        """
        if step_index is None:
            step_index = len(self.steps) - 1
        if step_index < 0 or step_index >= len(self.steps):
            raise ValueError("Invalid step index or no steps in the plan")
        return step_index


class Variable(BaseModel):
    """A reference to an output of a step.

    Args:
        name (str): The name of the output to reference, e.g. $best_offers.
        description (str): A description of the output.

    """

    model_config = ConfigDict(extra="ignore")

    name: str = Field(
        description="The name of the output to reference, e.g. $best_offers.",
    )
    description: str = Field(
        description="A description of the output.",
    )

    def pretty_print(self) -> str:
        """Return the pretty print representation of the variable.

        Returns:
            str: A pretty print representation of the variable's name, and description.

        """
        return f"{self.name}: ({self.description})"


class Step(BaseModel):
    """A step in a PlanRun.

    A step represents a task in the run to be executed. It contains inputs (variables) and
    outputs, and may reference a tool to complete the task.

    Args:
        task (str): The task that needs to be completed by this step.
        inputs (list[Vairable]): The input to the step, as an output of a previous step.
        tool_id (str | None): The ID of the tool used in this step, if applicable.
        output (str): The unique output ID for the result of this step.

    """

    model_config = ConfigDict(extra="allow")

    task: str = Field(
        description="The task that needs to be completed by this step",
    )
    inputs: list[Variable] = Field(
        default=[],
        description=("The input to the step, as a reference to an output of a previous step."),
    )
    tool_id: str | None = Field(
        default=None,
        description="The ID of the tool listed in <Tools/>",
    )
    output: str = Field(
        ...,
        description="The unique output id of this step e.g. $best_offers.",
    )
    condition: str | None = Field(
        default=None,
        description="A human readable condition which controls if the step is run or not. "
        "If provided the condition will be evaluated and the step skipped if false. "
        "The step will run by default if not provided.",
    )

    def pretty_print(self) -> str:
        """Return the pretty print representation of the step.

        Returns:
            str: A pretty print representation of the step's task, inputs, tool_id, and output.

        """
        message = (
            f"- {self.task}\n"
            f"  Inputs: {', '.join([in_variable.pretty_print() for in_variable in self.inputs])}\n"
            f"  Tool ID: {self.tool_id}\n"
            f"  Output: {self.output}\n"
        )
        if self.condition:
            message += f"  Condition: {self.condition}\n"
        return message


class ReadOnlyStep(Step):
    """A read-only copy of a step, passed to agents for reference.

    This class creates an immutable representation of a step, which is used to ensure agents
    do not modify the original plan during execution.

    Args:
        step (Step): A step object from which to create a read-only version.

    """

    model_config = ConfigDict(frozen=True, extra="forbid")

    @classmethod
    def from_step(cls, step: Step) -> ReadOnlyStep:
        """Create a read-only step from a normal step.

        Args:
            step (Step): The step to be converted to read-only.

        Returns:
            ReadOnlyStep: A new read-only step.

        """
        return cls(
            task=step.task,
            inputs=step.inputs,
            tool_id=step.tool_id,
            output=step.output,
        )


class PlanContext(BaseModel):
    """Context for a plan.

    The plan context contains information about the original query and the tools available
    for the planning agent to use when generating the plan.

    Args:
        query (str): The original query given by the user.
        tool_ids (list[str]): A list of tool IDs available to the planning agent.

    """

    model_config = ConfigDict(extra="forbid")

    query: str = Field(description="The original query given by the user.")
    tool_ids: list[str] = Field(
        description="The list of tools IDs available to the planning agent.",
    )

    @field_serializer("tool_ids")
    def serialize_tool_ids(self, tool_ids: list[str]) -> list[str]:
        """Serialize the tool_ids to a sorted list.

        Returns:
            list[str]: The tool_ids as a sorted list.

        """
        return sorted(tool_ids)


class Plan(BaseModel):
    """A plan represents a series of steps that an agent should follow to execute the query.

    A plan defines the entire sequence of steps required to process a query and generate a result.
    It also includes the context in which the plan was created.

    Args:
        id (PlanUUID): A unique ID for the plan.
        plan_context (PlanContext): The context for when the plan was created.
        steps (list[Step]): The set of steps that make up the plan.

    """

    model_config = ConfigDict(extra="forbid")
    id: PlanUUID = Field(
        default_factory=PlanUUID,
        description="The ID of the plan.",
    )
    plan_context: PlanContext = Field(description="The context for when the plan was created.")
    steps: list[Step] = Field(description="The set of steps to solve the query.")

    def __str__(self) -> str:
        """Return the string representation of the plan.

        Returns:
            str: A string representation of the plan's ID, context, and steps.

        """
        return (
            f"PlanModel(id={self.id!r},"
            f"plan_context={self.plan_context!r}, "
            f"steps={self.steps!r}"
        )

    def pretty_print(self) -> str:
        """Return the pretty print representation of the plan.

        Returns:
            str: A pretty print representation of the plan's ID, context, and steps.

        """
        portia_tools = [tool for tool in self.plan_context.tool_ids if tool.startswith("portia:")]
        other_tools = [
            tool for tool in self.plan_context.tool_ids if not tool.startswith("portia:")
        ]
        tools_summary = f"{len(portia_tools)} portia tools, {len(other_tools)} other tools"
        return (
            f"Task: {self.plan_context.query}\n"
            f"Tools Available Summary: {tools_summary}\n"
            f"Steps:\n" + "\n".join([step.pretty_print() for step in self.steps])
        )

    @model_validator(mode="after")
    def validate_plan(self) -> Plan:
        """Validate the plan.

        Checks that all outputs + conditions are unique.

        Returns:
            Plan: The validated plan.

        """
        outputs = [step.output + (step.condition or "") for step in self.steps]
        if len(outputs) != len(set(outputs)):
            raise ValueError("Outputs + conditions must be unique")
        return self


class ReadOnlyPlan(Plan):
    """A read-only copy of a plan, passed to agents for reference.

    This class provides a non-modifiable view of a plan instance,
    ensuring that agents can access plan details without altering them.
    """

    model_config = ConfigDict(frozen=True, extra="forbid")

    @classmethod
    def from_plan(cls, plan: Plan) -> ReadOnlyPlan:
        """Create a read-only plan from a normal plan.

        Args:
            plan (Plan): The original plan instance to create a read-only copy from.

        Returns:
            ReadOnlyPlan: A new read-only instance of the provided plan.

        """
        return cls(
            id=plan.id,
            plan_context=plan.plan_context,
            steps=plan.steps,
        )

```

## File: portia/model.py

```python
"""LLM provider model classes for Portia Agents."""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any, Literal, TypeVar

import instructor
from anthropic import Anthropic
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_openai import AzureChatOpenAI, ChatOpenAI
from langsmith import wrappers
from openai import AzureOpenAI, OpenAI
from pydantic import BaseModel, SecretStr

from portia.common import validate_extras_dependencies

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel
    from openai.types.chat import ChatCompletionMessageParam


class Message(BaseModel):
    """Portia LLM message class."""

    role: Literal["user", "assistant", "system"]
    content: str

    @classmethod
    def from_langchain(cls, message: BaseMessage) -> Message:
        """Create a Message from a LangChain message.

        Args:
            message (BaseMessage): The LangChain message to convert.

        Returns:
            Message: The converted message.

        """
        if isinstance(message, HumanMessage):
            return cls.model_validate(
                {"role": "user", "content": message.content or ""},
            )
        if isinstance(message, AIMessage):
            return cls.model_validate(
                {"role": "assistant", "content": message.content or ""},
            )
        if isinstance(message, SystemMessage):
            return cls.model_validate(
                {"role": "system", "content": message.content or ""},
            )
        raise ValueError(f"Unsupported message type: {type(message)}")

    def to_langchain(self) -> BaseMessage:
        """Convert to LangChain BaseMessage sub-type.

        Returns:
            BaseMessage: The converted message, subclass of LangChain's BaseMessage.

        """
        if self.role == "user":
            return HumanMessage(content=self.content)
        if self.role == "assistant":
            return AIMessage(content=self.content)
        if self.role == "system":
            return SystemMessage(content=self.content)
        raise ValueError(f"Unsupported role: {self.role}")


BaseModelT = TypeVar("BaseModelT", bound=BaseModel)


class GenerativeModel(ABC):
    """Base class for all generative model clients."""

    provider_name: str

    def __init__(self, model_name: str) -> None:
        """Initialize the model.

        Args:
            model_name: The name of the model.

        """
        self.model_name = model_name

    @abstractmethod
    def get_response(self, messages: list[Message]) -> Message:
        """Given a list of messages, call the model and return its response as a new message.

        Args:
            messages (list[Message]): The list of messages to send to the model.

        Returns:
            Message: The response from the model.

        """

    @abstractmethod
    def get_structured_response(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
    ) -> BaseModelT:
        """Get a structured response from the model, given a Pydantic model.

        Args:
            messages (list[Message]): The list of messages to send to the model.
            schema (type[BaseModelT]): The Pydantic model to use for the response.

        Returns:
            BaseModelT: The structured response from the model.

        """

    def __str__(self) -> str:
        """Get the string representation of the model."""
        return f"{self.provider_name}/{self.model_name}"

    def __repr__(self) -> str:
        """Get the string representation of the model."""
        return f'{self.__class__.__name__}("{self.provider_name}/{self.model_name}")'


class LangChainGenerativeModel(GenerativeModel):
    """Base class for LangChain-based models."""

    provider_name: str

    def __init__(self, client: BaseChatModel, model_name: str) -> None:
        """Initialize with LangChain client.

        Args:
            client: LangChain chat model instance
            model_name: The name of the model

        """
        super().__init__(model_name)
        self._client = client

    def to_langchain(self) -> BaseChatModel:
        """Get the LangChain client."""
        return self._client

    def get_response(self, messages: list[Message]) -> Message:
        """Get response using LangChain model."""
        langchain_messages = [msg.to_langchain() for msg in messages]
        response = self._client.invoke(langchain_messages)
        return Message.from_langchain(response)

    def get_structured_response(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
        **kwargs: Any,
    ) -> BaseModelT:
        """Get structured response using LangChain model.

        Args:
            messages (list[Message]): The list of messages to send to the model.
            schema (type[BaseModelT]): The Pydantic model to use for the response.
            **kwargs: Additional keyword arguments to pass to the with_structured_output method.

        Returns:
            BaseModelT: The structured response from the model.

        """
        langchain_messages = [msg.to_langchain() for msg in messages]
        structured_client = self._client.with_structured_output(schema, **kwargs)
        response = structured_client.invoke(langchain_messages)
        if isinstance(response, schema):
            return response
        return schema.model_validate(response)


class OpenAIGenerativeModel(LangChainGenerativeModel):
    """OpenAI model implementation."""

    provider_name: str = "openai"

    def __init__(
        self,
        *,
        model_name: str,
        api_key: SecretStr,
        seed: int = 343,
        max_retries: int = 3,
        temperature: float = 0,
        **kwargs: Any,
    ) -> None:
        """Initialize with OpenAI client.

        Args:
            model_name: OpenAI model to use
            api_key: API key for OpenAI
            seed: Random seed for model generation
            max_retries: Maximum number of retries
            temperature: Temperature parameter
            **kwargs: Additional keyword arguments to pass to ChatOpenAI

        """
        if "disabled_params" not in kwargs:
            # This is a workaround for o3 mini to avoid parallel tool calls.
            # See https://github.com/langchain-ai/langchain/issues/25357
            kwargs["disabled_params"] = {"parallel_tool_calls": None}

        # Unfortunately you get errors from o3 mini with Langchain unless you set
        # temperature to 1. See https://github.com/ai-christianson/RA.Aid/issues/70
        temperature = 1 if "o3-mini" in model_name.lower() else temperature

        client = ChatOpenAI(
            name=model_name,
            model=model_name,
            seed=seed,
            api_key=api_key,
            max_retries=max_retries,
            temperature=temperature,
            **kwargs,
        )
        super().__init__(client, model_name)
        self._instructor_client = instructor.from_openai(
            client=wrappers.wrap_openai(OpenAI(api_key=api_key.get_secret_value())),
            mode=instructor.Mode.JSON,
        )
        self._seed = seed

    def get_structured_response(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
        **kwargs: Any,
    ) -> BaseModelT:
        """Call the model in structured output mode targeting the given Pydantic model.

        Args:
            messages (list[Message]): The list of messages to send to the model.
            schema (type[BaseModelT]): The Pydantic model to use for the response.
            **kwargs: Additional keyword arguments to pass to the model.

        Returns:
            BaseModelT: The structured response from the model.

        """
        if schema.__name__ == "StepsOrError":
            return self.get_structured_response_instructor(messages, schema)
        return super().get_structured_response(
            messages,
            schema,
            method="function_calling",
            **kwargs,
        )

    def get_structured_response_instructor(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
    ) -> BaseModelT:
        """Get structured response using instructor."""
        instructor_messages = [map_message_to_instructor(msg) for msg in messages]
        return self._instructor_client.chat.completions.create(
            response_model=schema,
            messages=instructor_messages,
            model=self.model_name,
            seed=self._seed,
        )


class AzureOpenAIGenerativeModel(LangChainGenerativeModel):
    """Azure OpenAI model implementation."""

    provider_name: str = "azure-openai"

    def __init__(  # noqa: PLR0913
        self,
        *,
        model_name: str,
        api_key: SecretStr,
        azure_endpoint: str,
        api_version: str = "2025-01-01-preview",
        seed: int = 343,
        max_retries: int = 3,
        temperature: float = 0,
        **kwargs: Any,
    ) -> None:
        """Initialize with Azure OpenAI client.

        Args:
            model_name: OpenAI model to use
            azure_endpoint: Azure OpenAI endpoint
            api_version: Azure API version
            seed: Random seed for model generation
            api_key: API key for Azure OpenAI
            max_retries: Maximum number of retries
            temperature: Temperature parameter (defaults to 1 for O_3_MINI, 0 otherwise)
            **kwargs: Additional keyword arguments to pass to AzureChatOpenAI

        """
        if "disabled_params" not in kwargs:
            # This is a workaround for o3 mini to avoid parallel tool calls.
            # See https://github.com/langchain-ai/langchain/issues/25357
            kwargs["disabled_params"] = {"parallel_tool_calls": None}

        # Unfortunately you get errors from o3 mini with Langchain unless you set
        # temperature to 1. See https://github.com/ai-christianson/RA.Aid/issues/70
        temperature = 1 if "o3-mini" in model_name.lower() else temperature

        client = AzureChatOpenAI(
            name=model_name,
            model=model_name,
            azure_endpoint=azure_endpoint,
            api_version=api_version,
            seed=seed,
            api_key=api_key,
            max_retries=max_retries,
            temperature=temperature,
            **kwargs,
        )
        super().__init__(client, model_name)
        self._instructor_client = instructor.from_openai(
            client=AzureOpenAI(
                api_key=api_key.get_secret_value(),
                azure_endpoint=azure_endpoint,
                api_version=api_version,
            ),
            mode=instructor.Mode.JSON,
        )
        self._seed = seed

    def get_structured_response(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
        **kwargs: Any,
    ) -> BaseModelT:
        """Call the model in structured output mode targeting the given Pydantic model.

        Args:
            messages (list[Message]): The list of messages to send to the model.
            schema (type[BaseModelT]): The Pydantic model to use for the response.
            **kwargs: Additional keyword arguments to pass to the model.

        Returns:
            BaseModelT: The structured response from the model.

        """
        if schema.__name__ == "StepsOrError":
            return self.get_structured_response_instructor(messages, schema)
        return super().get_structured_response(
            messages,
            schema,
            method="function_calling",
            **kwargs,
        )

    def get_structured_response_instructor(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
    ) -> BaseModelT:
        """Get structured response using instructor."""
        instructor_messages = [map_message_to_instructor(msg) for msg in messages]
        return self._instructor_client.chat.completions.create(
            response_model=schema,
            messages=instructor_messages,
            model=self.model_name,
            seed=self._seed,
        )


class AnthropicGenerativeModel(LangChainGenerativeModel):
    """Anthropic model implementation."""

    provider_name: str = "anthropic"

    def __init__(
        self,
        *,
        model_name: str = "claude-3-5-sonnet-latest",
        api_key: SecretStr,
        timeout: int = 120,
        max_retries: int = 3,
        **kwargs: Any,
    ) -> None:
        """Initialize with Anthropic client.

        Args:
            model_name: Name of the Anthropic model
            timeout: Request timeout in seconds
            max_retries: Maximum number of retries
            api_key: API key for Anthropic
            **kwargs: Additional keyword arguments to pass to ChatAnthropic

        """
        client = ChatAnthropic(
            model_name=model_name,
            timeout=timeout,
            max_retries=max_retries,
            api_key=api_key,
            **kwargs,
        )
        super().__init__(client, model_name)
        self._instructor_client = instructor.from_anthropic(
            client=wrappers.wrap_anthropic(Anthropic(api_key=api_key.get_secret_value())),
            mode=instructor.Mode.ANTHROPIC_JSON,
        )

    def get_structured_response(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
        **kwargs: Any,
    ) -> BaseModelT:
        """Call the model in structured output mode targeting the given Pydantic model.

        Args:
            messages (list[Message]): The list of messages to send to the model.
            schema (type[BaseModelT]): The Pydantic model to use for the response.
            **kwargs: Additional keyword arguments to pass to the model.

        Returns:
            BaseModelT: The structured response from the model.

        """
        if schema.__name__ == "StepsOrError":
            return self.get_structured_response_instructor(messages, schema)
        return super().get_structured_response(messages, schema, **kwargs)

    def get_structured_response_instructor(
        self,
        messages: list[Message],
        schema: type[BaseModelT],
    ) -> BaseModelT:
        """Get structured response using instructor."""
        instructor_messages = [map_message_to_instructor(msg) for msg in messages]
        return self._instructor_client.chat.completions.create(
            model=self.model_name,
            response_model=schema,
            messages=instructor_messages,
            max_tokens=2048,
        )


if validate_extras_dependencies("mistral", raise_error=False):
    from langchain_mistralai import ChatMistralAI
    from mistralai import Mistral

    class MistralAIGenerativeModel(LangChainGenerativeModel):
        """MistralAI model implementation."""

        provider_name: str = "mistralai"

        def __init__(
            self,
            *,
            model_name: str = "mistral-large-latest",
            api_key: SecretStr,
            max_retries: int = 3,
            **kwargs: Any,
        ) -> None:
            """Initialize with MistralAI client.

            Args:
                model_name: Name of the MistralAI model
                api_key: API key for MistralAI
                max_retries: Maximum number of retries
                **kwargs: Additional keyword arguments to pass to ChatMistralAI

            """
            client = ChatMistralAI(
                model_name=model_name,
                api_key=api_key,
                max_retries=max_retries,
                **kwargs,
            )
            super().__init__(client, model_name)
            self._instructor_client = instructor.from_mistral(
                client=Mistral(api_key=api_key.get_secret_value()),
                use_async=False,
            )

        def get_structured_response(
            self,
            messages: list[Message],
            schema: type[BaseModelT],
            **kwargs: Any,
        ) -> BaseModelT:
            """Call the model in structured output mode targeting the given Pydantic model.

            Args:
                messages (list[Message]): The list of messages to send to the model.
                schema (type[BaseModelT]): The Pydantic model to use for the response.
                **kwargs: Additional keyword arguments to pass to the model.

            Returns:
                BaseModelT: The structured response from the model.

            """
            if schema.__name__ == "StepsOrError":
                return self.get_structured_response_instructor(messages, schema)
            return super().get_structured_response(
                messages,
                schema,
                method="function_calling",
                **kwargs,
            )

        def get_structured_response_instructor(
            self,
            messages: list[Message],
            schema: type[BaseModelT],
        ) -> BaseModelT:
            """Get structured response using instructor."""
            instructor_messages = [map_message_to_instructor(msg) for msg in messages]
            return self._instructor_client.chat.completions.create(
                model=self.model_name,
                response_model=schema,
                messages=instructor_messages,
            )


if validate_extras_dependencies("google", raise_error=False):
    import google.generativeai as genai
    from langchain_google_genai import ChatGoogleGenerativeAI

    class GoogleGenAiGenerativeModel(LangChainGenerativeModel):
        """Google Generative AI (Gemini)model implementation."""

        provider_name: str = "google-generative-ai"

        def __init__(
            self,
            *,
            model_name: str = "gemini-2.0-flash",
            api_key: SecretStr,
            max_retries: int = 3,
            **kwargs: Any,
        ) -> None:
            """Initialize with Google Generative AI client.

            Args:
                model_name: Name of the Google Generative AI model
                api_key: API key for Google Generative AI
                max_retries: Maximum number of retries
                **kwargs: Additional keyword arguments to pass to ChatGoogleGenerativeAI

            """
            # Configure genai with the api key
            genai.configure(api_key=api_key.get_secret_value())  # pyright: ignore[reportPrivateImportUsage]

            client = ChatGoogleGenerativeAI(
                model=model_name,
                api_key=api_key,
                max_retries=max_retries,
                **kwargs,
            )
            super().__init__(client, model_name)
            self._instructor_client = instructor.from_gemini(
                client=genai.GenerativeModel(model_name=model_name),  # pyright: ignore[reportPrivateImportUsage]
                mode=instructor.Mode.GEMINI_JSON,
                use_async=False,
            )

        def get_structured_response(
            self,
            messages: list[Message],
            schema: type[BaseModelT],
            **_: Any,
        ) -> BaseModelT:
            """Get structured response from Google Generative AI model using instructor.

            NB. We use the instructor library to get the structured response, because the Google
            Generative AI API does not support Any-types in structured output mode. Instructor
            works around this by NOT using the API structured output mode, and instead using the
            text generation API to generate a JSON-formatted response, which is then parsed into
            the Pydantic model.

            Args:
                messages (list[Message]): The list of messages to send to the model.
                schema (type[BaseModelT]): The Pydantic model to use for the response.
                **kwargs: Additional keyword arguments to pass to the model.

            Returns:
                BaseModelT: The structured response from the model.

            """
            instructor_messages = [map_message_to_instructor(msg) for msg in messages]
            return self._instructor_client.messages.create(
                messages=instructor_messages,
                response_model=schema,
            )


def map_message_to_instructor(message: Message) -> ChatCompletionMessageParam:
    """Map a Message to ChatCompletionMessageParam.

    Args:
        message (Message): The message to map.

    Returns:
        ChatCompletionMessageParam: Message in the format expected by instructor.

    """
    match message:
        case Message(role="user", content=content):
            return {"role": "user", "content": content}
        case Message(role="assistant", content=content):
            return {"role": "assistant", "content": content}
        case Message(role="system", content=content):
            return {"role": "system", "content": content}
        case _:
            raise ValueError(f"Unsupported message role: {message.role}")

```

## File: portia/portia.py

```python
"""Portia classes that plan and execute runs for queries.

This module contains the core classes responsible for generating, managing, and executing plans
in response to queries. The `Portia` class serves as the main entry point, orchestrating the
planning and execution process. It uses various agents and tools to carry out tasks step by step,
saving the state of the run at each stage. It also handles error cases, clarification
requests, and run state transitions.

The `Portia` class provides methods to:

- Generate a plan for executing a query.
- Create and manage runs.
- Execute runs step by step, using agents to handle the execution of tasks.
- Resolve clarifications required during the execution of runs.
- Wait for runs to reach a state where they can be resumed.

Modules in this file work with different storage backends (memory, disk, cloud) and can handle
complex queries using various planning and execution agent configurations.

"""

from __future__ import annotations

import time
from typing import TYPE_CHECKING

from portia.clarification import (
    Clarification,
    ClarificationCategory,
)
from portia.cloud import PortiaCloudClient
from portia.config import (
    Config,
    ExecutionAgentType,
    PlanningAgentType,
    StorageClass,
)
from portia.errors import (
    InvalidPlanRunStateError,
    PlanError,
)
from portia.execution_agents.default_execution_agent import DefaultExecutionAgent
from portia.execution_agents.one_shot_agent import OneShotAgent
from portia.execution_agents.output import LocalOutput, Output
from portia.execution_agents.utils.final_output_summarizer import FinalOutputSummarizer
from portia.execution_context import (
    execution_context,
    get_execution_context,
    is_execution_context_set,
)
from portia.introspection_agents.default_introspection_agent import DefaultIntrospectionAgent
from portia.introspection_agents.introspection_agent import (
    BaseIntrospectionAgent,
    PreStepIntrospection,
    PreStepIntrospectionOutcome,
)
from portia.logger import logger, logger_manager
from portia.open_source_tools.llm_tool import LLMTool
from portia.plan import Plan, PlanContext, ReadOnlyPlan, ReadOnlyStep, Step
from portia.plan_run import PlanRun, PlanRunState, PlanRunUUID, ReadOnlyPlanRun
from portia.planning_agents.default_planning_agent import DefaultPlanningAgent
from portia.storage import (
    DiskFileStorage,
    InMemoryStorage,
    PortiaCloudStorage,
)
from portia.tool import ToolRunContext
from portia.tool_registry import (
    DefaultToolRegistry,
    PortiaToolRegistry,
    ToolRegistry,
)
from portia.tool_wrapper import ToolCallWrapper

if TYPE_CHECKING:
    from portia.clarification_handler import ClarificationHandler
    from portia.execution_agents.base_execution_agent import BaseExecutionAgent
    from portia.planning_agents.base_planning_agent import BasePlanningAgent
    from portia.tool import Tool


class ExecutionHooks:
    """Hooks that can be used to modify or add extra functionality to the run of a plan.

    Currently, the only hook is a clarification handler which can be used to handle clarifications
    that arise during the run of a plan.
    """

    def __init__(self, clarification_handler: ClarificationHandler | None = None) -> None:
        """Initialize ExecutionHooks with default values."""
        self.clarification_handler = clarification_handler


class Portia:
    """Portia client is the top level abstraction and entrypoint for most programs using the SDK.

    It is responsible for intermediating planning via PlanningAgents and
    execution via ExecutionAgents.
    """

    def __init__(
        self,
        config: Config | None = None,
        tools: ToolRegistry | list[Tool] | None = None,
        execution_hooks: ExecutionHooks | None = None,
    ) -> None:
        """Initialize storage and tools.

        Args:
            config (Config): The configuration to initialize the Portia client. If not provided, the
                default configuration will be used.
            tools (ToolRegistry | list[Tool]): The registry or list of tools to use. If not
                provided, the open source tool registry will be used, alongside the default tools
                from Portia cloud if a Portia API key is set.
            execution_hooks (ExecutionHooks | None): Hooks that can be used to modify or add
                extra functionality to the run of a plan.

        """
        self.config = config if config else Config.from_default()
        logger_manager.configure_from_config(self.config)
        self.execution_hooks = execution_hooks if execution_hooks else ExecutionHooks()
        if not self.config.has_api_key("portia_api_key"):
            logger().warning(
                "No Portia API key found, Portia cloud tools and storage will not be available.",
            )

        if isinstance(tools, ToolRegistry):
            self.tool_registry = tools
        elif isinstance(tools, list):
            self.tool_registry = ToolRegistry(tools)
        else:
            self.tool_registry = DefaultToolRegistry(self.config)

        match self.config.storage_class:
            case StorageClass.MEMORY:
                self.storage = InMemoryStorage()
            case StorageClass.DISK:
                self.storage = DiskFileStorage(storage_dir=self.config.must_get("storage_dir", str))
            case StorageClass.CLOUD:
                self.storage = PortiaCloudStorage(config=self.config)

    def run(
        self,
        query: str,
        tools: list[Tool] | list[str] | None = None,
        example_plans: list[Plan] | None = None,
    ) -> PlanRun:
        """End-to-end function to generate a plan and then execute it.

        This is the simplest way to plan and execute a query using the SDK.

        Args:
            query (str): The query to be executed.
            tools (list[Tool] | list[str] | None): List of tools to use for the query.
            If not provided all tools in the registry will be used.
            example_plans (list[Plan] | None): Optional list of example plans. If not
            provide a default set of example plans will be used.

        Returns:
            PlanRun: The run resulting from executing the query.

        """
        plan = self.plan(query, tools, example_plans)
        plan_run = self.create_plan_run(plan)
        return self.resume(plan_run)

    def plan(
        self,
        query: str,
        tools: list[Tool] | list[str] | None = None,
        example_plans: list[Plan] | None = None,
    ) -> Plan:
        """Plans how to do the query given the set of tools and any examples.

        Args:
            query (str): The query to generate the plan for.
            tools (list[Tool] | list[str] | None): List of tools to use for the query.
            If not provided all tools in the registry will be used.
            example_plans (list[Plan] | None): Optional list of example plans. If not
            provide a default set of example plans will be used.

        Returns:
            Plan: The plan for executing the query.

        Raises:
            PlanError: If there is an error while generating the plan.

        """
        if isinstance(tools, list):
            tools = [
                self.tool_registry.get_tool(tool) if isinstance(tool, str) else tool
                for tool in tools
            ]

        if not tools:
            tools = self.tool_registry.match_tools(query)
        logger().info(f"Running planning_agent for query - {query}")
        planning_agent = self._get_planning_agent()
        outcome = planning_agent.generate_steps_or_error(
            query=query,
            tool_list=tools,
            examples=example_plans,
        )
        if outcome.error:
            if (
                isinstance(self.tool_registry, DefaultToolRegistry)
                and not self.config.portia_api_key
            ):
                self._log_replan_with_portia_cloud_tools(outcome.error, query, example_plans)
            else:
                logger().error(f"Error in planning - {outcome.error}")
                raise PlanError(outcome.error)
        plan = Plan(
            plan_context=PlanContext(
                query=query,
                tool_ids=[tool.id for tool in tools],
            ),
            steps=outcome.steps,
        )
        self.storage.save_plan(plan)
        logger().info(
            f"Plan created with {len(plan.steps)} steps",
            plan=str(plan.id),
        )
        logger().debug(
            "Plan: " + plan.model_dump_json(indent=4),
        )

        return plan

    def run_plan(self, plan: Plan) -> PlanRun:
        """Run a plan.

        Args:
            plan (Plan): The plan to run.

        Returns:
            PlanRun: The resulting PlanRun object.

        """
        plan_run = self.create_plan_run(plan)
        return self.resume(plan_run)

    def resume(
        self,
        plan_run: PlanRun | None = None,
        plan_run_id: PlanRunUUID | str | None = None,
    ) -> PlanRun:
        """Resume a PlanRun.

        If a clarification handler was provided as part of the execution hooks, it will be used
        to handle any clarifications that are raised during the execution of the plan run.
        If no clarification handler was provided and a clarification is raised, the run will be
        returned in the `NEED_CLARIFICATION` state. The clarification will then need to be handled
        by the caller before the plan run is resumed.

        Args:
            plan_run (PlanRun | None): The PlanRun to resume. Defaults to None.
            plan_run_id (RunUUID | str | None): The ID of the PlanRun to resume. Defaults to
                None.

        Returns:
            PlanRun: The resulting PlanRun after execution.

        Raises:
            ValueError: If neither plan_run nor plan_run_id is provided.
            InvalidPlanRunStateError: If the plan run is not in a valid state to be resumed.

        """
        if not plan_run:
            if not plan_run_id:
                raise ValueError("Either plan_run or plan_run_id must be provided")

            parsed_id = (
                PlanRunUUID.from_string(plan_run_id)
                if isinstance(plan_run_id, str)
                else plan_run_id
            )
            plan_run = self.storage.get_plan_run(parsed_id)

        if plan_run.state not in [
            PlanRunState.NOT_STARTED,
            PlanRunState.IN_PROGRESS,
            PlanRunState.NEED_CLARIFICATION,
            PlanRunState.READY_TO_RESUME,
        ]:
            raise InvalidPlanRunStateError(plan_run.id)

        plan = self.storage.get_plan(plan_id=plan_run.plan_id)

        # if the run has execution context associated, but none is set then use it
        if not is_execution_context_set():
            with execution_context(plan_run.execution_context):
                return self.execute_plan_run_and_handle_clarifications(plan, plan_run)

        return self.execute_plan_run_and_handle_clarifications(plan, plan_run)

    def execute_plan_run_and_handle_clarifications(
        self,
        plan: Plan,
        plan_run: PlanRun,
    ) -> PlanRun:
        """Execute a plan run and handle any clarifications that are raised."""
        while plan_run.state not in [
            PlanRunState.COMPLETE,
            PlanRunState.FAILED,
        ]:
            plan_run.execution_context = get_execution_context()
            plan_run = self._execute_plan_run(plan, plan_run)

            # If we don't have a clarification handler, return the plan run even if a clarification
            # has been raised
            if not self.execution_hooks.clarification_handler:
                return plan_run

            clarifications = plan_run.get_outstanding_clarifications()
            for clarification in clarifications:
                self.execution_hooks.clarification_handler.handle(
                    clarification=clarification,
                    on_resolution=lambda c, r: self.resolve_clarification(c, r) and None,
                    on_error=lambda c, r: self.error_clarification(c, r) and None,
                )

            if len(clarifications) > 0:
                # If clarifications are handled synchronously, we'll go through this immediately.
                # If they're handled asynchronously, we'll wait for the plan run to be ready.
                plan_run = self.wait_for_ready(plan_run)

        return plan_run

    def resolve_clarification(
        self,
        clarification: Clarification,
        response: object,
        plan_run: PlanRun | None = None,
    ) -> PlanRun:
        """Resolve a clarification updating the run state as needed.

        Args:
            clarification (Clarification): The clarification to resolve.
            response (object): The response to the clarification.
            plan_run (PlanRun | None): Optional - the plan run being updated.

        Returns:
            PlanRun: The updated PlanRun.

        """
        if plan_run is None:
            plan_run = self.storage.get_plan_run(clarification.plan_run_id)

        matched_clarification = next(
            (c for c in plan_run.outputs.clarifications if c.id == clarification.id),
            None,
        )

        if not matched_clarification:
            raise InvalidPlanRunStateError("Could not match clarification to run")

        matched_clarification.resolved = True
        matched_clarification.response = response

        if len(plan_run.get_outstanding_clarifications()) == 0:
            self._set_plan_run_state(plan_run, PlanRunState.READY_TO_RESUME)

        logger().info(
            f"Clarification resolved with response: {matched_clarification.response}",
        )

        logger().debug(
            f"Clarification resolved: {matched_clarification.model_dump_json(indent=4)}",
        )
        self.storage.save_plan_run(plan_run)
        return plan_run

    def error_clarification(
        self,
        clarification: Clarification,
        error: object,
        plan_run: PlanRun | None = None,
    ) -> PlanRun:
        """Mark that there was an error handling the clarification."""
        logger().error(
            f"Error handling clarification with guidance '{clarification.user_guidance}': {error}",
        )
        if plan_run is None:
            plan_run = self.storage.get_plan_run(clarification.plan_run_id)
        self._set_plan_run_state(plan_run, PlanRunState.FAILED)
        return plan_run

    def wait_for_ready(  # noqa: C901
        self,
        plan_run: PlanRun,
        max_retries: int = 6,
        backoff_start_time_seconds: int = 7 * 60,
        backoff_time_seconds: int = 2,
    ) -> PlanRun:
        """Wait for the run to be in a state that it can be re-plan_run.

        This is generally because there are outstanding clarifications that need to be resolved.

        Args:
            plan_run (PlanRun): The PlanRun to wait for.
            max_retries (int): The maximum number of retries to wait for the run to be ready
                after the backoff period starts.
            backoff_start_time_seconds (int): The time after which the backoff period starts.
            backoff_time_seconds (int): The time to wait between retries after the backoff period
                starts.

        Returns:
            PlanRun: The updated PlanRun once it is ready to be re-plan_run.

        Raises:
            InvalidRunStateError: If the run cannot be waited for.

        """
        start_time = time.time()
        tries = 0
        if plan_run.state not in [
            PlanRunState.IN_PROGRESS,
            PlanRunState.NOT_STARTED,
            PlanRunState.READY_TO_RESUME,
            PlanRunState.NEED_CLARIFICATION,
        ]:
            raise InvalidPlanRunStateError("Cannot wait for run that is not ready to run")

        # These states can continue straight away
        if plan_run.state in [
            PlanRunState.IN_PROGRESS,
            PlanRunState.NOT_STARTED,
            PlanRunState.READY_TO_RESUME,
        ]:
            return plan_run

        plan = self.storage.get_plan(plan_run.plan_id)
        plan_run = self.storage.get_plan_run(plan_run.id)
        current_step_clarifications = plan_run.get_clarifications_for_step()
        while plan_run.state != PlanRunState.READY_TO_RESUME:
            if tries >= max_retries:
                raise InvalidPlanRunStateError("Run is not ready to resume after max retries")

            # if we've waited longer than the backoff time, start the backoff period
            if time.time() - start_time > backoff_start_time_seconds:
                tries += 1
                backoff_time_seconds *= 2

            # wait a couple of seconds as we're long polling
            time.sleep(backoff_time_seconds)

            step = plan.steps[plan_run.current_step_index]
            next_tool = self._get_tool_for_step(step, plan_run)
            if next_tool:
                tool_ready = next_tool.ready(
                    ToolRunContext(
                        execution_context=plan_run.execution_context,
                        plan_run_id=plan_run.id,
                        config=self.config,
                        clarifications=current_step_clarifications,
                    ),
                )
                logger().debug(f"Tool state for {next_tool.name} is ready={tool_ready}")
                if tool_ready:
                    for clarification in current_step_clarifications:
                        if clarification.category is ClarificationCategory.ACTION:
                            clarification.resolved = True
                            clarification.response = "complete"
                    if len(plan_run.get_outstanding_clarifications()) == 0:
                        self._set_plan_run_state(plan_run, PlanRunState.READY_TO_RESUME)
                else:
                    for clarification in current_step_clarifications:
                        logger().info(
                            f"Waiting for clarification {clarification.category} to be resolved",
                        )

            logger().info(f"New run state for {plan_run.id!s} is {plan_run.state!s}")

        logger().info(f"Run {plan_run.id!s} is ready to resume")

        return plan_run

    def _set_plan_run_state(self, plan_run: PlanRun, state: PlanRunState) -> None:
        """Set the state of a plan run and persist it to storage."""
        plan_run.state = state
        self.storage.save_plan_run(plan_run)

    def create_plan_run(self, plan: Plan) -> PlanRun:
        """Create a PlanRun from a Plan.

        Args:
            plan (Plan): The plan to create a plan run from.

        Returns:
            PlanRun: The created PlanRun object.

        """
        plan_run = PlanRun(
            plan_id=plan.id,
            state=PlanRunState.NOT_STARTED,
            execution_context=get_execution_context(),
        )
        self.storage.save_plan_run(plan_run)
        return plan_run

    def _execute_plan_run(self, plan: Plan, plan_run: PlanRun) -> PlanRun:
        """Execute the run steps, updating the run state as needed.

        Args:
            plan (Plan): The plan to execute.
            plan_run (PlanRun): The plan run to execute.

        Returns:
            Run: The updated run after execution.

        """
        self._set_plan_run_state(plan_run, PlanRunState.IN_PROGRESS)

        dashboard_url = self.config.must_get("portia_dashboard_url", str)

        dashboard_message = (
            (
                f" View in your Portia AI dashboard: "
                f"{dashboard_url}/dashboard/plan-runs?plan_run_id={plan_run.id!s}"
            )
            if self.config.storage_class == StorageClass.CLOUD
            else ""
        )

        logger().info(
            f"Plan Run State is updated to {plan_run.state!s}.{dashboard_message}",
        )

        last_executed_step_output = self._get_last_executed_step_output(plan, plan_run)
        introspection_agent = self._get_introspection_agent()
        for index in range(plan_run.current_step_index, len(plan.steps)):
            step = plan.steps[index]
            plan_run.current_step_index = index

            # Handle the introspection outcome
            (plan_run, pre_step_outcome) = self._handle_introspection_outcome(
                introspection_agent=introspection_agent,
                plan=plan,
                plan_run=plan_run,
                last_executed_step_output=last_executed_step_output,
            )
            if pre_step_outcome.outcome == PreStepIntrospectionOutcome.SKIP:
                continue
            if pre_step_outcome.outcome != PreStepIntrospectionOutcome.CONTINUE:
                self._log_final_output(plan_run, plan)
                return plan_run

            logger().info(
                f"Executing step {index}: {step.task}",
                plan=str(plan.id),
                plan_run=str(plan_run.id),
            )
            # we pass read only copies of the state to the agent so that the portia remains
            # responsible for handling the output of the agent and updating the state.
            agent = self._get_agent_for_step(
                step=ReadOnlyStep.from_step(step),
                plan_run=ReadOnlyPlanRun.from_plan_run(plan_run),
            )
            logger().debug(
                f"Using agent: {type(agent).__name__}",
                plan=str(plan.id),
                plan_run=str(plan_run.id),
            )
            try:
                last_executed_step_output = agent.execute_sync()
            except Exception as e:  # noqa: BLE001 - We want to capture all failures here
                error_output = LocalOutput(value=str(e))
                self._set_step_output(error_output, plan_run, step)
                plan_run.outputs.final_output = error_output
                self._set_plan_run_state(plan_run, PlanRunState.FAILED)
                logger().error(
                    "error: {error}",
                    error=e,
                    plan=str(plan.id),
                    plan_run=str(plan_run.id),
                )
                logger().debug(
                    f"Final run status: {plan_run.state!s}",
                    plan=str(plan.id),
                    plan_run=str(plan_run.id),
                )
                return plan_run
            else:
                self._set_step_output(last_executed_step_output, plan_run, step)
                logger().info(
                    f"Step output - {last_executed_step_output.get_summary()!s}",
                )

            if self._raise_clarifications(plan_run, last_executed_step_output, plan):
                return plan_run

            # persist at the end of each step
            self.storage.save_plan_run(plan_run)
            logger().debug(
                f"New PlanRun State: {plan_run.model_dump_json(indent=4)}",
            )

        if last_executed_step_output:
            plan_run.outputs.final_output = self._get_final_output(
                plan,
                plan_run,
                last_executed_step_output,
            )
        self._set_plan_run_state(plan_run, PlanRunState.COMPLETE)
        self._log_final_output(plan_run, plan)
        return plan_run

    def _log_final_output(self, plan_run: PlanRun, plan: Plan) -> None:
        logger().debug(
            f"Final run status: {plan_run.state!s}",
            plan=str(plan.id),
            plan_run=str(plan_run.id),
        )
        if plan_run.outputs.final_output:
            logger().info(
                f"Final output: {plan_run.outputs.final_output.get_summary()!s}",
            )

    def _get_last_executed_step_output(self, plan: Plan, plan_run: PlanRun) -> Output | None:
        """Get the output of the last executed step.

        Args:
            plan (Plan): The plan containing steps.
            plan_run (PlanRun): The plan run to get the output from.

        Returns:
            Output | None: The output of the last executed step.

        """
        return next(
            (
                plan_run.outputs.step_outputs[step.output]
                for i in range(plan_run.current_step_index, -1, -1)
                if i < len(plan.steps)
                and (step := plan.steps[i]).output in plan_run.outputs.step_outputs
                and (step_output := plan_run.outputs.step_outputs[step.output])
                and step_output.get_value() != PreStepIntrospectionOutcome.SKIP
            ),
            None,
        )

    def _handle_introspection_outcome(
        self,
        introspection_agent: BaseIntrospectionAgent,
        plan: Plan,
        plan_run: PlanRun,
        last_executed_step_output: Output | None,
    ) -> tuple[PlanRun, PreStepIntrospection]:
        """Handle the outcome of the pre-step introspection.

        Args:
            introspection_agent (BaseIntrospectionAgent): The introspection agent to use.
            plan (Plan): The plan being executed.
            plan_run (PlanRun): The plan run being executed.
            last_executed_step_output (Output | None): The output of the last step executed.

        Returns:
            tuple[PlanRun, PreStepIntrospectionOutcome]: The updated plan run and the
                outcome of the introspection.

        """
        current_step_index = plan_run.current_step_index
        step = plan.steps[current_step_index]
        if not step.condition:
            return (
                plan_run,
                PreStepIntrospection(
                    outcome=PreStepIntrospectionOutcome.CONTINUE,
                    reason="No condition to evaluate.",
                ),
            )

        logger().info(
            f"Running Pre Introspection for Step #{current_step_index}, "
            f"evaluating condition: #{step.condition}",
        )

        pre_step_outcome = introspection_agent.pre_step_introspection(
            plan=ReadOnlyPlan.from_plan(plan),
            plan_run=ReadOnlyPlanRun.from_plan_run(plan_run),
        )

        log_message = (
            f"Pre Introspection Outcome for Step #{current_step_index}: "
            f"{pre_step_outcome.outcome} for {step.output}. "
            f"Reason: {pre_step_outcome.reason}",
        )

        if pre_step_outcome.outcome == PreStepIntrospectionOutcome.FAIL:
            logger().error(*log_message)
        else:
            logger().debug(*log_message)

        match pre_step_outcome.outcome:
            case PreStepIntrospectionOutcome.SKIP:
                output = LocalOutput(
                    value=PreStepIntrospectionOutcome.SKIP,
                    summary=pre_step_outcome.reason,
                )
                self._set_step_output(output, plan_run, step)
            case PreStepIntrospectionOutcome.COMPLETE:
                output = LocalOutput(
                    value=PreStepIntrospectionOutcome.COMPLETE,
                    summary=pre_step_outcome.reason,
                )
                self._set_step_output(output, plan_run, step)
                if last_executed_step_output:
                    plan_run.outputs.final_output = self._get_final_output(
                        plan,
                        plan_run,
                        last_executed_step_output,
                    )
                self._set_plan_run_state(plan_run, PlanRunState.COMPLETE)
            case PreStepIntrospectionOutcome.FAIL:
                failed_output = LocalOutput(
                    value=PreStepIntrospectionOutcome.FAIL,
                    summary=pre_step_outcome.reason,
                )
                self._set_step_output(failed_output, plan_run, step)
                plan_run.outputs.final_output = failed_output
                self._set_plan_run_state(plan_run, PlanRunState.FAILED)
        return (plan_run, pre_step_outcome)

    def _get_planning_agent(self) -> BasePlanningAgent:
        """Get the planning_agent based on the configuration.

        Returns:
            BasePlanningAgent: The planning agent to be used for generating plans.

        """
        cls: type[BasePlanningAgent]
        match self.config.planning_agent_type:
            case PlanningAgentType.DEFAULT:
                cls = DefaultPlanningAgent

        return cls(self.config)

    def _get_final_output(self, plan: Plan, plan_run: PlanRun, step_output: Output) -> Output:
        """Get the final output and add summarization to it.

        Args:
            plan (Plan): The plan to execute.
            plan_run (PlanRun): The PlanRun to execute.
            step_output (Output): The output of the last step.

        """
        final_output = LocalOutput(
            value=step_output.get_value(),
            summary=None,
        )

        try:
            summarizer = FinalOutputSummarizer(config=self.config)
            summary = summarizer.create_summary(
                plan_run=ReadOnlyPlanRun.from_plan_run(plan_run),
                plan=ReadOnlyPlan.from_plan(plan),
            )
            final_output.summary = summary

        except Exception as e:  # noqa: BLE001
            logger().warning(f"Error summarising run: {e}")

        return final_output

    def _raise_clarifications(self, plan_run: PlanRun, step_output: Output, plan: Plan) -> bool:
        """Update the plan run based on any clarifications raised.

        Args:
            plan_run (PlanRun): The PlanRun to execute.
            step_output (Output): The output of the last step.
            plan (Plan): The plan to execute.

        Returns:
            bool: True if clarification is needed and run execution should stop.

        """
        output_value = step_output.get_value()
        if isinstance(output_value, Clarification) or (
            isinstance(output_value, list)
            and len(output_value) > 0
            and all(isinstance(item, Clarification) for item in output_value)
        ):
            new_clarifications = (
                [output_value] if isinstance(output_value, Clarification) else output_value
            )
            for clarification in new_clarifications:
                clarification.step = plan_run.current_step_index
                logger().info(
                    f"Clarification requested - category: {clarification.category}, "
                    f"user_guidance: {clarification.user_guidance}.",
                    plan=str(plan.id),
                    plan_run=str(plan_run.id),
                )
                logger().debug(
                    f"Clarification requested: {clarification.model_dump_json(indent=4)}",
                )

            plan_run.outputs.clarifications = plan_run.outputs.clarifications + new_clarifications
            self._set_plan_run_state(plan_run, PlanRunState.NEED_CLARIFICATION)
            return True
        return False

    def _get_tool_for_step(self, step: Step, plan_run: PlanRun) -> Tool | None:
        if not step.tool_id:
            return None
        if step.tool_id == LLMTool.LLM_TOOL_ID:
            # Special case LLMTool so it doesn't need to be in all tool registries
            child_tool = LLMTool()
        else:
            child_tool = self.tool_registry.get_tool(step.tool_id)
        return ToolCallWrapper(
            child_tool=child_tool,
            storage=self.storage,
            plan_run=plan_run,
        )

    def _get_agent_for_step(
        self,
        step: Step,
        plan_run: PlanRun,
    ) -> BaseExecutionAgent:
        """Get the appropriate agent for executing a given step.

        Args:
            step (Step): The step for which the agent is needed.
            plan_run (PlanRun): The run associated with the step.

        Returns:
            BaseAgent: The agent to execute the step.

        """
        tool = self._get_tool_for_step(step, plan_run)
        cls: type[BaseExecutionAgent]
        match self.config.execution_agent_type:
            case ExecutionAgentType.ONE_SHOT:
                cls = OneShotAgent
            case ExecutionAgentType.DEFAULT:
                cls = DefaultExecutionAgent
        return cls(
            step,
            plan_run,
            self.config,
            tool,
        )

    def _log_replan_with_portia_cloud_tools(
        self,
        original_error: str,
        query: str,
        example_plans: list[Plan] | None = None,
    ) -> None:
        """Generate a plan using Portia cloud tools for users who's plans fail without them."""
        unauthenticated_client = PortiaCloudClient.new_client(
            self.config,
            allow_unauthenticated=True,
        )
        portia_registry = PortiaToolRegistry(
            client=unauthenticated_client,
        ).with_default_tool_filter()
        cloud_registry = self.tool_registry + portia_registry
        tools = cloud_registry.match_tools(query)
        planning_agent = self._get_planning_agent()
        replan_outcome = planning_agent.generate_steps_or_error(
            query=query,
            tool_list=tools,
            examples=example_plans,
        )
        if not replan_outcome.error:
            tools_used = ", ".join([str(step.tool_id) for step in replan_outcome.steps])
            logger().error(
                f"Error in planning - {original_error.rstrip('.')}.\n"
                f"Replanning with Portia cloud tools would successfully generate a plan using "
                f"tools: {tools_used}.\n"
                f"Go to https://app.portialabs.ai to sign up.",
            )
            raise PlanError(
                "PORTIA_API_KEY is required to use Portia cloud tools.",
            ) from PlanError(original_error)

    def _get_introspection_agent(self) -> BaseIntrospectionAgent:
        return DefaultIntrospectionAgent(self.config)

    def _set_step_output(self, output: Output, plan_run: PlanRun, step: Step) -> None:
        """Set the output for a step."""
        plan_run.outputs.step_outputs[step.output] = output
        self._persist_step_state(plan_run, step)

    def _persist_step_state(self, plan_run: PlanRun, step: Step) -> None:
        """Ensure the plan run state is persisted to storage."""
        step_output = plan_run.outputs.step_outputs[step.output]
        if isinstance(step_output, LocalOutput) and self.config.exceeds_output_threshold(
            step_output.serialize_value(),
        ):
            step_output = self.storage.save_plan_run_output(step.output, step_output, plan_run.id)
            plan_run.outputs.step_outputs[step.output] = step_output

        self.storage.save_plan_run(plan_run)

```

## File: portia/tool.py

```python
"""Tools module.

This module defines an abstract base class for tools, providing a structure for creating custom
tools that can integrate with external systems. It includes an implementation of a base `Tool` class
that defines common attributes and behaviors, such as a unique ID and name. Child classes should
implement the `run` method to define the specific logic for interacting with the external systems
or performing actions.

The module also contains `PortiaRemoteTool`, a subclass of `Tool`, which implements the logic to
interact with Portia Cloud, including handling API responses and tool errors.

The tools in this module are designed to be extendable, allowing users to create their own tools
while relying on common functionality provided by the base class.
"""

from __future__ import annotations

import asyncio
import json
from abc import abstractmethod
from functools import partial
from typing import Any, Generic, Self

import httpx
from jsonref import replace_refs
from langchain_core.tools import StructuredTool
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    HttpUrl,
    ValidationError,
    field_serializer,
    model_validator,
)

from portia.clarification import (
    ActionClarification,
    Clarification,
    ClarificationCategory,
    ClarificationListType,
    ClarificationUUID,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)
from portia.common import SERIALIZABLE_TYPE_VAR, combine_args_kwargs
from portia.config import Config
from portia.errors import InvalidToolDescriptionError, ToolHardError, ToolSoftError
from portia.execution_agents.execution_utils import is_clarification
from portia.execution_agents.output import LocalOutput, Output
from portia.execution_context import ExecutionContext
from portia.logger import logger
from portia.mcp_session import McpClientConfig, get_mcp_session
from portia.plan_run import PlanRunUUID
from portia.templates.render import render_template

"""MAX_TOOL_DESCRIPTION_LENGTH is the max length tool descriptions can be to respect API limits."""
MAX_TOOL_DESCRIPTION_LENGTH = 1024


class ToolRunContext(BaseModel):
    """Context passed to tools when running.

    Attributes:
        execution_context(ExecutionContext): The execution context the tool is running in.
        plan_run_id(RunUUID): The run id the tool run is part of.
        config(Config): The config for the SDK as a whole.
        clarifications(ClarificationListType): Relevant clarifications for this tool plan_run.

    """

    model_config = ConfigDict(extra="forbid")

    execution_context: ExecutionContext
    plan_run_id: PlanRunUUID
    config: Config
    clarifications: ClarificationListType


class _ArgsSchemaPlaceholder(BaseModel):
    """Placeholder ArgsSchema for tools that take no arguments."""


class Tool(BaseModel, Generic[SERIALIZABLE_TYPE_VAR]):
    """Abstract base class for a tool.

    This class serves as the blueprint for all tools. Child classes must implement the `run` method.

    Attributes:
    id (str): A unique identifier for the tool.
        This must be unique as collisions in a tool registry will lead to errors.
    name (str): The name of the tool. The name is informational only but useful for debugging.
    description (str): Purpose of the tool and usage.
        This is important information for the planning_agent module to know when and
        how to use this tool.
    args_schema (type[BaseModel]): The schema defining the expected input arguments for the tool.
        We use Pydantic models to define these types.
    output_schema (tuple[str, str]): A tuple containing the type and description of the tool's
        output. To maximize the advantages of using an agentic approach this doesn't need to be
        tightly defined. Instead it should give just a high level overview of the type and
        contents of the tools output.
    should_summarize (bool): Indicates whether the tool's output should be automatically summarized
        by the summarizer agent. For some tools summarization is useful (for example: a tool
        that fetches the latest news) whereas other tools it's not (for example: a tool
        that fetches raw price data).

    """

    model_config = ConfigDict(extra="forbid", arbitrary_types_allowed=True)

    id: str = Field(description="ID of the tool")
    name: str = Field(description="Name of the tool")
    description: str = Field(description="Purpose of the tool and usage")
    args_schema: type[BaseModel] = Field(default_factory=lambda _: _ArgsSchemaPlaceholder)
    output_schema: tuple[str, str] = Field(
        ...,
        description="Output schema of the tool",
        examples=["(TYPE, DESCRIPTION)", "(json, json with API response, single object)"],
    )
    should_summarize: bool = Field(
        default=False,
        description="Whether the tool's output requires a summary. "
        "Tools may not require a summary if they already produce a nice textual output.",
    )

    def ready(self, ctx: ToolRunContext) -> bool:  # noqa: ARG002
        """Check whether the tool can be plan_run.

        This method can be implemented by subclasses to allow checking if the tool can be plan_run.
        It may run any authentication logic or other required checks before returning its status.
        If left unimplemented will always return true.

        Args:
            ctx (ToolRunContext): Context of the tool run

        Returns:
            bool: Whether the tool is ready to run

        """
        return True

    @abstractmethod
    def run(
        self,
        ctx: ToolRunContext,
        *args: Any,
        **kwargs: Any,
    ) -> SERIALIZABLE_TYPE_VAR | Clarification:
        """Run the tool.

        This method must be implemented by subclasses to define the tool's specific behavior.

        Args:
            ctx (ToolRunContext): Context of the tool execution
            args (Any): The arguments passed to the tool for execution.
            kwargs (Any): The keyword arguments passed to the tool for execution.

        Returns:
            Any: The result of the tool's execution which can be any serializable type
            or a clarification.

        """

    def _run(
        self,
        ctx: ToolRunContext,
        *args: Any,
        **kwargs: Any,
    ) -> Output:
        """Invoke the Tool.run function and handle converting the result into an Output object.

        This is the entry point for agents to invoke a tool.

        Args:
            ctx (ToolRunContext): The context for the tool.
            *args (Any): Additional positional arguments for the tool function.
            **kwargs (Any): Additional keyword arguments for the tool function.

        Returns:
            Output: The tool's output wrapped in an Output object.

        Raises:
            ToolSoftError: If an error occurs and it is not already a Hard or Soft Tool error.

        """
        try:
            output = self.run(ctx, *args, **kwargs)
        except Exception as e:
            # check if error is wrapped as a Hard or Soft Tool Error.
            # if not wrap as ToolSoftError
            if not isinstance(e, ToolHardError) and not isinstance(e, ToolSoftError):
                raise ToolSoftError(e) from e
            raise

        # handle clarifications cleanly
        if is_clarification(output):
            clarifications = output if isinstance(output, list) else [output]
            return LocalOutput[list[Clarification]](
                value=clarifications,
            )
        return LocalOutput[SERIALIZABLE_TYPE_VAR](value=output)  # type: ignore  # noqa: PGH003

    def _run_with_artifacts(
        self,
        ctx: ToolRunContext,
        *args: Any,
        **kwargs: Any,
    ) -> tuple[str, Output]:
        """Invoke the Tool.run function and handle converting to an Output object.

        This function returns a tuple consisting of the output and an Output object, as expected by
        langchain tools. It captures the output (artifact) directly instead of serializing
        it to a string first.

        Args:
            ctx (ToolRunContext): The context for the tool.
            *args (Any): Additional positional arguments for the tool function.
            **kwargs (Any): Additional keyword arguments for the tool function.

        Returns:
            tuple[str, Output]: A tuple containing the output and the Output.

        """
        intermediate_output = self._run(ctx, *args, **kwargs)
        return (intermediate_output.get_value(), intermediate_output)  # type: ignore  # noqa: PGH003

    def _generate_tool_description(self) -> str:
        """Generate tool descriptions.

        This function generates a comprehensive description of the tool, including its name,
        arguments, and output schema. The description is rendered using a Jinja template.

        Returns:
            str: The generated tool description in XML format.

        """
        args = []
        args_name_description_dict = []
        out_type = self.output_schema[0]
        out_description = self.output_schema[1]
        schema = self.args_json_schema()
        for arg, attribute in schema["properties"].items():
            arg_dict = {
                "name": arg,
                "type": attribute.get("type", None),
                "required": arg in schema.get("required", []),
            }
            if attribute.get("enum", None):
                arg_dict["enum"] = attribute.get("enum")
            if attribute.get("default", None):
                arg_dict["default"] = attribute.get("default")

            args_name_description_dict.append(arg_dict)
            if "type" in attribute:
                args.append(f"{arg}: '{attribute['type']}'")

        description = self.description.replace("\n", " ")
        overview = f"{self.name.replace(' ', '_')}({', '.join(args)})"

        if out_type:
            overview += f" -> {out_type}"

        template_dict = {
            "overview": overview,
            "overview_description": description,
            "args": args_name_description_dict,
            "output_description": out_description,
        }

        return render_template(
            "tool_description.xml.jinja",
            tool=template_dict,
        )

    @model_validator(mode="after")
    def check_description_length(self) -> Self:
        """Check that the description is less than 1024 characters.

        OpenAI has a maximum function description length of 1024 characters. This validator
        ensures that the tool description does not exceed this limit.

        Returns:
            Self: The current instance of the tool.

        Raises:
            InvalidToolDescriptionError: If the description exceeds the maximum length.

        """
        description_length = len(self._generate_tool_description())
        if description_length > MAX_TOOL_DESCRIPTION_LENGTH:
            raise InvalidToolDescriptionError(self.id)
        return self

    def to_langchain(self, ctx: ToolRunContext) -> StructuredTool:
        """Return a LangChain representation of this tool.

        This function provides a LangChain-compatible version of the tool. The response format is
        the default one without including artifacts. The ExecutionContext is baked into the
        StructuredTool via a partial run function.

        Args:
            ctx (ToolRunContext): The context for the tool.

        Returns:
            StructuredTool: The LangChain-compatible representation of the tool, including the
            tool's name, description, and argument schema, with the execution context baked
            into the function.

        """
        return StructuredTool(
            name=self.name.replace(" ", "_"),
            description=self._generate_tool_description(),
            args_schema=self.args_schema,
            func=partial(self._run, ctx),
        )

    def to_langchain_with_artifact(self, ctx: ToolRunContext) -> StructuredTool:
        """Return a LangChain representation of this tool with content and artifact.

        This function provides a LangChain-compatible version of the tool, where the response format
        includes both the content and the artifact. The ToolRunContext is baked into the
        StructuredTool via a partial run function for capturing output directly.

        Args:
            ctx (ToolRunContext): The context for the tool.

        Returns:
            StructuredTool: The LangChain-compatible representation of the tool, including the
            tool's name, description, argument schema, and the ability to return both content
            and artifact.

        """
        return StructuredTool(
            name=self.name.replace(" ", "_"),
            description=self._generate_tool_description(),
            args_schema=self.args_schema,
            func=partial(self._run_with_artifacts, ctx),
            return_direct=True,
            response_format="content_and_artifact",
        )

    def args_json_schema(self) -> dict[str, Any]:
        """Return the json_schema for the tool args.

        This function retrieves the JSON schema for the tool's arguments, which defines the expected
        input structure.

        Returns:
            dict[str, Any]: The JSON schema representing the tool's arguments.

        """
        return replace_refs(self.args_schema.model_json_schema())  # type: ignore  # noqa: PGH003

    def __str__(self) -> str:
        """Return the string representation.

        This method generates a string representation of the tool, including its ID, name,
        description, argument schema, and output schema.

        Returns:
            str: A string representation of the tool.

        """
        return (
            f"ToolModel(id={self.id!r}, name={self.name!r}, "
            f"description={self.description!r}, "
            f"args_schema={self.args_schema.__name__!r}, "
            f"output_schema={self.output_schema!r})"
        )

    @field_serializer("args_schema")
    def serialize_args_schema(self, value: type[BaseModel]) -> str:
        """Serialize the args_schema by returning its class name.

        This function serializes the arguments schema by returning the class name of the schema.

        Args:
            value (type[BaseModel]): The argument schema class.

        Returns:
            str: The class name of the argument schema.

        """
        return value.__name__


class PortiaRemoteTool(Tool, Generic[SERIALIZABLE_TYPE_VAR]):
    """Tool that passes run execution to Portia Cloud."""

    client: httpx.Client

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def parse_response(self, ctx: ToolRunContext, response: dict[str, Any]) -> Output:
        """Parse a JSON response into domain models or errors.

        This method handles the response from the Portia Cloud API, converting it into domain
        specific models. It also handles errors, including `ToolSoftError` and `ToolHardError`,
        as well as clarifications of different types.

        Args:
            ctx (ToolRunContext): Context of the environment
            response (dict[str, Any]): The JSON response returned by the Portia Cloud API.

        Returns:
            Output: The parsed output wrapped in an `Output` object.

        Raises:
            ToolSoftError: If a soft error is encountered in the response.
            ToolHardError: If a hard error is encountered in the response.

        """
        output = LocalOutput.model_validate(response["output"])
        output_value = output.get_value()

        # Handle Tool Errors
        if isinstance(output_value, str):
            if "ToolSoftError" in output_value:
                raise ToolSoftError(output_value)
            if "ToolHardError" in output_value:
                raise ToolHardError(output_value)
        # Handle Clarifications
        if isinstance(output_value, list) and output_value and "category" in output_value[0]:
            clarification = output_value[0]
            match clarification["category"]:
                case ClarificationCategory.ACTION:
                    return LocalOutput(
                        value=ActionClarification(
                            plan_run_id=ctx.plan_run_id,
                            id=ClarificationUUID.from_string(clarification["id"]),
                            action_url=HttpUrl(clarification["action_url"]),
                            user_guidance=clarification["user_guidance"],
                        ),
                    )
                case ClarificationCategory.INPUT:
                    return LocalOutput(
                        value=InputClarification(
                            plan_run_id=ctx.plan_run_id,
                            id=ClarificationUUID.from_string(clarification["id"]),
                            argument_name=clarification["argument_name"],
                            user_guidance=clarification["user_guidance"],
                        ),
                    )
                case ClarificationCategory.MULTIPLE_CHOICE:
                    return LocalOutput(
                        value=MultipleChoiceClarification(
                            plan_run_id=ctx.plan_run_id,
                            id=ClarificationUUID.from_string(clarification["id"]),
                            argument_name=clarification["argument_name"],
                            user_guidance=clarification["user_guidance"],
                            options=clarification["options"],
                        ),
                    )
                case ClarificationCategory.VALUE_CONFIRMATION:
                    return LocalOutput(
                        value=ValueConfirmationClarification(
                            plan_run_id=ctx.plan_run_id,
                            id=ClarificationUUID.from_string(clarification["id"]),
                            argument_name=clarification["argument_name"],
                            user_guidance=clarification["user_guidance"],
                        ),
                    )
        return output

    def ready(self, ctx: ToolRunContext) -> bool:
        """Check if the remote tool is ready by calling the /ready endpoint.

        Args:
            ctx (ToolRunContext): Context of the environment

        Returns:
            bool: Whether the tool is ready to run

        """
        try:
            # Send to Cloud
            response = self.client.post(
                url=f"/api/v0/tools/{self.id}/ready/",
                content=json.dumps(
                    {
                        "execution_context": {
                            "end_user_id": ctx.execution_context.end_user_id or "",
                            "plan_run_id": str(ctx.plan_run_id),
                            "additional_data": ctx.execution_context.additional_data or {},
                        },
                    },
                ),
            )
            response.raise_for_status()
        except Exception as e:  # noqa: BLE001
            logger().error(f"Unhandled error from Portia Cloud: {e}")
            return False
        else:
            response_json = response.json()
            return "success" in response_json

    def run(
        self,
        ctx: ToolRunContext,
        *args: Any,
        **kwargs: Any,
    ) -> SERIALIZABLE_TYPE_VAR | None | Clarification:
        """Invoke the run endpoint and handle the response.

        This method sends the execution request to the Portia Cloud API, passing the arguments and
        execution context. It then processes the response by calling `parse_response`. Errors
        during the request or parsing are raised as `ToolHardError`.

        Args:
            ctx (ToolRunContext): The context of the execution, including end user ID, run ID
            and additional data.
            *args (Any): The positional arguments for the tool.
            **kwargs (Any): The keyword arguments for the tool.

        Returns:
            SERIALIZABLE_TYPE_VAR | None | Clarification: The result of the run execution, which
            could either be a serialized value, None, or a `Clarification` object.

        Raises:
            ToolHardError: If the request fails or there is an error parsing the response.

        """
        try:
            # Send to Cloud
            response = self.client.post(
                url=f"/api/v0/tools/{self.id}/run/",
                content=json.dumps(
                    {
                        "arguments": combine_args_kwargs(*args, **kwargs),
                        "execution_context": {
                            "end_user_id": ctx.execution_context.end_user_id or "",
                            "plan_run_id": str(ctx.plan_run_id),
                            "additional_data": ctx.execution_context.additional_data or {},
                        },
                    },
                ),
            )
            response.raise_for_status()
        except httpx.HTTPStatusError as e:
            logger().error(f"Error from Portia Cloud: {e.response.content}")
            raise ToolHardError(str(e.response.json())) from e
        except Exception as e:
            logger().error(f"Unhandled error from Portia Cloud: {e}")
            raise ToolHardError(e) from e
        else:
            try:
                output = self.parse_response(ctx, response.json())
            except (ValidationError, KeyError) as e:
                logger().error(f"Error parsing response from Portia Cloud: {e}")
                raise ToolHardError(e) from e
            else:
                return output.get_value()


class PortiaMcpTool(Tool[str]):
    """A Portia Tool wrapper for an MCP server-based tool."""

    mcp_client_config: McpClientConfig

    def run(self, _: ToolRunContext, **kwargs: Any) -> str:
        """Invoke the tool by dispatching to the MCP server.

        Args:
            _: The tool run context
            **kwargs: The arguments to pass to the MCP tool invocation

        Returns:
            str: The result of the tool call

        """
        logger().debug(f"Calling tool {self.name} with arguments {kwargs}")
        return asyncio.run(self.call_remote_mcp_tool(self.name, kwargs))

    async def call_remote_mcp_tool(self, name: str, arguments: dict | None = None) -> str:
        """Call a tool using the MCP session."""
        async with get_mcp_session(self.mcp_client_config) as session:
            tool_result = await session.call_tool(name, arguments)
            if tool_result.isError:
                raise ToolHardError(
                    f"MCP tool {name} returned an error: {tool_result.model_dump_json()}",
                )
            return tool_result.model_dump_json()

```

## File: portia/cli.py

```python
"""CLI Implementation.

Usage:

portia-cli run "add 4 + 8" - run a query
portia-cli plan "add 4 + 8" - plan a query
portia-cli list-tools
"""

from __future__ import annotations

import builtins
import importlib.metadata
import json
import sys
from enum import Enum
from functools import wraps
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable

import click
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from pydantic_core import PydanticUndefined
from typing_extensions import get_origin

from portia.clarification_handler import ClarificationHandler
from portia.config import Config
from portia.errors import InvalidConfigError
from portia.execution_context import execution_context
from portia.logger import logger
from portia.portia import ExecutionHooks, Portia
from portia.tool_registry import DefaultToolRegistry

if TYPE_CHECKING:
    from pydantic.fields import FieldInfo

    from portia.clarification import (
        ActionClarification,
        Clarification,
        CustomClarification,
        InputClarification,
        MultipleChoiceClarification,
        ValueConfirmationClarification,
    )

DEFAULT_FILE_PATH = ".portia"
PORTIA_API_KEY = "portia_api_key"


class EnvLocation(Enum):
    """The location of the environment variables."""

    ENV_FILE = "ENV_FILE"
    ENV_VARS = "ENV_VARS"


class CLIConfig(BaseModel):
    """Config for the CLI."""

    env_location: EnvLocation = Field(
        default=EnvLocation.ENV_VARS,
        description="The location of the environment variables.",
    )

    config_file: str = Field(
        default=f"{DEFAULT_FILE_PATH}/config.json",
        description="The location of the JSON config file for the CLI to use.",
    )

    end_user_id: str = Field(
        default="",
        description="The end user id to use in the execution context.",
    )

    confirm: bool = Field(
        default=True,
        description="Whether to confirm plans before running them.",
    )

    output_path: str = Field(
        default=DEFAULT_FILE_PATH,
        description="Where to output to",
    )

    tool_id: str | None = Field(
        default=None,
        description="The tool ID to use. If not provided, all tools will be used.",
    )


def generate_cli_option_from_pydantic_field(  # noqa: C901, PLR0912
    f: Callable[..., Any],
    field: str,
    info: FieldInfo,
) -> Callable[..., Any]:
    """Generate a click option from a pydantic field."""
    option_name = field.replace("_", "-")

    # Don't support passing API Keys as options as it leaks them to history/logs etc
    if option_name.endswith("api-key"):
        return f

    field_type = click.STRING
    field_default = info.default
    callback = None
    if info.default_factory:
        field_default = info.default_factory()  # type: ignore  # noqa: PGH003

    match info.annotation:
        case builtins.int:
            field_type = click.INT
        case builtins.float:
            field_type = click.FLOAT
        case builtins.bool:
            field_type = click.BOOL
        case builtins.str:
            field_type = click.STRING
        case builtins.list:
            field_type = click.Tuple([str])
        case _ if get_origin(info.annotation) is dict:

            def dict_callback(_1: click.Context, _2: click.Parameter, value: str) -> dict:
                if value is None:
                    return field_default
                try:
                    return json.loads(value)
                except json.JSONDecodeError as e:
                    raise click.BadParameter(f"Invalid JSON: {value}") from e

            callback = dict_callback
        case _:
            if isinstance(info.annotation, type) and issubclass(info.annotation, Enum):
                field_type = click.Choice(
                    [e.name for e in info.annotation],
                    case_sensitive=False,
                )
                if info.default and info.default is not PydanticUndefined:
                    field_default = info.default.name
                elif info.default_factory:
                    field_default = info.default_factory().name  # type: ignore[reportCallIssue]
                else:
                    field_default = None

    field_help = info.description or f"Set the value for {option_name}"

    return click.option(
        f"--{option_name}",
        type=field_type,
        default=field_default,
        help=field_help,
        callback=callback,
    )(f)


def common_options(f: Callable[..., Any]) -> Callable[..., Any]:
    """Define common options for CLI commands."""
    for field, info in Config.model_fields.items():
        generate_cli_option_from_pydantic_field(f, field, info)
    for field, info in CLIConfig.model_fields.items():
        generate_cli_option_from_pydantic_field(f, field, info)

    @wraps(f)
    def wrapper(*args: Any, **kwargs: Any) -> Any:  # noqa: ANN401
        return f(*args, **kwargs)

    return wrapper


class CLIExecutionHooks(ExecutionHooks):
    """Execution hooks for the CLI."""

    def __init__(self) -> None:
        """Set up execution hooks for the CLI."""
        super().__init__(clarification_handler=CLIClarificationHandler())


class CLIClarificationHandler(ClarificationHandler):
    """Handles clarifications by obtaining user input from the CLI."""

    def handle_action_clarification(
        self,
        clarification: ActionClarification,
        on_resolution: Callable[[Clarification, object], None],  # noqa: ARG002
        on_error: Callable[[Clarification, object], None],  # noqa: ARG002
    ) -> None:
        """Handle an action clarification.

        Does this by showing the user the URL on the CLI and instructing them to click on
        it to proceed.
        """
        logger().info(
            click.style(
                f"{clarification.user_guidance} -- Please click on the link below to proceed."
                f"{clarification.action_url}",
                fg=87,
            ),
        )

    def handle_input_clarification(
        self,
        clarification: InputClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],  # noqa: ARG002
    ) -> None:
        """Handle a user input clarifications by asking the user for input from the CLI."""
        user_input = click.prompt(
            click.style(clarification.user_guidance + "\nPlease enter a value", fg=87),
        )
        return on_resolution(clarification, user_input)

    def handle_multiple_choice_clarification(
        self,
        clarification: MultipleChoiceClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],  # noqa: ARG002
    ) -> None:
        """Handle a multi-choice clarification by asking the user for input from the CLI."""
        choices = click.Choice(clarification.options)
        user_input = click.prompt(
            click.style(clarification.user_guidance + "\nPlease choose a value:\n", fg=87),
            type=choices,
        )
        return on_resolution(clarification, user_input)

    def handle_value_confirmation_clarification(
        self,
        clarification: ValueConfirmationClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a value confirmation clarification by asking the user to confirm from the CLI."""
        if click.confirm(text=click.style(clarification.user_guidance, fg=87), default=False):
            on_resolution(clarification, True)  # noqa: FBT003
        else:
            on_error(clarification, "Clarification was rejected by the user")

    def handle_custom_clarification(
        self,
        clarification: CustomClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],  # noqa: ARG002
    ) -> None:
        """Handle a custom clarification."""
        click.echo(click.style(clarification.user_guidance, fg=87))
        click.echo(click.style(f"Additional data: {json.dumps(clarification.data)}", fg=87))
        user_input = click.prompt(click.style("\nPlease enter a value:\n", fg=87))
        return on_resolution(clarification, user_input)


@click.group(context_settings={"max_content_width": 240})
def cli() -> None:
    """Portia CLI."""


@click.command()
def version() -> None:
    """Print the CLI tool version."""
    click.echo(importlib.metadata.version("portia-sdk-python"))


@click.command()
@common_options
@click.argument("query")
def run(
    query: str,
    **kwargs,  # noqa: ANN003
) -> None:
    """Run a query."""
    cli_config, config = _get_config(**kwargs)

    # Add the tool registry
    registry = DefaultToolRegistry(config)

    # Run the query
    portia = Portia(
        config=config,
        tools=(
            registry.match_tools(tool_ids=[cli_config.tool_id]) if cli_config.tool_id else registry
        ),
        execution_hooks=CLIExecutionHooks(),
    )

    with execution_context(end_user_id=cli_config.end_user_id):
        plan = portia.plan(query)

        if cli_config.confirm:
            click.echo(plan.pretty_print())
            if not click.confirm("Do you want to execute the plan?"):
                return

        plan_run = portia.run_plan(plan)
        click.echo(plan_run.model_dump_json(indent=4))


@click.command()
@common_options
@click.argument("query")
def plan(
    query: str,
    **kwargs,  # noqa: ANN003
) -> None:
    """Plan a query."""
    cli_config, config = _get_config(**kwargs)
    portia = Portia(config=config)

    with execution_context(end_user_id=cli_config.end_user_id):
        output = portia.plan(query)

    click.echo(output.model_dump_json(indent=4))


@click.command()
@common_options
def list_tools(
    **kwargs,  # noqa: ANN003
) -> None:
    """List tools."""
    cli_config, config = _get_config(**kwargs)

    for tool in DefaultToolRegistry(config).get_tools():
        click.echo(tool.model_dump_json(indent=4))


@click.command()
@common_options
def config_write(
    **kwargs,  # noqa: ANN003
) -> None:
    """Write config file to disk."""
    cli_config, config = _get_config(**kwargs)

    output_path = Path(cli_config.output_path, "config.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    file_contents = config.model_dump_json(indent=4)

    with output_path.open("w") as f:
        f.write(file_contents)


def _get_config(
    **kwargs,  # noqa: ANN003
) -> tuple[CLIConfig, Config]:
    """Init config."""
    cli_config = CLIConfig(**kwargs)
    if cli_config.env_location == EnvLocation.ENV_FILE:
        load_dotenv(override=True)
    try:
        config = Config.from_default(**kwargs)
    except InvalidConfigError as e:
        logger().error(e.message)
        sys.exit(1)

    return (cli_config, config)


cli.add_command(version)
cli.add_command(run)
cli.add_command(plan)
cli.add_command(list_tools)
cli.add_command(config_write)

if __name__ == "__main__":
    cli(obj={})

```

## File: portia/execution_context.py

```python
"""Provides execution context to the planning and execution agents.

This module defines the `ExecutionContext` class and utilities for managing execution
contexts for planning and execution agents. It provides a way to pass runtime-specific information
for each run execution, ensuring flexibility and context isolation, especially in
multi-threaded or asynchronous applications.

Key Features:
- The `ExecutionContext` class encapsulates information such as user identification
  and additional data for planning and execution agents.
- The `execution_context` context manager allows for context isolation, ensuring
  that each task or thread has its own independent execution context.
- The `get_execution_context` function allows retrieval of the current execution context.

"""

from __future__ import annotations

from contextlib import contextmanager
from contextvars import ContextVar
from typing import TYPE_CHECKING

from pydantic import BaseModel, ConfigDict, Field

if TYPE_CHECKING:
    from collections.abc import Generator

# Define a ContextVar for execution context
_execution_context: ContextVar[ExecutionContext | None] = ContextVar(
    "_execution_context",
    default=None,
)


class ExecutionContext(BaseModel):
    """Execution context provides runtime information to the portia client and planning and execution agents.

    Unlike configuration settings, it is designed to be used on a per-request basis,
    allowing customization at runtime. For example, this can pass end-user-specific
    information to planning and execution agents for dynamic adjustments.

    Attributes:
        end_user_id (Optional[str]): The identifier of the user for whom the run is running.
            Used for authentication and debugging purposes.
        additional_data (dict[str, str]): Arbitrary additional data useful for debugging.

    """  # noqa: E501

    model_config = ConfigDict(extra="ignore")

    end_user_id: str | None = None

    additional_data: dict[str, str] = Field(default={})


def empty_context() -> ExecutionContext:
    """Return an empty execution context.

    Returns:
        ExecutionContext: A default `ExecutionContext` instance with no specific data set.

    """
    return ExecutionContext(
        end_user_id=None,
        additional_data={},
    )


@contextmanager
def execution_context(
    context: ExecutionContext | None = None,
    end_user_id: str | None = None,
    additional_data: dict[str, str] | None = None,
) -> Generator[None, None, None]:
    """Set the execution context for the duration of the PlanRun.

    This context manager ensures context isolation by using `contextvars.ContextVar`,
    meaning that the execution context set within this block will only affect
    the current task or thread. This is particularly useful in both multi-threaded
    and asynchronous applications, such as web servers or task queues, where multiple
    tasks or threads may need independent contexts simultaneously.

    Args:
        context (Optional[ExecutionContext]): The execution context to set for the current task.
            If not provided, a new `ExecutionContext` is created using the provided parameters.
        end_user_id (Optional[str]): An identifier for the end user, used to customize
            the execution for specific users. Defaults to `None`.
        additional_data (Optional[Dict[str, str]]): Arbitrary additional data to associate
            with the context. Defaults to an empty dictionary.

    Yields:
        None: The block of code within the context manager executes with the specified context.

    Context Isolation:
        - The `_execution_context` object is a `ContextVar`, ensuring that the `ExecutionContext`
          set in one task or thread does not affect others.
        - When the context manager exits, the context for the current task is cleaned up
          to avoid memory leaks or unintended persistence of data.

    Example:
    ```python
        with execution_context(end_user_id="user123", additional_data={"key": "value"}):
            # Code here runs with the specified execution context
        # Outside the block, the execution context is cleared for the current task.
    ```

    """
    if context is None:
        context = ExecutionContext(
            end_user_id=end_user_id,
            additional_data=additional_data or {},
        )
    token = _execution_context.set(context)
    try:
        yield
    finally:
        _execution_context.reset(token)


def get_execution_context() -> ExecutionContext:
    """Retrieve the current execution context.

    This function retrieves the `ExecutionContext` that is currently set. If no context
    is set, an empty `ExecutionContext` is returned.

    Returns:
        ExecutionContext: The current execution context, or an empty context if none is set.

    """
    return _execution_context.get() or empty_context()


def is_execution_context_set() -> bool:
    """Check whether an execution context is currently set.

    Returns:
        bool: `True` if an execution context is set, otherwise `False`.

    """
    return _execution_context.get() is not None

```

## File: portia/llm_wrapper.py

```python
"""Wrapper around different LLM providers, standardizing their usage.

WARNING: This module is deprecated. It will be removed in a future version.

This module provides an abstraction layer around various large language model (LLM) providers,
allowing them to be treated uniformly in the application. It defines an `LLMWrapper` that wraps
a `Model` instance and provides methods to convert the provider's model to LangChain-compatible
models and to generate responses using the instructor tool.

Classes in this file include:

- `LLMWrapper`: A concrete implementation that supports different LLM providers and provides
functionality for converting to LangChain models and generating responses using instructor.

"""

from __future__ import annotations

import logging
import warnings
from typing import TYPE_CHECKING, TypeVar

from pydantic import BaseModel, SecretStr

from portia.common import validate_extras_dependencies
from portia.config import LLMModel, LLMProvider
from portia.model import (
    AnthropicGenerativeModel,
    AzureOpenAIGenerativeModel,
    GenerativeModel,
    LangChainGenerativeModel,
    Message,
    OpenAIGenerativeModel,
)

if TYPE_CHECKING:
    from langchain_core.language_models.chat_models import BaseChatModel

    from portia.config import Config

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseModel)


class LLMWrapper:
    """LLMWrapper class for different LLMs.

    **DEPRECATED**: This class is deprecated. It will be removed in an upcoming version.

    This class provides functionality for working with various LLM providers, such as OpenAI,
    Anthropic, and MistralAI. It includes methods to convert the LLM provider's model to a
    LangChain-compatible model and to generate responses using the instructor tool.

    Attributes:
        model_name (LLMModel): The name of the model to use.
        api_key (SecretStr): The API key for the LLM provider.
        model_seed (int): The seed for the model's random generation.
        api_endpoint (str | None): The API endpoint for the LLM provider (Optional, many API's don't
                                   require it).

    Methods:
        to_langchain: Converts the LLM provider's model to a LangChain-compatible model.
        to_instructor: Generates a response using instructor for the selected LLM provider.

    """

    def __init__(
        self,
        model_name: LLMModel | None = None,
        api_key: SecretStr | None = None,
        model_seed: int = 343,
        api_endpoint: str | None = None,
        model: GenerativeModel | None = None,
    ) -> None:
        """Initialize the wrapper.

        Args:
            model_name (LLMModel): The name of the model to use.
            api_key (SecretStr): The API key for the LLM provider.
            model_seed (int): The seed for the model's random generation.
            api_endpoint (str | None): The API endpoint for the LLM provider
                                       (Optional, many API's don't require it).
            model (GenerativeModel | None): The language model to use.

        """
        warnings.warn(
            "LLMWrapper is deprecated. It will be removed in an upcoming version.",
            DeprecationWarning,
            stacklevel=2,
        )
        if model is None:
            if model_name is None or api_key is None:
                raise ValueError("model_name and api_key must be provided if model is not provided")
            model = self._construct_model(model_name, api_key, model_seed, api_endpoint)
        self.model = model

    @staticmethod
    def _construct_model(
        llm_model: LLMModel,
        api_key: SecretStr,
        model_seed: int,
        api_endpoint: str | None,
    ) -> GenerativeModel:
        """Construct a model from a LLMModel."""
        match llm_model.provider():
            case LLMProvider.OPENAI:
                return OpenAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=api_key,
                    seed=model_seed,
                )
            case LLMProvider.ANTHROPIC:
                return AnthropicGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=api_key,
                )
            case LLMProvider.MISTRALAI:
                validate_extras_dependencies("mistral")
                from portia.model import MistralAIGenerativeModel

                return MistralAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=api_key,
                )
            case LLMProvider.GOOGLE_GENERATIVE_AI:
                validate_extras_dependencies("google")
                from portia.model import GoogleGenAiGenerativeModel

                return GoogleGenAiGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=api_key,
                )
            case LLMProvider.AZURE_OPENAI:
                return AzureOpenAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=api_key,
                    azure_endpoint=api_endpoint if api_endpoint else "",
                    seed=model_seed,
                )

    @classmethod
    def for_usage(cls, usage: str, config: Config) -> LLMWrapper:
        """Create an LLMWrapper from a LLMModel."""
        model = config.resolve_model(usage)
        return cls(model=model)

    def to_langchain(self) -> BaseChatModel:
        """Return a LangChain chat model based on the LLM provider.

        Converts the LLM provider's model to a LangChain-compatible model for interaction
        within the LangChain framework.

        Returns:
            BaseChatModel: A LangChain-compatible model.

        """
        if isinstance(self.model, LangChainGenerativeModel):
            return self.model.to_langchain()
        raise ValueError(
            f"LangChain is not supported for this model type {self.model.__class__.__name__}",
        )

    def to_instructor(
        self,
        response_model: type[T],
        messages: list[Message],
    ) -> T:
        """Use instructor to generate an object of the specified response model type.

        Args:
            response_model (type[T]): The Pydantic model to deserialize the response into.
            messages (list[Message]): The messages to send to the LLM.

        Returns:
            T: The deserialized response from the LLM provider.

        """
        return self.model.get_structured_response(messages, response_model)

```

## File: portia/cloud.py

```python
"""Core client for interacting with portia cloud."""

import httpx

from portia.config import Config


class PortiaCloudClient:
    """Base HTTP client for interacting with portia cloud."""

    _client = None

    @classmethod
    def get_client(cls, config: Config) -> httpx.Client:
        """Return the client using a singleton pattern to help manage limits across the SDK."""
        if cls._client is None:
            cls._client = cls.new_client(config, allow_unauthenticated=False)
        return cls._client

    @classmethod
    def new_client(
        cls,
        config: Config,
        *,
        allow_unauthenticated: bool = False,
        json_headers: bool = True,
    ) -> httpx.Client:
        """Create a new httpx client.

        Args:
            config (Config): The Portia Configuration instance, containing the API key and endpoint.
            allow_unauthenticated (bool): Whether to allow creation of an unauthenticated client.
            json_headers (bool): Whether to add json headers to the request.

        """
        headers = {}
        if json_headers:
            headers = {
                "Content-Type": "application/json",
            }
        if config.portia_api_key or allow_unauthenticated is False:
            api_key = config.must_get_api_key("portia_api_key").get_secret_value()
            headers["Authorization"] = f"Api-Key {api_key}"
        return httpx.Client(
            base_url=config.must_get("portia_api_endpoint", str),
            headers=headers,
            timeout=httpx.Timeout(60),
            limits=httpx.Limits(max_connections=10),
        )

```

## File: portia/__init__.py

```python
"""portia defines the base abstractions for building Agentic workflows."""

from __future__ import annotations

# Clarification related classes
from portia.clarification import (
    ActionClarification,
    Clarification,
    ClarificationCategory,
    ClarificationListType,
    ClarificationType,
    CustomClarification,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)
from portia.clarification_handler import ClarificationHandler
from portia.config import (
    SUPPORTED_ANTHROPIC_MODELS,
    SUPPORTED_MISTRALAI_MODELS,
    SUPPORTED_OPENAI_MODELS,
    Config,
    ExecutionAgentType,
    LLMModel,
    LLMProvider,
    LogLevel,
    PlanningAgentType,
    StorageClass,
    default_config,
)

# Error classes
from portia.errors import (
    ConfigNotFoundError,
    DuplicateToolError,
    InvalidAgentError,
    InvalidAgentOutputError,
    InvalidConfigError,
    InvalidPlanRunStateError,
    InvalidToolDescriptionError,
    PlanError,
    PlanNotFoundError,
    PlanRunNotFoundError,
    PortiaBaseError,
    StorageError,
    ToolFailedError,
    ToolHardError,
    ToolNotFoundError,
    ToolRetryError,
)
from portia.execution_agents.output import Output

# Execution context
from portia.execution_context import (
    ExecutionContext,
    execution_context,
)

# Logging
from portia.logger import logger

# MCP related classes
from portia.mcp_session import SseMcpClientConfig, StdioMcpClientConfig

# Open source tools
from portia.open_source_tools.llm_tool import LLMTool
from portia.open_source_tools.local_file_reader_tool import FileReaderTool
from portia.open_source_tools.local_file_writer_tool import FileWriterTool
from portia.open_source_tools.registry import (
    example_tool_registry,
    open_source_tool_registry,
)
from portia.open_source_tools.search_tool import SearchTool
from portia.open_source_tools.weather import WeatherTool

# Plan and execution related classes
from portia.plan import Plan, PlanContext, Step
from portia.plan_run import PlanRun, PlanRunState

# Core classes
from portia.portia import ExecutionHooks, Portia

# Tool related classes
from portia.tool import Tool, ToolRunContext
from portia.tool_registry import (
    DefaultToolRegistry,
    InMemoryToolRegistry,
    McpToolRegistry,
    PortiaToolRegistry,
    ToolRegistry,
)

# Define explicitly what should be available when using "from portia import *"
__all__ = [
    "SUPPORTED_ANTHROPIC_MODELS",
    "SUPPORTED_MISTRALAI_MODELS",
    "SUPPORTED_OPENAI_MODELS",
    "ActionClarification",
    "Clarification",
    "ClarificationCategory",
    "ClarificationHandler",
    "ClarificationListType",
    "ClarificationType",
    "Config",
    "ConfigNotFoundError",
    "CustomClarification",
    "DefaultToolRegistry",
    "DuplicateToolError",
    "ExecutionAgentType",
    "ExecutionContext",
    "ExecutionHooks",
    "FileReaderTool",
    "FileWriterTool",
    "InMemoryToolRegistry",
    "InputClarification",
    "InvalidAgentError",
    "InvalidAgentOutputError",
    "InvalidConfigError",
    "InvalidPlanRunStateError",
    "InvalidToolDescriptionError",
    "LLMModel",
    "LLMProvider",
    "LLMTool",
    "LogLevel",
    "McpToolRegistry",
    "MultipleChoiceClarification",
    "Output",
    "Plan",
    "PlanContext",
    "PlanError",
    "PlanNotFoundError",
    "PlanRun",
    "PlanRunNotFoundError",
    "PlanRunState",
    "PlanningAgentType",
    "Portia",
    "PortiaBaseError",
    "PortiaToolRegistry",
    "SearchTool",
    "SseMcpClientConfig",
    "StdioMcpClientConfig",
    "Step",
    "StorageClass",
    "StorageError",
    "Tool",
    "ToolFailedError",
    "ToolHardError",
    "ToolNotFoundError",
    "ToolRegistry",
    "ToolRetryError",
    "ToolRunContext",
    "ValueConfirmationClarification",
    "WeatherTool",
    "default_config",
    "example_tool_registry",
    "execution_context",
    "logger",
    "open_source_tool_registry",
]

```

## File: portia/common.py

```python
"""Types and utilities useful across the package.

This module defines various types, utilities, and base classes used throughout the package.
It includes a custom Enum class, helper functions, and base models with special configurations for
use in the Portia framework.
"""

from __future__ import annotations

import importlib.util
from enum import Enum
from typing import Any, TypeVar

Serializable = Any
SERIALIZABLE_TYPE_VAR = TypeVar("SERIALIZABLE_TYPE_VAR", bound=Serializable)


class PortiaEnum(str, Enum):
    """Base enum class for Portia enums.

    This class provides common functionality for Portia enums, including the ability to retrieve all
    choices as (name, value) pairs through the `enumerate` method.
    """

    @classmethod
    def enumerate(cls) -> tuple[tuple[str, str], ...]:
        """Return a tuple of all choices as (name, value) pairs.

        This method iterates through all enum members and returns their name and value in a tuple
        format.

        Returns:
            tuple: A tuple containing pairs of enum member names and values.

        """
        return tuple((x.name, x.value) for x in cls)


def combine_args_kwargs(*args: Any, **kwargs: Any) -> Any:  # noqa: ANN401
    """Combine Args + Kwargs into a single dictionary.

    This function takes arbitrary positional and keyword arguments and combines them into a single
    dictionary. Positional arguments are indexed as string keys (e.g., "0", "1", ...) while keyword
    arguments retain their names.

    Args:
        *args: Positional arguments to be included in the dictionary.
        **kwargs: Keyword arguments to be included in the dictionary.

    Returns:
        dict: A dictionary combining both positional and keyword arguments.

    """
    args_dict = {f"{i}": arg for i, arg in enumerate(args)}
    return {**args_dict, **kwargs}


EXTRAS_GROUPS_DEPENDENCIES = {
    "mistral": ["mistralai", "langchain_mistralai"],
    "google": ["google.generativeai", "langchain_google_genai"],
}


def validate_extras_dependencies(extra_group: str, *, raise_error: bool = True) -> bool:
    """Validate that the dependencies for an extras group are installed.

    Returns True if all dependencies are installed, False otherwise.

    Args:
        extra_group (str): The extras group to validate, e.g. "mistral" or "google".
        raise_error (bool): Whether to raise an ImportError if the dependencies are not installed.

    Returns:
        bool: True if all dependencies are installed, False otherwise.

    """

    def are_packages_installed(packages: list[str]) -> bool:
        """Check if a list of packages are installed."""
        try:
            return all(importlib.util.find_spec(package) is not None for package in packages)
        except ImportError:
            return False

    if not are_packages_installed(EXTRAS_GROUPS_DEPENDENCIES[extra_group]):
        if raise_error:
            raise ImportError(
                f"Please install portia-sdk-python[{extra_group}] to use this functionality.",
            )
        return False
    return True

```

## File: portia/storage.py

```python
"""Storage classes for managing the saving and retrieval of plans, runs, and tool calls.

This module defines a set of storage classes that provide different backends for saving, retrieving,
and managing plans, runs, and tool calls. These storage classes include both in-memory and
file-based storage, as well as integration with the Portia Cloud API. Each class is responsible
for handling interactions with its respective storage medium, including validating responses
and raising appropriate exceptions when necessary.

Classes:
    - Storage (Base Class): A base class that defines common interfaces for all storage types,
    ensuring consistent methods for saving and retrieving plans, runs, and tool calls.
    - InMemoryStorage: An in-memory implementation of the `Storage` class for storing plans,
    runs, and tool calls in a temporary, volatile storage medium.
    - FileStorage: A file-based implementation of the `Storage` class for storing plans, runs,
      and tool calls as local files in the filesystem.
    - PortiaCloudStorage: A cloud-based implementation of the `Storage` class that interacts with
    the Portia Cloud API to save and retrieve plans, runs, and tool call records.

Each storage class handles the following tasks:
    - Sending and receiving data to its respective storage medium - memory, file system, or API.
    - Validating responses from storage and raising errors when necessary.
    - Handling exceptions and re-raising them as custom `StorageError` exceptions to provide
    more informative error handling.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from collections import defaultdict
from io import BytesIO
from pathlib import Path
from typing import TYPE_CHECKING, Protocol, TypeVar
from urllib.parse import urlencode

import httpx
from pydantic import BaseModel, ValidationError

from portia.cloud import PortiaCloudClient
from portia.errors import PlanNotFoundError, PlanRunNotFoundError, StorageError
from portia.execution_agents.output import (
    AgentMemoryOutput,
    LocalOutput,
    Output,
)
from portia.execution_context import ExecutionContext
from portia.logger import logger
from portia.plan import Plan, PlanContext, PlanUUID, Step
from portia.plan_run import (
    PlanRun,
    PlanRunOutputs,
    PlanRunState,
    PlanRunUUID,
)
from portia.prefixed_uuid import PLAN_RUN_UUID_PREFIX
from portia.tool_call import ToolCallRecord, ToolCallStatus

if TYPE_CHECKING:
    from portia.config import Config

T = TypeVar("T", bound=BaseModel)

MAX_OUTPUT_LOG_LENGTH = 1000


class PlanStorage(ABC):
    """Abstract base class for storing and retrieving plans.

    Subclasses must implement the methods to save and retrieve plans.

    Methods:
        save_plan(self, plan: Plan) -> None:
            Save a plan.
        get_plan(self, plan_id: PlanUUID) -> Plan:
            Get a plan by ID.

    """

    @abstractmethod
    def save_plan(self, plan: Plan) -> None:
        """Save a plan.

        Args:
            plan (Plan): The Plan object to save.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("save_plan is not implemented")

    @abstractmethod
    def get_plan(self, plan_id: PlanUUID) -> Plan:
        """Retrieve a plan by its ID.

        Args:
            plan_id (PlanUUID): The UUID of the plan to retrieve.

        Returns:
            Plan: The Plan object associated with the provided plan_id.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("get_plan is not implemented")


class PlanRunListResponse(BaseModel):
    """Response for the get_plan_runs operation. Can support pagination."""

    results: list[PlanRun]
    count: int
    total_pages: int
    current_page: int


class RunStorage(ABC):
    """Abstract base class for storing and retrieving runs.

    Subclasses must implement the methods to save and retrieve PlanRuns.

    Methods:
        save_plan_run(self, run: Run) -> None:
            Save a PlanRun.
        get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
            Get PlanRun by ID.
        get_plan_runs(self, run_state: RunState | None = None, page=int | None = None)
            -> PlanRunListResponse:
            Return runs that match the given run_state

    """

    @abstractmethod
    def save_plan_run(self, plan_run: PlanRun) -> None:
        """Save a PlanRun.

        Args:
            plan_run (PlanRun): The Run object to save.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("save_run is not implemented")

    @abstractmethod
    def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
        """Retrieve PlanRun by its ID.

        Args:
            plan_run_id (RunUUID): The UUID of the run to retrieve.

        Returns:
            Run: The Run object associated with the provided plan_run_id.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("get_run is not implemented")

    @abstractmethod
    def get_plan_runs(
        self,
        run_state: PlanRunState | None = None,
        page: int | None = None,
    ) -> PlanRunListResponse:
        """List runs by their state.

        Args:
            run_state (RunState | None): Optionally filter runs by their state.
            page (int | None): Optional pagination data

        Returns:
            list[Run]: A list of Run objects that match the given state.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("get_plan_runs is not implemented")


class AdditionalStorage(ABC):
    """Abstract base class for additional storage.

    Subclasses must implement the methods.

    Methods:
        save_tool_call(self, tool_call: ToolCallRecord) -> None:
            Save a tool_call.

    """

    @abstractmethod
    def save_tool_call(self, tool_call: ToolCallRecord) -> None:
        """Save a ToolCall.

        Args:
            tool_call (ToolCallRecord): The ToolCallRecord object to save.

        Raises:
            NotImplementedError: If the method is not implemented.

        """
        raise NotImplementedError("save_tool_call is not implemented")


class LogAdditionalStorage(AdditionalStorage):
    """AdditionalStorage that logs calls rather than persisting them.

    Useful for storages that don't care about tool_calls etc.
    """

    def save_tool_call(self, tool_call: ToolCallRecord) -> None:
        """Log the tool call.

        Args:
            tool_call (ToolCallRecord): The ToolCallRecord object to log.

        """
        logger().debug(
            f"Tool {tool_call.tool_name!s} executed in {tool_call.latency_seconds:.2f} seconds",
        )
        # Limit log to just first 1000 characters
        output = tool_call.output
        if len(str(tool_call.output)) > MAX_OUTPUT_LOG_LENGTH:
            output = (
                str(tool_call.output)[:MAX_OUTPUT_LOG_LENGTH]
                + "...[truncated - only first 1000 characters shown]"
            )
        match tool_call.status:
            case ToolCallStatus.SUCCESS:
                logger().debug(
                    f"Tool call {tool_call.tool_name!s} completed",
                    output=output,
                )
            case ToolCallStatus.FAILED:
                logger().error("Tool returned error", output=output)
            case ToolCallStatus.NEED_CLARIFICATION:
                logger().debug("Tool returned clarifications", output=output)


class Storage(PlanStorage, RunStorage, LogAdditionalStorage):
    """Combined base class for Plan Run + Additional storages."""


class AgentMemory(Protocol):
    """Abstract base class for storing items in agent memory."""

    @abstractmethod
    def save_plan_run_output(
        self,
        output_name: str,
        output: Output,
        plan_run_id: PlanRunUUID,
    ) -> Output:
        """Save an output from a plan run to agent memory.

        Args:
            output_name (str): The name of the output within the plan
            output (Output): The Output object to save
            plan_run_id (PlanRunUUID): The ID of the current plan run

        Returns:
            Output: The Output object with value marked as stored in agent memory.

        Raises:
            NotImplementedError: If the method is not implemented.

        """

    @abstractmethod
    def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> Output:
        """Retrieve an Output from the storage.

        Args:
            output_name (str): The name of the output to retrieve
            plan_run_id (PlanRunUUID): The ID of the plan run

        Returns:
            Output: The retrieved Output object with value filled in from agent memory.

        Raises:
            NotImplementedError: If the method is not implemented.

        """


class InMemoryStorage(PlanStorage, RunStorage, LogAdditionalStorage, AgentMemory):
    """Simple storage class that keeps plans + runs in memory.

    Tool Calls are logged via the LogAdditionalStorage.
    """

    plans: dict[PlanUUID, Plan]
    runs: dict[PlanRunUUID, PlanRun]
    outputs: defaultdict[PlanRunUUID, dict[str, Output]]

    def __init__(self) -> None:
        """Initialize Storage."""
        self.plans = {}
        self.runs = {}
        self.outputs = defaultdict(dict)

    def save_plan(self, plan: Plan) -> None:
        """Add plan to dict.

        Args:
            plan (Plan): The Plan object to save.

        """
        self.plans[plan.id] = plan

    def get_plan(self, plan_id: PlanUUID) -> Plan:
        """Get plan from dict.

        Args:
            plan_id (PlanUUID): The UUID of the plan to retrieve.

        Returns:
            Plan: The Plan object associated with the provided plan_id.

        Raises:
            PlanNotFoundError: If the plan is not found.

        """
        if plan_id in self.plans:
            return self.plans[plan_id]
        raise PlanNotFoundError(plan_id)

    def save_plan_run(self, plan_run: PlanRun) -> None:
        """Add run to dict.

        Args:
            plan_run (PlanRun): The Run object to save.

        """
        self.runs[plan_run.id] = plan_run

    def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
        """Get run from dict.

        Args:
            plan_run_id (PlanRunUUID): The UUID of the PlanRun to retrieve.

        Returns:
            PlanRun: The PlanRun object associated with the provided plan_run_id.

        Raises:
            PlanRunNotFoundError: If the PlanRun is not found.

        """
        if plan_run_id in self.runs:
            return self.runs[plan_run_id]
        raise PlanRunNotFoundError(plan_run_id)

    def get_plan_runs(
        self,
        run_state: PlanRunState | None = None,
        page: int | None = None,  # noqa: ARG002
    ) -> PlanRunListResponse:
        """Get run from dict.

        Args:
            run_state (RunState | None): Optionally filter runs by their state.
            page (int | None): Optional pagination data which is not used for in memory storage.

        Returns:
            list[Run]: A list of Run objects that match the given state.

        """
        if not run_state:
            results = list(self.runs.values())
        else:
            results = [plan_run for plan_run in self.runs.values() if plan_run.state == run_state]

        return PlanRunListResponse(
            results=results,
            count=len(results),
            current_page=1,
            total_pages=1,
        )

    def save_plan_run_output(
        self,
        output_name: str,
        output: Output,
        plan_run_id: PlanRunUUID,
    ) -> Output:
        """Save Output from a plan run to memory.

        Args:
            output_name (str): The name of the output within the plan
            output (Output): The Output object to save
            plan_run_id (PlanRunUUID): The ID of the current plan run

        """
        if output.get_summary() is None:
            logger().warning(
                f"Storing Output {output} with no summary",
            )
        self.outputs[plan_run_id][output_name] = output
        return AgentMemoryOutput(
            output_name=output_name,
            plan_run_id=plan_run_id,
            summary=output.get_summary() or "",
        )

    def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> Output:
        """Retrieve an Output from memory.

        Args:
            output_name (str): The name of the output to retrieve
            plan_run_id (PlanRunUUID): The ID of the plan run

        Returns:
            Output: The retrieved Output object

        Raises:
            KeyError: If the output is not found

        """
        return self.outputs[plan_run_id][output_name]


class DiskFileStorage(PlanStorage, RunStorage, LogAdditionalStorage, AgentMemory):
    """Disk-based implementation of the Storage interface.

    Stores serialized Plan and Run objects as JSON files on disk.
    """

    def __init__(self, storage_dir: str | None) -> None:
        """Set storage dir.

        Args:
            storage_dir (str | None): Optional directory for storing files.

        """
        self.storage_dir = storage_dir or ".portia"

    def _ensure_storage(self, file_path: str | None = None) -> None:
        """Ensure that we have the storage directories required.

        This ensures that the storage directory exists as well as any other sub-directories
        needed for the file_path.

        Raises:
            FileNotFoundError: If the directory cannot be created.

        """
        Path(self.storage_dir).mkdir(parents=True, exist_ok=True)
        if file_path:
            Path(self.storage_dir, file_path).parent.mkdir(parents=True, exist_ok=True)

    def _write(self, file_path: str, content: BaseModel) -> None:
        """Write a serialized Plan or Run to a JSON file.

        Args:
            file_path (str): Path of the file to write.
            content (BaseModel): The Plan or Run object to serialize.

        """
        self._ensure_storage(file_path)  # Ensure storage directory exists
        with Path(self.storage_dir, file_path).open("w") as file:
            file.write(content.model_dump_json(indent=4))

    def _read(self, file_name: str, model: type[T]) -> T:
        """Read a JSON file and deserialize it into a BaseModel instance.

        Args:
            file_name (str): Name of the file to read.
            model (type[T]): The model class to deserialize into.

        Returns:
            T: The deserialized model instance.

        Raises:
            FileNotFoundError: If the file is not found.
            ValidationError: If the deserialization fails.

        """
        with Path(self.storage_dir, file_name).open("r") as file:
            f = file.read()
            return model.model_validate_json(f)

    def save_plan(self, plan: Plan) -> None:
        """Save a Plan object to the storage.

        Args:
            plan (Plan): The Plan object to save.

        """
        self._write(f"{plan.id}.json", plan)

    def get_plan(self, plan_id: PlanUUID) -> Plan:
        """Retrieve a Plan object by its ID.

        Args:
            plan_id (PlanUUID): The ID of the Plan to retrieve.

        Returns:
            Plan: The retrieved Plan object.

        Raises:
            PlanNotFoundError: If the Plan is not found or validation fails.

        """
        try:
            return self._read(f"{plan_id}.json", Plan)
        except (ValidationError, FileNotFoundError) as e:
            raise PlanNotFoundError(plan_id) from e

    def save_plan_run(self, plan_run: PlanRun) -> None:
        """Save PlanRun object to the storage.

        Args:
            plan_run (PlanRun): The Run object to save.

        """
        self._write(f"{plan_run.id}.json", plan_run)

    def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
        """Retrieve PlanRun object by its ID.

        Args:
            plan_run_id (RunUUID): The ID of the Run to retrieve.

        Returns:
            Run: The retrieved Run object.

        Raises:
            RunNotFoundError: If the Run is not found or validation fails.

        """
        try:
            return self._read(f"{plan_run_id}.json", PlanRun)
        except (ValidationError, FileNotFoundError) as e:
            raise PlanRunNotFoundError(plan_run_id) from e

    def get_plan_runs(
        self,
        run_state: PlanRunState | None = None,
        page: int | None = None,  # noqa: ARG002
    ) -> PlanRunListResponse:
        """Find all plan runs in storage that match state.

        Args:
            run_state (RunState | None): Optionally filter runs by their state.
            page (int | None): Optional pagination data which is not used for in memory storage.

        Returns:
            list[Run]: A list of Run objects that match the given state.

        """
        self._ensure_storage()

        plan_runs = []

        directory_path = Path(self.storage_dir)
        for f in directory_path.iterdir():
            if f.is_file() and f.name.startswith(PLAN_RUN_UUID_PREFIX):
                plan_run = self._read(f.name, PlanRun)
                if not run_state or plan_run.state == run_state:
                    plan_runs.append(plan_run)

        return PlanRunListResponse(
            results=plan_runs,
            count=len(plan_runs),
            current_page=1,
            total_pages=1,
        )

    def save_plan_run_output(
        self,
        output_name: str,
        output: Output,
        plan_run_id: PlanRunUUID,
    ) -> Output:
        """Save Output from a plan run to agent memory on disk.

        Args:
            output_name (str): The name of the output within the plan
            output (Output): The Output object to save
            plan_run_id (PlanRunUUID): The ID of the current plan run

        """
        filename = f"{plan_run_id}/{output_name}.json"
        self._write(filename, output)
        return AgentMemoryOutput(
            output_name=output_name,
            plan_run_id=plan_run_id,
            summary=output.get_summary() or "",
        )

    def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> Output:
        """Retrieve an Output from agent memory on disk.

        Args:
            output_name (str): The name of the output to retrieve
            plan_run_id (PlanRunUUID): The ID of the plan run

        Returns:
            Output: The retrieved Output object

        Raises:
            FileNotFoundError: If the output file is not found
            ValidationError: If the deserialization fails

        """
        file_name = f"{plan_run_id}/{output_name}.json"
        return self._read(file_name, LocalOutput)


class PortiaCloudStorage(Storage, AgentMemory):
    """Save plans, runs and tool calls to portia cloud."""

    DEFAULT_MAX_CACHE_SIZE = 20

    def __init__(
        self,
        config: Config,
        cache_dir: str | None = None,
        max_cache_size: int = DEFAULT_MAX_CACHE_SIZE,
    ) -> None:
        """Initialize the PortiaCloudStorage instance.

        Args:
            config (Config): The configuration containing API details for Portia Cloud.
            cache_dir (str | None): Optional directory for local caching of outputs.
            max_cache_size (int): The maximum number of files to cache locally.

        """
        self.client = PortiaCloudClient().get_client(config)
        self.form_client = PortiaCloudClient().new_client(config, json_headers=False)
        self.cache_dir = cache_dir or ".portia/cache/agent_memory"
        self.max_cache_size = max_cache_size
        self._ensure_cache_dir()

    def _ensure_cache_dir(self, file_path: str | None = None) -> None:
        """Ensure that we have the cache directories required.

        This ensures that the cache directory exists as well as any other sub-directories
        needed for the file_path.

        Args:
            file_path (str | None): Optional path to ensure parent directories exist.

        """
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
        if file_path:
            Path(self.cache_dir, file_path).parent.mkdir(parents=True, exist_ok=True)

    def _ensure_cache_size(self) -> None:
        """Manage the cache size by removing the oldest file if the cache is full."""
        json_files = list(Path(self.cache_dir).glob("**/*.json"))
        if len(json_files) >= self.max_cache_size:
            oldest_file = min(json_files, key=lambda f: f.stat().st_mtime)
            oldest_file.unlink()
            logger().debug(f"Removed oldest cache file: {oldest_file}")

    def _write_to_cache(self, file_path: str, content: BaseModel) -> None:
        """Write a serialized object to a JSON file in the cache.

        Args:
            file_path (str): Path of the file to write.
            content (BaseModel): The object to serialize.

        """
        self._ensure_cache_dir(file_path)
        self._ensure_cache_size()

        # Write the file
        with Path(self.cache_dir, file_path).open("w") as file:
            file.write(content.model_dump_json(indent=4))

    def _read_from_cache(self, file_name: str, model: type[T]) -> T:
        """Read a JSON file from cache and deserialize it into a BaseModel instance.

        Args:
            file_name (str): Name of the file to read.
            model (type[T]): The model class to deserialize into.

        Returns:
            T: The deserialized model instance.

        Raises:
            FileNotFoundError: If the file is not found.
            ValidationError: If the deserialization fails.

        """
        with Path(self.cache_dir, file_name).open("r") as file:
            f = file.read()
            return model.model_validate_json(f)

    def check_response(self, response: httpx.Response) -> None:
        """Validate the response from Portia API.

        Args:
            response (httpx.Response): The response from the Portia API to check.

        Raises:
            StorageError: If the response from the Portia API indicates an error.

        """
        if not response.is_success:
            error_str = str(response.content)
            logger().error(f"Error from Portia Cloud: {error_str}")
            raise StorageError(error_str)

    def save_plan(self, plan: Plan) -> None:
        """Save a plan to Portia Cloud.

        Args:
            plan (Plan): The Plan object to save to the cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails.

        """
        try:
            response = self.client.post(
                url="/api/v0/plans/",
                json={
                    "id": str(plan.id),
                    "query": plan.plan_context.query,
                    "tool_ids": plan.plan_context.tool_ids,
                    "steps": [step.model_dump(mode="json") for step in plan.steps],
                },
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)

    def get_plan(self, plan_id: PlanUUID) -> Plan:
        """Retrieve a plan from Portia Cloud.

        Args:
            plan_id (PlanUUID): The ID of the plan to retrieve.

        Returns:
            Plan: The Plan object retrieved from Portia Cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails or the plan does not exist.

        """
        try:
            response = self.client.get(
                url=f"/api/v0/plans/{plan_id}/",
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)
            response_json = response.json()
            return Plan(
                id=PlanUUID.from_string(response_json["id"]),
                plan_context=PlanContext(
                    query=response_json["query"],
                    tool_ids=response_json["tool_ids"],
                ),
                steps=[Step.model_validate(step) for step in response_json["steps"]],
            )

    def save_plan_run(self, plan_run: PlanRun) -> None:
        """Save PlanRun to Portia Cloud.

        Args:
            plan_run (PlanRun): The Run object to save to the cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails.

        """
        try:
            response = self.client.put(
                url=f"/api/v0/plan-runs/{plan_run.id}/",
                json={
                    "current_step_index": plan_run.current_step_index,
                    "state": plan_run.state,
                    "execution_context": plan_run.execution_context.model_dump(mode="json"),
                    "outputs": plan_run.outputs.model_dump(mode="json"),
                    "plan_id": str(plan_run.plan_id),
                },
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)

    def get_plan_run(self, plan_run_id: PlanRunUUID) -> PlanRun:
        """Retrieve PlanRun from Portia Cloud.

        Args:
            plan_run_id (RunUUID): The ID of the run to retrieve.

        Returns:
            Run: The Run object retrieved from Portia Cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails or the run does not exist.

        """
        try:
            response = self.client.get(
                url=f"/api/v0/plan-runs/{plan_run_id}/",
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)
            response_json = response.json()
            return PlanRun(
                id=PlanRunUUID.from_string(response_json["id"]),
                plan_id=PlanUUID.from_string(response_json["plan"]["id"]),
                current_step_index=response_json["current_step_index"],
                state=PlanRunState(response_json["state"]),
                execution_context=ExecutionContext.model_validate(
                    response_json["execution_context"],
                ),
                outputs=PlanRunOutputs.model_validate(response_json["outputs"]),
            )

    def get_plan_runs(
        self,
        run_state: PlanRunState | None = None,
        page: int | None = None,
    ) -> PlanRunListResponse:
        """Find all runs in storage that match state.

        Args:
            run_state (RunState | None): Optionally filter runs by their state.
            page (int | None): Optional pagination data which is not used for in memory storage.

        Returns:
            list[Run]: A list of Run objects retrieved from Portia Cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails.

        """
        try:
            query = {}
            if page:
                query["page"] = page
            if run_state:
                query["run_state"] = run_state.value
            response = self.client.get(
                url=f"/api/v0/plan-runs/?{urlencode(query)}",
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)
            response_json = response.json()
            return PlanRunListResponse(
                results=[
                    PlanRun(
                        id=PlanRunUUID.from_string(plan_run["id"]),
                        plan_id=PlanUUID.from_string(plan_run["plan"]["id"]),
                        current_step_index=plan_run["current_step_index"],
                        state=PlanRunState(plan_run["state"]),
                        execution_context=ExecutionContext.model_validate(
                            plan_run["execution_context"],
                        ),
                        outputs=PlanRunOutputs.model_validate(plan_run["outputs"]),
                    )
                    for plan_run in response_json["results"]
                ],
                count=response_json["count"],
                current_page=response_json["current_page"],
                total_pages=response_json["total_pages"],
            )

    def save_tool_call(self, tool_call: ToolCallRecord) -> None:
        """Save a tool call to Portia Cloud.

        Args:
            tool_call (ToolCallRecord): The ToolCallRecord object to save to the cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails.

        """
        try:
            response = self.client.post(
                url="/api/v0/tool-calls/",
                json={
                    "plan_run_id": str(tool_call.plan_run_id),
                    "tool_name": tool_call.tool_name,
                    "step": tool_call.step,
                    "end_user_id": tool_call.end_user_id or "",
                    "additional_data": tool_call.additional_data,
                    "input": tool_call.input,
                    "output": tool_call.output,
                    "status": tool_call.status,
                    "latency_seconds": tool_call.latency_seconds,
                },
            )
        except Exception as e:
            raise StorageError(e) from e
        else:
            self.check_response(response)
            LogAdditionalStorage.save_tool_call(self, tool_call)

    def save_plan_run_output(
        self,
        output_name: str,
        output: Output,
        plan_run_id: PlanRunUUID,
    ) -> Output:
        """Save Output from a plan run to Portia Cloud.

        Args:
            output_name (str): The name of the output within the plan
            output (Output): The Output object to save
            plan_run_id (PlanRun): The if of the current plan run

        Raises:
            StorageError: If the request to Portia Cloud fails.

        """
        try:
            response = self.form_client.put(
                url=f"/api/v0/agent-memory/plan-runs/{plan_run_id}/outputs/{output_name}/",
                files={
                    "value": (
                        "output",
                        BytesIO(output.serialize_value().encode("utf-8")),
                    ),
                },
                data={
                    "summary": output.get_summary(),
                },
            )
            self.check_response(response)

            # Save to local cache
            if isinstance(output, LocalOutput):
                cache_file_path = f"{plan_run_id}/{output_name}.json"
                self._write_to_cache(cache_file_path, output)
                logger().debug(f"Saved output to local cache: {cache_file_path}")

            return AgentMemoryOutput(
                output_name=output_name,
                plan_run_id=plan_run_id,
                summary=output.get_summary() or "",
            )
        except Exception as e:
            raise StorageError(e) from e

    def get_plan_run_output(self, output_name: str, plan_run_id: PlanRunUUID) -> Output:
        """Retrieve an Output from Portia Cloud.

        Args:
            output_name: The name of the output to get from memory
            plan_run_id (RunUUID): The ID of the run to retrieve.

        Returns:
            Run: The Run object retrieved from Portia Cloud.

        Raises:
            StorageError: If the request to Portia Cloud fails or the run does not exist.

        """
        # Try to get from local cache first
        cache_file_path = f"{plan_run_id}/{output_name}.json"
        try:
            return self._read_from_cache(cache_file_path, LocalOutput)
        except (FileNotFoundError, ValidationError):
            # If not in cache, fetch from Portia Cloud
            logger().debug(
                f"Output not found in local cache, fetching from Portia Cloud: {cache_file_path}",
            )

        try:
            # Retrieving a value is a two step process
            # 1. Get the output with the storage URL from the backend
            # 2. Fetch the value from the storage URL
            output_response = self.client.get(
                url=f"/api/v0/agent-memory/plan-runs/{plan_run_id}/outputs/{output_name}/",
            )
            self.check_response(output_response)
            output_json = output_response.json()
            summary = output_json["summary"]
            value_url = output_json["url"]

            with httpx.Client() as client:
                value_response = client.get(value_url)
                value_response.raise_for_status()

            # Create the output object
            output = LocalOutput(
                summary=summary,
                value=value_response.text,
            )

            # Save to local cache for future use
            self._write_to_cache(cache_file_path, output)
            logger().debug(f"Saved output to local cache: {cache_file_path}")
        except Exception as e:
            raise StorageError(e) from e
        else:
            return output

```

## File: portia/tool_registry.py

```python
"""A ToolRegistry represents a source of tools.

This module defines various implementations of `ToolRegistry`, which is responsible for managing
and interacting with tools. It provides interfaces for registering, retrieving, and listing tools.
The `ToolRegistry` can also support aggregation of multiple registries and searching for tools
based on queries.

Classes:
    ToolRegistry: The base interface for managing tools.
    AggregatedToolRegistry: A registry that aggregates multiple tool registries.
    InMemoryToolRegistry: A simple in-memory implementation of `ToolRegistry`.
    PortiaToolRegistry: A tool registry that interacts with the Portia API to manage tools.
    MCPToolRegistry: A tool registry that interacts with a locally running MCP server.
"""

from __future__ import annotations

import asyncio
import os
import re
from enum import StrEnum
from typing import TYPE_CHECKING, Any, Callable, Literal, Union

from jsonref import replace_refs
from pydantic import BaseModel, Field, create_model

from portia.cloud import PortiaCloudClient
from portia.errors import DuplicateToolError, ToolNotFoundError
from portia.logger import logger
from portia.mcp_session import (
    McpClientConfig,
    SseMcpClientConfig,
    StdioMcpClientConfig,
    get_mcp_session,
)
from portia.open_source_tools.calculator_tool import CalculatorTool
from portia.open_source_tools.image_understanding_tool import ImageUnderstandingTool
from portia.open_source_tools.llm_tool import LLMTool
from portia.open_source_tools.local_file_reader_tool import FileReaderTool
from portia.open_source_tools.local_file_writer_tool import FileWriterTool
from portia.open_source_tools.search_tool import SearchTool
from portia.open_source_tools.weather import WeatherTool
from portia.tool import PortiaMcpTool, PortiaRemoteTool, Tool

if TYPE_CHECKING:
    from collections.abc import Sequence

    import httpx
    import mcp

    from portia.config import Config


class ToolRegistry:
    """ToolRegistry is the base class for managing tools.

    This class implements the essential methods for interacting with tool registries, including
    registering, retrieving, and listing tools. Specific tool registries can override these methods
    and provide additional functionality.

    Methods:
        with_tool(tool: Tool, *, overwrite: bool = False) -> None:
            Inserts a new tool.
        replace_tool(tool: Tool) -> None:
            Replaces a tool with a new tool.
            NB. This is a shortcut for `with_tool(tool, overwrite=True)`.
        get_tool(tool_id: str) -> Tool:
            Retrieves a tool by its ID.
        get_tools() -> list[Tool]:
            Retrieves all tools in the registry.
        match_tools(query: str | None = None, tool_ids: list[str] | None = None) -> list[Tool]:
            Optionally, retrieve tools that match a given query and tool_ids. Useful to implement
            tool filtering.

    """

    def __init__(self, tools: dict[str, Tool] | Sequence[Tool] | None = None) -> None:
        """Initialize the tool registry with a sequence or dictionary of tools.

        Args:
            tools (dict[str, Tool] | Sequence[Tool]): A sequence of tools or a
              dictionary of tool IDs to tools.

        """
        if tools is None:
            self._tools = {}
        elif not isinstance(tools, dict):
            self._tools = {tool.id: tool for tool in tools}
        else:
            self._tools = tools

    def with_tool(self, tool: Tool, *, overwrite: bool = False) -> None:
        """Update a tool based on tool ID or inserts a new tool.

        Args:
            tool (Tool): The tool to be added or updated.
            overwrite (bool): Whether to overwrite an existing tool with the same ID.

        Returns:
            None: The tool registry is updated in place.

        """
        if tool.id in self._tools and not overwrite:
            raise DuplicateToolError(tool.id)
        self._tools[tool.id] = tool

    def replace_tool(self, tool: Tool) -> None:
        """Replace a tool with a new tool.

        Args:
            tool (Tool): The tool to replace the existing tool with.

        Returns:
            None: The tool registry is updated in place.

        """
        self.with_tool(tool, overwrite=True)

    def get_tool(self, tool_id: str) -> Tool:
        """Retrieve a tool's information.

        Args:
            tool_id (str): The ID of the tool to retrieve.

        Returns:
            Tool: The requested tool.

        Raises:
            ToolNotFoundError: If the tool with the given ID does not exist.

        """
        if tool_id not in self._tools:
            raise ToolNotFoundError(tool_id)
        return self._tools[tool_id]

    def get_tools(self) -> list[Tool]:
        """Get all tools registered with the registry.

        Returns:
            list[Tool]: A list of all tools in the registry.

        """
        return list(self._tools.values())

    def match_tools(
        self,
        query: str | None = None,  # noqa: ARG002 - useful to have variable name
        tool_ids: list[str] | None = None,
    ) -> list[Tool]:
        """Provide a set of tools that match a given query and tool_ids.

        Args:
            query (str | None): The query to match tools against.
            tool_ids (list[str] | None): The list of tool ids to match.

        Returns:
            list[Tool]: A list of tools matching the query.

        This method is useful to implement tool filtering whereby only a selection of tools are
        passed to the PlanningAgent based on the query.
        This method is optional to implement and will default to providing all tools.

        """
        return (
            [tool for tool in self.get_tools() if tool.id in tool_ids]
            if tool_ids
            else self.get_tools()
        )

    def filter_tools(self, predicate: Callable[[Tool], bool]) -> ToolRegistry:
        """Filter the tools in the registry based on a predicate.

        Args:
            predicate (Callable[[Tool], bool]): A predicate to filter the tools.

        Returns:
            Self: A new ToolRegistry with the filtered tools.

        """
        return ToolRegistry({tool.id: tool for tool in self._tools.values() if predicate(tool)})

    def __add__(self, other: ToolRegistry | list[Tool]) -> ToolRegistry:
        """Return an aggregated tool registry combining two registries or a registry and tool list.

        Tool IDs must be unique across the two registries otherwise an error will be thrown.

        Args:
            other (ToolRegistry): Another tool registry to be combined.

        Returns:
            AggregatedToolRegistry: A new tool registry containing tools from both registries.

        """
        return self._add(other)

    def __radd__(self, other: ToolRegistry | list[Tool]) -> ToolRegistry:
        """Return an aggregated tool registry combining two registries or a registry and tool list.

        Tool IDs must be unique across the two registries otherwise an error will be thrown.

        Args:
            other (ToolRegistry): Another tool registry to be combined.

        Returns:
            ToolRegistry: A new tool registry containing tools from both registries.

        """
        return self._add(other)

    def _add(self, other: ToolRegistry | list[Tool]) -> ToolRegistry:
        """Add a tool registry or Tool list to the current registry."""
        other_registry = other if isinstance(other, ToolRegistry) else ToolRegistry(other)
        self_tools = self.get_tools()
        other_tools = other_registry.get_tools()
        tools = {}
        for tool in [*self_tools, *other_tools]:
            if tool.id in tools:
                logger().warning(
                    f"Duplicate tool ID found: {tool.id!s}. Unintended behavior may occur.",
                )
            tools[tool.id] = tool

        return ToolRegistry(tools)


class InMemoryToolRegistry(ToolRegistry):
    """Provides a simple in-memory tool registry.

    This class stores tools in memory, allowing for quick access without persistence.

    Warning: This registry is DEPRECATED. Use ToolRegistry instead.
    """

    @classmethod
    def from_local_tools(cls, tools: Sequence[Tool]) -> InMemoryToolRegistry:
        """Easily create a local tool registry from a sequence of tools.

        Args:
            tools (Sequence[Tool]): A sequence of tools to initialize the registry.

        Returns:
            InMemoryToolRegistry: A new in-memory tool registry.

        """
        return cls(tools)


class PortiaToolRegistry(ToolRegistry):
    """Provides access to Portia tools.

    This class interacts with the Portia API to retrieve and manage tools.
    """

    EXCLUDED_BY_DEFAULT_TOOL_REGEXS: frozenset[str] = frozenset(
        {
            # Exclude Outlook by default as it clashes with Gmail
            "portia:microsoft:outlook:*",
        },
    )

    def __init__(
        self,
        config: Config | None = None,
        client: httpx.Client | None = None,
        tools: dict[str, Tool] | Sequence[Tool] | None = None,
    ) -> None:
        """Initialize the PortiaToolRegistry with the given configuration.

        Args:
            config (Config | None): The configuration containing the API key and endpoint.
            client (httpx.Client | None): An optional httpx client to use. If not provided, a new
              client will be created.
            tools (dict[str, Tool] | None): A dictionary of tool IDs to tools to create the
              registry with. If not provided, all tools will be loaded from the Portia API.

        """
        if tools is not None:
            super().__init__(tools)
        elif client is not None:
            super().__init__(self._load_tools(client))
        elif config is not None:
            client = PortiaCloudClient().get_client(config)
            super().__init__(self._load_tools(client))
        else:
            raise ValueError("Either config, client or tools must be provided")

    def with_default_tool_filter(self) -> PortiaToolRegistry:
        """Create a PortiaToolRegistry with a default tool filter."""

        def default_tool_filter(tool: Tool) -> bool:
            """Filter out tools that match the default tool regexes."""
            return not any(
                re.match(regex, tool.id) for regex in self.EXCLUDED_BY_DEFAULT_TOOL_REGEXS
            )

        return PortiaToolRegistry(tools=self.filter_tools(default_tool_filter).get_tools())

    @classmethod
    def _load_tools(cls, client: httpx.Client) -> dict[str, Tool]:
        """Load the tools from the API into the into the internal storage."""
        response = client.get(
            url="/api/v0/tools/descriptions/",
        )
        response.raise_for_status()
        tools = {}
        for raw_tool in response.json():
            tool = PortiaRemoteTool(
                id=raw_tool["tool_id"],
                name=raw_tool["tool_name"],
                should_summarize=raw_tool.get("should_summarize", False),
                description=raw_tool["description"]["overview_description"],
                args_schema=generate_pydantic_model_from_json_schema(
                    raw_tool["tool_name"],
                    raw_tool["schema"],
                ),
                output_schema=(
                    raw_tool["description"]["overview"],
                    raw_tool["description"]["output_description"],
                ),
                # pass API info
                client=client,
            )
            tools[raw_tool["tool_id"]] = tool
        return tools


class McpToolRegistry(ToolRegistry):
    """Provides access to tools within a Model Context Protocol (MCP) server.

    See https://modelcontextprotocol.io/introduction for more information on MCP.
    """

    def __init__(self, mcp_client_config: McpClientConfig) -> None:
        """Initialize the MCPToolRegistry with the given configuration."""
        super().__init__({t.id: t for t in self._load_tools(mcp_client_config)})

    @classmethod
    def from_sse_connection(
        cls,
        server_name: str,
        url: str,
        headers: dict[str, Any] | None = None,
        timeout: float = 5,
        sse_read_timeout: float = 60 * 5,
    ) -> McpToolRegistry:
        """Create a new MCPToolRegistry using an SSE connection."""
        return cls(
            SseMcpClientConfig(
                server_name=server_name,
                url=url,
                headers=headers,
                timeout=timeout,
                sse_read_timeout=sse_read_timeout,
            ),
        )

    @classmethod
    def from_stdio_connection(  # noqa: PLR0913
        cls,
        server_name: str,
        command: str,
        args: list[str] | None = None,
        env: dict[str, str] | None = None,
        encoding: str = "utf-8",
        encoding_error_handler: Literal["strict", "ignore", "replace"] = "strict",
    ) -> McpToolRegistry:
        """Create a new MCPToolRegistry using a stdio connection."""
        return cls(
            StdioMcpClientConfig(
                server_name=server_name,
                command=command,
                args=args if args is not None else [],
                env=env,
                encoding=encoding,
                encoding_error_handler=encoding_error_handler,
            ),
        )

    @classmethod
    def _load_tools(cls, mcp_client_config: McpClientConfig) -> list[PortiaMcpTool]:
        """Get a list of tools from an MCP server wrapped at Portia tools.

        Args:
            mcp_client_config: The configuration for the MCP client

        Returns:
            A list of Portia tools

        """

        async def async_inner() -> list[PortiaMcpTool]:
            async with get_mcp_session(mcp_client_config) as session:
                logger().debug("Fetching tools from MCP server")
                tools = await session.list_tools()
                logger().debug(f"Got {len(tools.tools)} tools from MCP server")
                return [
                    cls._portia_tool_from_mcp_tool(tool, mcp_client_config) for tool in tools.tools
                ]

        return asyncio.run(async_inner())

    @classmethod
    def _portia_tool_from_mcp_tool(
        cls,
        mcp_tool: mcp.Tool,
        mcp_client_config: McpClientConfig,
    ) -> PortiaMcpTool:
        """Conversion of a remote MCP server tool to a Portia tool."""
        tool_name_snake_case = re.sub(r"[^a-zA-Z0-9]+", "_", mcp_tool.name)

        description = (
            mcp_tool.description
            if mcp_tool.description is not None
            else f"{mcp_tool.name} tool from {mcp_client_config.server_name}"
        )

        return PortiaMcpTool(
            id=f"mcp:{mcp_client_config.server_name}:{tool_name_snake_case}",
            name=mcp_tool.name,
            description=description,
            args_schema=generate_pydantic_model_from_json_schema(
                f"{tool_name_snake_case}_schema",
                mcp_tool.inputSchema,
            ),
            output_schema=("str", "The response from the tool formatted as a JSON string"),
            mcp_client_config=mcp_client_config,
        )


class DefaultToolRegistry(ToolRegistry):
    """A registry providing a default set of tools.

    This includes the following tools:
    - All open source tools that don't require API keys
    - Search tool if you have a Tavily API key
    - Weather tool if you have an OpenWeatherMap API key
    - Portia cloud tools if you have a Portia cloud API key
    """

    def __init__(self, config: Config) -> None:
        """Initialize the default tool registry with the given configuration."""
        tools = [
            CalculatorTool(),
            LLMTool(),
            FileWriterTool(),
            FileReaderTool(),
            ImageUnderstandingTool(),
        ]
        if os.getenv("TAVILY_API_KEY"):
            tools.append(SearchTool())
        if os.getenv("OPENWEATHERMAP_API_KEY"):
            tools.append(WeatherTool())

        if config.portia_api_key:
            tools.extend(PortiaToolRegistry(config).with_default_tool_filter().get_tools())

        super().__init__(tools)


def generate_pydantic_model_from_json_schema(
    model_name: str,
    json_schema: dict[str, Any],
) -> type[BaseModel]:
    """Generate a Pydantic model based on a JSON schema.

    Args:
        model_name (str): The name of the Pydantic model.
        json_schema (dict[str, Any]): The schema to generate the model from.

    Returns:
        type[BaseModel]: The generated Pydantic model class.

    """
    schema_without_refs = replace_refs(json_schema, proxies=False)

    # Extract properties and required fields
    properties = schema_without_refs.get("properties", {})  # type: ignore  # noqa: PGH003
    required = set(schema_without_refs.get("required", []))  # type: ignore  # noqa: PGH003

    # Define fields for the model
    fields = dict(
        [
            _generate_field(key, value, required=key in required)
            for key, value in properties.items()
        ],
    )

    # Create the Pydantic model dynamically
    return create_model(model_name, **fields)  # type: ignore  # noqa: PGH003 - We want to use default config


def _generate_field(
    field_name: str,
    field: dict[str, Any],
    *,
    required: bool,
) -> tuple[str, tuple[type | Any, Any]]:
    """Generate a Pydantic field from a JSON schema field."""
    default_from_schema = field.get("default")
    return (
        field_name,
        (
            _map_pydantic_type(field_name, field),
            Field(
                default=... if required else default_from_schema,
                description=field.get("description", ""),
            ),
        ),
    )


def _map_pydantic_type(field_name: str, field: dict[str, Any]) -> type | Any:  # noqa: ANN401
    match field:
        case {"type": _}:
            return _map_single_pydantic_type(field_name, field)
        case {"oneOf": union_types} | {"anyOf": union_types}:
            types = [
                _map_single_pydantic_type(field_name, t, allow_nonetype=True) for t in union_types
            ]
            return Union[*types]
        case _:
            logger().debug(f"Unsupported JSON schema type: {field.get('type')}: {field}")
            return Any


def _map_single_pydantic_type(  # noqa: PLR0911
    field_name: str,
    field: dict[str, Any],
    *,
    allow_nonetype: bool = False,
) -> type | Any:  # noqa: ANN401
    match field.get("type"):
        case "string":
            if field.get("enum"):
                return StrEnum(field_name, {v.upper(): v for v in field.get("enum", [])})
            return str
        case "integer":
            return int
        case "number":
            return float
        case "boolean":
            return bool
        case "array":
            item_type = _map_pydantic_type(field_name, field.get("items", {}))
            return list[item_type]
        case "object":
            return generate_pydantic_model_from_json_schema(f"{field_name}_model", field)
        case "null":
            if allow_nonetype:
                return None
            logger().debug(f"Null type is not allowed for a non-union field: {field_name}")
            return Any
        case _:
            logger().debug(f"Unsupported JSON schema type: {field.get('type')}: {field}")
            return Any

```

## File: portia/tool_wrapper.py

```python
"""Tool Wrapper that intercepts run calls and records them.

This module contains the `ToolCallWrapper` class, which wraps around an existing tool and records
information about the tool's execution, such as input, output, latency, and status. The recorded
data is stored in `AdditionalStorage` for later use.

Classes:
    ToolCallWrapper: A wrapper that intercepts tool calls, records execution data, and stores it.
"""

from __future__ import annotations

from datetime import UTC, datetime
from typing import TYPE_CHECKING, Any

from pydantic import ConfigDict

from portia.clarification import Clarification
from portia.common import combine_args_kwargs
from portia.execution_agents.output import LocalOutput
from portia.logger import logger
from portia.storage import AdditionalStorage, ToolCallRecord, ToolCallStatus
from portia.tool import Tool, ToolRunContext

if TYPE_CHECKING:
    from portia.plan_run import PlanRun


class ToolCallWrapper(Tool):
    """Tool Wrapper that records calls to its child tool and sends them to the AdditionalStorage.

    This class is a wrapper around a child tool. It captures the input and output, measures latency,
    and records the status of the execution. The results are then stored in the provided
    `AdditionalStorage`.

    Attributes:
        model_config (ConfigDict): Pydantic configuration that allows arbitrary types.
        _child_tool (Tool): The child tool to be wrapped and executed.
        _storage (AdditionalStorage): Storage mechanism to save tool call records.
        _plan_run (PlanRun): The run context for the current execution.

    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    _child_tool: Tool
    _storage: AdditionalStorage
    _plan_run: PlanRun

    def __init__(self, child_tool: Tool, storage: AdditionalStorage, plan_run: PlanRun) -> None:
        """Initialize parent fields using child_tool's attributes.

        Args:
            child_tool (Tool): The child tool to be wrapped.
            storage (AdditionalStorage): The storage to save execution records.
            plan_run (PlanRun): The PlanRun to execute.

        """
        super().__init__(
            id=child_tool.id,
            name=child_tool.name,
            description=child_tool.description,
            args_schema=child_tool.args_schema,
            output_schema=child_tool.output_schema,
            should_summarize=child_tool.should_summarize,
        )
        self._child_tool = child_tool
        self._storage = storage
        self._plan_run = plan_run

    def ready(self, ctx: ToolRunContext) -> bool:
        """Check if the child tool is ready.

        Args:
            ctx (ToolRunContext): Context of the tool run

        Returns:
            bool: Whether the tool is ready to run

        """
        return self._child_tool.ready(ctx)

    def run(self, ctx: ToolRunContext, *args: Any, **kwargs: Any) -> Any | Clarification:  # noqa: ANN401
        """Run the child tool and store the outcome.

        This method executes the child tool with the provided arguments, records the input,
        output, latency, and status of the execution, and stores the details in `AdditionalStorage`.

        Args:
            ctx (ToolRunContext): The context containing user data and metadata.
            *args (Any): Positional arguments for the child tool.
            **kwargs (Any): Keyword arguments for the child tool.

        Returns:
            Any | Clarification: The output of the child tool or a clarification request.

        Raises:
            Exception: If an error occurs during execution, the exception is logged, and the
                status is set to `FAILED`.

        """
        # initialize empty call record

        record = ToolCallRecord(
            input=combine_args_kwargs(*args, **kwargs),
            output=None,
            latency_seconds=0,
            tool_name=self._child_tool.name,
            plan_run_id=self._plan_run.id,
            step=self._plan_run.current_step_index,
            end_user_id=ctx.execution_context.end_user_id,
            additional_data=ctx.execution_context.additional_data,
            status=ToolCallStatus.IN_PROGRESS,
        )
        logger().info(
            f"Invoking {record.tool_name} with args: {record.input}",
        )
        start_time = datetime.now(tz=UTC)
        try:
            output = self._child_tool.run(ctx, *args, **kwargs)
        except Exception as e:
            record.output = str(e)
            record.latency_seconds = (datetime.now(tz=UTC) - start_time).total_seconds()
            record.status = ToolCallStatus.FAILED
            self._storage.save_tool_call(record)
            raise
        else:
            if isinstance(output, Clarification):
                record.status = ToolCallStatus.NEED_CLARIFICATION
                record.output = output.model_dump(mode="json")
            elif output is None:
                record.output = LocalOutput(value=output).model_dump(mode="json")
                record.status = ToolCallStatus.SUCCESS
            else:
                record.output = output
                record.status = ToolCallStatus.SUCCESS
            record.latency_seconds = (datetime.now(tz=UTC) - start_time).total_seconds()
            self._storage.save_tool_call(record)
        return output

```

## File: portia/errors.py

```python
"""Central definition of error classes.

This module defines custom exception classes used throughout the application. These exceptions
help identify specific error conditions, particularly related to configuration, planning, runs,
tools, and storage. They provide more context and clarity than generic exceptions.

Classes in this file include:

- `ConfigNotFoundError`: Raised when a required configuration value is not found.
- `InvalidConfigError`: Raised when a configuration value is invalid.
- `PlanError`: A base class for exceptions in the query planning_agent module.
- `PlanNotFoundError`: Raised when a plan is not found.
- `PlanRunNotFoundError`: Raised when a PlanRun is not found.
- `ToolNotFoundError`: Raised when a tool is not found.
- `DuplicateToolError`: Raised when a tool is registered with the same name.
- `InvalidToolDescriptionError`: Raised when a tool description is invalid.
- `ToolRetryError`: Raised when a tool fails after retries.
- `ToolFailedError`: Raised when a tool fails with a hard error.
- `InvalidPlanRunStateError`: Raised when a plan run is in an invalid state.
- `InvalidAgentOutputError`: Raised when the agent produces invalid output.
- `ToolHardError`: Raised when a tool encounters an unrecoverable error.
- `ToolSoftError`: Raised when a tool encounters an error that can be retried.
- `StorageError`: Raised when an issue occurs with storage.

"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from portia.plan import PlanUUID
    from portia.plan_run import PlanRunUUID


class PortiaBaseError(Exception):
    """Base class for all our errors."""


class ConfigNotFoundError(PortiaBaseError):
    """Raised when a required configuration value is not found.

    Args:
        value (str): The name of the configuration value that is missing.

    """

    def __init__(self, value: str) -> None:
        """Set custom error message."""
        super().__init__(f"Config value {value} is not set")


class InvalidConfigError(PortiaBaseError):
    """Raised when a configuration value is invalid.

    Args:
        value (str): The name of the invalid configuration value.
        issue (str): A description of the issue with the configuration value.

    """

    def __init__(self, value: str, issue: str) -> None:
        """Set custom error message."""
        self.message = f"Config value {value.upper()} is not valid - {issue}"
        super().__init__(self.message)


class PlanError(PortiaBaseError):
    """Base class for exceptions in the query planning_agent module.

    This exception indicates an error that occurred during the planning phase.

    Args:
        error_string (str): A description of the error encountered during planning.

    """

    def __init__(self, error_string: str) -> None:
        """Set custom error message."""
        super().__init__(f"Error during planning: {error_string}")


class PlanNotFoundError(PortiaBaseError):
    """Raised when a plan with a specific ID is not found.

    Args:
        plan_id (PlanUUID): The ID of the plan that was not found.

    """

    def __init__(self, plan_id: PlanUUID) -> None:
        """Set custom error message."""
        super().__init__(f"Plan with id {plan_id!s} not found.")


class PlanRunNotFoundError(PortiaBaseError):
    """Raised when a PlanRun with a specific ID is not found.

    Args:
        plan_run_id (UUID | str | None): The ID or name of the PlanRun that was not found.

    """

    def __init__(self, plan_run_id: PlanRunUUID | str | None) -> None:
        """Set custom error message."""
        super().__init__(f"Run with id {plan_run_id!s} not found.")


class ToolNotFoundError(PortiaBaseError):
    """Raised when a tool with a specific ID is not found.

    Args:
        tool_id (str): The ID of the tool that was not found.

    """

    def __init__(self, tool_id: str) -> None:
        """Set custom error message."""
        super().__init__(f"Tool with id {tool_id} not found.")


class DuplicateToolError(PortiaBaseError):
    """Raised when a tool is registered with the same name.

    Args:
        tool_id (str): The ID of the tool that already exists.

    """

    def __init__(self, tool_id: str) -> None:
        """Set custom error message."""
        super().__init__(f"Tool with id {tool_id} already exists.")


class InvalidToolDescriptionError(PortiaBaseError):
    """Raised when a tool description is invalid.

    Args:
        tool_id (str): The ID of the tool with an invalid description.

    """

    def __init__(self, tool_id: str) -> None:
        """Set custom error message."""
        super().__init__(f"Invalid Description for tool with id {tool_id}")


class ToolRetryError(PortiaBaseError):
    """Raised when a tool fails after retrying.

    Args:
        tool_id (str): The ID of the tool that failed.
        error_string (str): A description of the error that occurred.

    """

    def __init__(self, tool_id: str, error_string: str) -> None:
        """Set custom error message."""
        super().__init__(f"Tool {tool_id} failed after retries: {error_string}")


class ToolFailedError(PortiaBaseError):
    """Raised when a tool fails with a hard error.

    Args:
        tool_id (str): The ID of the tool that failed.
        error_string (str): A description of the error that occurred.

    """

    def __init__(self, tool_id: str, error_string: str) -> None:
        """Set custom error message."""
        super().__init__(f"Tool {tool_id} failed: {error_string}")


class InvalidPlanRunStateError(PortiaBaseError):
    """Raised when a plan run is in an invalid state."""


class InvalidAgentError(PortiaBaseError):
    """Raised when an agent is in an invalid state."""

    def __init__(self, state: str) -> None:
        """Set custom error message."""
        super().__init__(f"Agent returned invalid state: {state}")


class InvalidAgentOutputError(PortiaBaseError):
    """Raised when the agent produces invalid output.

    Args:
        content (str): The invalid content returned by the agent.

    """

    def __init__(self, content: str) -> None:
        """Set custom error message."""
        super().__init__(f"Agent returned invalid content: {content}")


class ToolHardError(PortiaBaseError):
    """Raised when a tool encounters an error it cannot retry.

    Args:
        cause (Exception | str): The underlying exception or error message.

    """

    def __init__(self, cause: Exception | str) -> None:
        """Set custom error message."""
        super().__init__(cause)


class ToolSoftError(PortiaBaseError):
    """Raised when a tool encounters an error that can be retried.

    Args:
        cause (Exception | str): The underlying exception or error message.

    """

    def __init__(self, cause: Exception | str) -> None:
        """Set custom error message."""
        super().__init__(cause)


class StorageError(PortiaBaseError):
    """Raised when there's an issue with storage.

    Args:
        cause (Exception | str): The underlying exception or error message.

    """

    def __init__(self, cause: Exception | str) -> None:
        """Set custom error message."""
        super().__init__(cause)

```

## File: portia/logger.py

```python
"""Logging functions for managing and configuring loggers.

This module defines functions and classes to manage logging within the application. It provides a
`LoggerManager` class that manages the package-level logger and allows customization.
The `LoggerInterface` defines the general interface for loggers, and the default logger is provided
by `loguru`. The `logger` function returns the active logger, and the `LoggerManager` can be used
to configure logging behavior.

Classes in this file include:

- `LoggerInterface`: A protocol defining the common logging methods (`debug`, `info`, `warning`,
`error`, `critical`).
- `LoggerManager`: A class for managing the logger, allowing customization and configuration from
the application's settings.

This module ensures flexible and configurable logging, supporting both default and custom loggers.

"""

from __future__ import annotations

import re
import sys
from typing import TYPE_CHECKING, Any, Protocol

from loguru import logger as default_logger

if TYPE_CHECKING:
    from portia.config import Config

FUNCTION_COLOR_MAP = {
    "tool": "fg 87",
    "clarification": "fg 87",
    "introspection": "fg 87",
    "run": "fg 129",
    "step": "fg 129",
    "plan": "fg 39",
}


class LoggerInterface(Protocol):
    """General Interface for loggers.

    This interface defines the common methods that any logger should implement. The methods are:

    - `debug`: For logging debug-level messages.
    - `info`: For logging informational messages.
    - `warning`: For logging warning messages.
    - `error`: For logging error messages.
    - `critical`: For logging critical error messages.

    These methods are used throughout the application for logging messages at various levels.

    """

    def debug(self, msg: str, *args, **kwargs) -> None: ...  # noqa: ANN002, ANN003, D102
    def info(self, msg: str, *args, **kwargs) -> None: ...  # noqa: ANN002, ANN003, D102
    def warning(self, msg: str, *args, **kwargs) -> None: ...  # noqa: ANN002, ANN003, D102
    def error(self, msg: str, *args, **kwargs) -> None: ...  # noqa: ANN002, ANN003, D102
    def critical(self, msg: str, *args, **kwargs) -> None: ...  # noqa: ANN002, ANN003, D102


class Formatter:
    """A class used to format log records.

    Attributes
    ----------
    max_lines : int
        The maximum number of lines to include in the formatted log message.

    Methods
    -------
    format(record)
        Formats a log record into a string.

    """

    def __init__(self) -> None:
        """Initialize the logger with default settings.

        Attributes:
            max_lines (int): The maximum number of lines the logger can handle, default is 30.

        """
        self.max_lines = 30

    def format(self, record: Any) -> str:  # noqa: ANN401
        """Format a log record into a string with specific formatting.

        Args:
            record (dict): A dictionary containing log record information.
                Expected keys are "message", "extra", "time", "level", "name",
                "function", and "line".

        Returns:
            str: The formatted log record string.

        """
        msg = record["message"]
        if isinstance(msg, str):
            msg = self._sanitize_message_(msg)
        function_color = self._get_function_color_(record)

        # Create the base format string
        result = (
            f"<green>{record['time'].strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}</green> | "
            f"<level>{record['level'].name}</level> | "
            f"<{function_color}>{record['name']}</{function_color}>:"
            f"<{function_color}>{record['function']}</{function_color}>:"
            f"<{function_color}>{record['line']}</{function_color}> - "
            f"<level>{msg}</level>"
        )

        # Add extra information if present
        if record["extra"]:
            result += " | {extra}"

        result += "\n"
        return result

    def _sanitize_message_(self, msg: str) -> str:
        """Sanitize a message to be used in a log record."""
        # doubles opening curly braces in a string { -> {{
        msg = re.sub(r"(?<!\{)\{(?!\{)", "{{", msg)
        # doubles closing curly braces in a string } -> }}
        msg = re.sub(r"(?<!\})\}(?!\})", "}}", msg)
        # escapes < and > in a string
        msg = msg.replace("<", r"\<").replace(">", r"\>")

        return self._truncated_message_(msg)

    def _get_function_color_(self, record: Any) -> str:  # noqa: ANN401
        """Get color based on function/module name. Default is white."""
        return next(
            (
                color
                for key, color in FUNCTION_COLOR_MAP.items()
                if any(key in field for field in [record["function"], record["name"]])
            ),
            "white",
        )

    def _truncated_message_(self, msg: str) -> str:
        lines = msg.split("\n")
        if len(lines) > self.max_lines:
            # Keep first and last parts, truncate the middle
            keep_lines = self.max_lines - 1  # Reserve one line for truncation message
            head_lines = keep_lines // 2
            tail_lines = keep_lines - head_lines

            truncated_lines = lines[:head_lines]
            truncated_lines.append(f"... (truncated {len(lines) - keep_lines} lines) ...")
            truncated_lines.extend(lines[-tail_lines:])
            msg = "\n".join(truncated_lines)
        return msg


class LoggerManager:
    """Manages the package-level logger.

    The `LoggerManager` is responsible for initializing and managing the logger used throughout
    the application. It provides functionality to configure the logger, set a custom logger,
    and adjust logging settings based on the application's configuration.

    Args:
        custom_logger (LoggerInterface | None): A custom logger to be used. If not provided,
                                                 the default `loguru` logger will be used.

    Attributes:
        logger (LoggerInterface): The current active logger.
        custom_logger (bool): A flag indicating whether a custom logger is in use.

    Methods:
        logger: Returns the active logger.
        set_logger: Sets a custom logger.
        configure_from_config: Configures the logger based on the provided configuration.

    """

    def __init__(self, custom_logger: LoggerInterface | None = None) -> None:
        """Initialize the LoggerManager.

        Args:
            custom_logger (LoggerInterface | None): A custom logger to use. Defaults to None.

        """
        self.formatter = Formatter()
        default_logger.remove()
        default_logger.add(
            sys.stdout,
            level="INFO",
            format=self.formatter.format,
            serialize=False,
        )
        self._logger: LoggerInterface = custom_logger or default_logger  # type: ignore  # noqa: PGH003
        self.custom_logger = False

    @property
    def logger(self) -> LoggerInterface:
        """Get the current logger.

        Returns:
            LoggerInterface: The active logger being used.

        """
        return self._logger

    def set_logger(self, custom_logger: LoggerInterface) -> None:
        """Set a custom logger.

        Args:
            custom_logger (LoggerInterface): The custom logger to be used.

        """
        self._logger = custom_logger
        self.custom_logger = True

    def configure_from_config(self, config: Config) -> None:
        """Configure the global logger based on the library's configuration.

        This method configures the logger's log level and output sink based on the application's
        settings. If a custom logger is in use, it will skip the configuration and log a warning.

        Args:
            config (Config): The configuration object containing the logging settings.

        """
        if self.custom_logger:
            # Log a warning if a custom logger is being used
            self._logger.warning("Custom logger is in use; skipping log level configuration.")
        else:
            default_logger.remove()
            log_sink = config.default_log_sink
            match config.default_log_sink:
                case "sys.stdout":
                    log_sink = sys.stdout
                case "sys.stderr":
                    log_sink = sys.stderr

            default_logger.add(
                log_sink,
                level=config.default_log_level.value,
                format=self.formatter.format,
                serialize=config.json_log_serialize,
            )


# Expose manager to allow updating logger
logger_manager = LoggerManager()


def logger() -> LoggerInterface:
    """Return the active logger.

    Returns:
        LoggerInterface: The current active logger being used.

    """
    return logger_manager.logger

```

## File: portia/clarification_handler.py

```python
"""Clarification Handler.

This module defines the base ClarificationHandler interface that determines how to handle
clarifications that arise during the run of a plan. It also provides a
CLIClarificationHandler implementation that handles clarifications via the CLI.
"""

from __future__ import annotations

from abc import ABC
from typing import Callable

from portia.clarification import (
    ActionClarification,
    Clarification,
    CustomClarification,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)


class ClarificationHandler(ABC):  # noqa: B024
    """Handles clarifications that arise during the execution of a plan run."""

    def handle(
        self,
        clarification: Clarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a clarification by routing it to the appropriate handler.

        Args:
            clarification: The clarification object to handle
            on_resolution: Callback function that should be invoked once the clarification has been
                handled, prompting the plan run to resume. This can either be called synchronously
                in this function or called async after returning from this function.
            on_error: Callback function that should be invoked if the clarification handling has
                failed. This can either be called synchronously in this function or called async
                after returning from this function.

        """
        match clarification:
            case ActionClarification():
                return self.handle_action_clarification(
                    clarification,
                    on_resolution,
                    on_error,
                )
            case InputClarification():
                return self.handle_input_clarification(
                    clarification,
                    on_resolution,
                    on_error,
                )
            case MultipleChoiceClarification():
                return self.handle_multiple_choice_clarification(
                    clarification,
                    on_resolution,
                    on_error,
                )
            case ValueConfirmationClarification():
                return self.handle_value_confirmation_clarification(
                    clarification,
                    on_resolution,
                    on_error,
                )
            case CustomClarification():
                return self.handle_custom_clarification(
                    clarification,
                    on_resolution,
                    on_error,
                )
            case _:
                raise ValueError(
                    f"Attempted to handle an unknown clarification type: {type(clarification)}",
                )

    def handle_action_clarification(
        self,
        clarification: ActionClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle an action clarification."""
        raise NotImplementedError("handle_action_clarification is not implemented")

    def handle_input_clarification(
        self,
        clarification: InputClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a user input clarification."""
        raise NotImplementedError("handle_input_clarification is not implemented")

    def handle_multiple_choice_clarification(
        self,
        clarification: MultipleChoiceClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a multi-choice clarification."""
        raise NotImplementedError("handle_multiple_choice_clarification is not implemented")

    def handle_value_confirmation_clarification(
        self,
        clarification: ValueConfirmationClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a value confirmation clarification."""
        raise NotImplementedError("handle_value_confirmation_clarification is not implemented")

    def handle_custom_clarification(
        self,
        clarification: CustomClarification,
        on_resolution: Callable[[Clarification, object], None],
        on_error: Callable[[Clarification, object], None],
    ) -> None:
        """Handle a custom clarification."""
        raise NotImplementedError("handle_custom_clarification is not implemented")

```

## File: portia/plan_run.py

```python
"""Plan runs are executing instances of a Plan.

A plan run encapsulates all execution state, serving as the definitive record of its progress.
As the run runs, its `PlanRunState`, `current_step_index`, and `outputs` evolve to reflect
the current execution state.

The run also retains an `ExecutionContext`, which provides valuable insights for debugging
and analytics, capturing contextual information relevant to the run's execution.

Key Components
--------------
- **RunState**: Tracks the current status of the run (e.g., NOT_STARTED, IN_PROGRESS).
- **current_step_index**: Represents the step within the plan currently being executed.
- **outputs**: Stores the intermediate and final results of the PlanRun.
- **ExecutionContext**: Provides contextual metadata useful for logging and performance analysis.
"""

from __future__ import annotations

from pydantic import BaseModel, ConfigDict, Field

from portia.clarification import (
    ClarificationListType,
)
from portia.common import PortiaEnum
from portia.execution_agents.output import Output
from portia.execution_context import ExecutionContext, empty_context
from portia.prefixed_uuid import PlanRunUUID, PlanUUID


class PlanRunState(PortiaEnum):
    """The current state of the Plan Run.

    Attributes:
        NOT_STARTED: The run has not been started yet.
        IN_PROGRESS: The run is currently in progress.
        NEED_CLARIFICATION: The run requires further clarification before proceeding.
        READY_TO_RESUME: The run is ready to resume after clarifications have been resolved.
        COMPLETE: The run has been successfully completed.
        FAILED: The run has encountered an error and failed.

    """

    NOT_STARTED = "NOT_STARTED"
    IN_PROGRESS = "IN_PROGRESS"
    NEED_CLARIFICATION = "NEED_CLARIFICATION"
    READY_TO_RESUME = "READY_TO_RESUME"
    COMPLETE = "COMPLETE"
    FAILED = "FAILED"


class PlanRunOutputs(BaseModel):
    """Outputs of a Plan Run including clarifications.

    Attributes:
        clarifications (ClarificationListType): Clarifications raised by this plan run.
        step_outputs (dict[str, Output]): A dictionary containing outputs of individual steps.
            Outputs are indexed by the value given by the `step.output` field of the plan.
        final_output (Output | None): The final consolidated output of the PlanRun if available.

    """

    model_config = ConfigDict(extra="forbid")

    clarifications: ClarificationListType = Field(
        default=[],
        description="Clarifications raised by this plan_run.",
    )

    step_outputs: dict[str, Output] = Field(
        default={},
        description="A dictionary containing outputs of individual run steps.",
    )

    final_output: Output | None = Field(
        default=None,
        description="The final consolidated output of the PlanRun if available.",
    )


class PlanRun(BaseModel):
    """A plan run represents a running instance of a Plan.

    Attributes:
        id (PlanRunUUID): A unique ID for this plan_run.
        plan_id (PlanUUID): The ID of the Plan this run uses.
        current_step_index (int): The current step that is being executed.
        state (PlanRunState): The current state of the PlanRun.
        execution_context (ExecutionContext): Execution context for the PlanRun.
        outputs (PlanRunOutputs): Outputs of the PlanRun including clarifications.

    """

    model_config = ConfigDict(extra="forbid")

    id: PlanRunUUID = Field(
        default_factory=PlanRunUUID,
        description="A unique ID for this plan_run.",
    )
    plan_id: PlanUUID = Field(
        description="The ID of the Plan this run uses.",
    )
    current_step_index: int = Field(
        default=0,
        description="The current step that is being executed",
    )
    state: PlanRunState = Field(
        default=PlanRunState.NOT_STARTED,
        description="The current state of the PlanRun.",
    )
    execution_context: ExecutionContext = Field(
        default=empty_context(),
        description="Execution Context for the PlanRun.",
    )
    outputs: PlanRunOutputs = Field(
        default=PlanRunOutputs(),
        description="Outputs of the run including clarifications.",
    )

    def get_outstanding_clarifications(self) -> ClarificationListType:
        """Return all outstanding clarifications.

        Returns:
            ClarificationListType: A list of outstanding clarifications that have not been resolved.

        """
        return [
            clarification
            for clarification in self.outputs.clarifications
            if not clarification.resolved
        ]

    def get_clarifications_for_step(self, step: int | None = None) -> ClarificationListType:
        """Return clarifications for the given step.

        Args:
            step (int | None): the step to get clarifications for. Defaults to current step.

        Returns:
            ClarificationListType: A list of clarifications for the given step.

        """
        if step is None:
            step = self.current_step_index
        return [
            clarification
            for clarification in self.outputs.clarifications
            if clarification.step == step
        ]

    def __str__(self) -> str:
        """Return the string representation of the PlanRun.

        Returns:
            str: A string representation containing key run attributes.

        """
        return (
            f"Run(id={self.id}, plan_id={self.plan_id}, "
            f"state={self.state}, current_step_index={self.current_step_index}, "
            f"final_output={'set' if self.outputs.final_output else 'unset'})"
        )


class ReadOnlyPlanRun(PlanRun):
    """A read-only copy of a Plan Run passed to agents for reference.

    This class provides a non-modifiable view of a plan run instance,
    ensuring that agents can access run details without altering them.
    """

    model_config = ConfigDict(frozen=True, extra="forbid")

    @classmethod
    def from_plan_run(cls, plan_run: PlanRun) -> ReadOnlyPlanRun:
        """Create a read-only plan run from a normal PlanRun.

        Args:
            plan_run (PlanRun): The original run instance to create a read-only copy from.

        Returns:
            ReadOnlyPlanRun: A new read-only instance of the provided PlanRun.

        """
        return cls(
            id=plan_run.id,
            plan_id=plan_run.plan_id,
            current_step_index=plan_run.current_step_index,
            outputs=plan_run.outputs,
            state=plan_run.state,
            execution_context=plan_run.execution_context,
        )

```

## File: portia/config.py

```python
"""Configuration module for the SDK.

This module defines the configuration classes and enumerations used in the SDK,
including settings for storage, API keys, LLM providers, logging, and agent options.
It also provides validation for configuration values and loading mechanisms for
default settings.
"""

from __future__ import annotations

import os
from enum import Enum
from typing import NamedTuple, Self, TypeVar

import tiktoken
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    SecretStr,
    field_validator,
    model_validator,
)

from portia.common import validate_extras_dependencies
from portia.errors import ConfigNotFoundError, InvalidConfigError
from portia.model import (
    AnthropicGenerativeModel,
    AzureOpenAIGenerativeModel,
    GenerativeModel,
    LangChainGenerativeModel,
    OpenAIGenerativeModel,
)

T = TypeVar("T")


class StorageClass(Enum):
    """Enum representing locations plans and runs are stored.

    Attributes:
        MEMORY: Stored in memory.
        DISK: Stored on disk.
        CLOUD: Stored in the cloud.

    """

    MEMORY = "MEMORY"
    DISK = "DISK"
    CLOUD = "CLOUD"


class LLMProvider(Enum):
    """Enum for supported LLM providers.

    Attributes:
        OPENAI: OpenAI provider.
        ANTHROPIC: Anthropic provider.
        MISTRALAI: MistralAI provider.
        GOOGLE_GENERATIVE_AI: Google Generative AI provider.
        AZURE_OPENAI: Azure OpenAI provider.

    """

    OPENAI = "OPENAI"
    ANTHROPIC = "ANTHROPIC"
    MISTRALAI = "MISTRALAI"
    GOOGLE_GENERATIVE_AI = "GOOGLE_GENERATIVE_AI"
    AZURE_OPENAI = "AZURE_OPENAI"

    def to_api_key_name(self) -> str:
        """Get the name of the API key for the provider."""
        match self:
            case LLMProvider.OPENAI:
                return "openai_api_key"
            case LLMProvider.ANTHROPIC:
                return "anthropic_api_key"
            case LLMProvider.MISTRALAI:
                return "mistralai_api_key"
            case LLMProvider.GOOGLE_GENERATIVE_AI:
                return "google_api_key"
            case LLMProvider.AZURE_OPENAI:
                return "azure_openai_api_key"


class Model(NamedTuple):
    """Provider and model name tuple.

    Attributes:
        provider: The provider of the model.
        model_name: The name of the model in the provider's API.

    """

    provider: LLMProvider
    model_name: str


class LLMModel(Enum):
    """Enum for supported LLM models.

    Models are grouped by provider, with the following providers:
    - OpenAI
    - Anthropic
    - MistralAI
    - Google Generative AI
    - Azure OpenAI

    Attributes:
        GPT_4_O: GPT-4 model by OpenAI.
        GPT_4_O_MINI: Mini GPT-4 model by OpenAI.
        GPT_3_5_TURBO: GPT-3.5 Turbo model by OpenAI.
        CLAUDE_3_5_SONNET: Claude 3.5 Sonnet model by Anthropic.
        CLAUDE_3_5_HAIKU: Claude 3.5 Haiku model by Anthropic.
        CLAUDE_3_OPUS: Claude 3.0 Opus model by Anthropic.
        CLAUDE_3_7_SONNET: Claude 3.7 Sonnet model by Anthropic.
        MISTRAL_LARGE: Mistral Large Latest model by MistralAI.
        GEMINI_2_0_FLASH: Gemini 2.0 Flash model by Google Generative AI.
        GEMINI_2_0_FLASH_LITE: Gemini 2.0 Flash Lite model by Google Generative AI.
        GEMINI_1_5_FLASH: Gemini 1.5 Flash model by Google Generative AI.
        AZURE_GPT_4_O: GPT-4 model by Azure OpenAI.
        AZURE_GPT_4_O_MINI: Mini GPT-4 model by Azure OpenAI.
        AZURE_O_3_MINI: O3 Mini model by Azure OpenAI.

    Can be instantiated from a string with the following format:
        - provider/model_name  [e.g. LLMModel("openai/gpt-4o")]
        - model_name           [e.g. LLMModel("gpt-4o")]

    In the cases where the model name is not unique across providers, the earlier values in the enum
    definition will take precedence.

    """

    @classmethod
    def _missing_(cls, value: object) -> LLMModel:
        """Get the LLM model from the model name."""
        if isinstance(value, str):
            for member in cls:
                if member.api_name == value:
                    return member
                if "/" in value:
                    provider, model_name = value.split("/")
                    if (
                        member.provider().value.lower() == provider.lower()
                        and member.api_name == model_name
                    ):
                        return member
        raise ValueError(f"Invalid LLM model: {value}")

    # OpenAI
    GPT_4_O = Model(provider=LLMProvider.OPENAI, model_name="gpt-4o")
    GPT_4_O_MINI = Model(provider=LLMProvider.OPENAI, model_name="gpt-4o-mini")
    GPT_3_5_TURBO = Model(provider=LLMProvider.OPENAI, model_name="gpt-3.5-turbo")
    O_3_MINI = Model(provider=LLMProvider.OPENAI, model_name="o3-mini")

    # Anthropic
    CLAUDE_3_5_SONNET = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-5-sonnet-latest")
    CLAUDE_3_5_HAIKU = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-5-haiku-latest")
    CLAUDE_3_OPUS = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-opus-latest")
    CLAUDE_3_7_SONNET = Model(provider=LLMProvider.ANTHROPIC, model_name="claude-3-7-sonnet-latest")

    # MistralAI
    MISTRAL_LARGE = Model(provider=LLMProvider.MISTRALAI, model_name="mistral-large-latest")

    # Google Generative AI
    GEMINI_2_0_FLASH = Model(
        provider=LLMProvider.GOOGLE_GENERATIVE_AI,
        model_name="gemini-2.0-flash",
    )
    GEMINI_2_0_FLASH_LITE = Model(
        provider=LLMProvider.GOOGLE_GENERATIVE_AI,
        model_name="gemini-2.0-flash-lite",
    )
    GEMINI_1_5_FLASH = Model(
        provider=LLMProvider.GOOGLE_GENERATIVE_AI,
        model_name="gemini-1.5-flash",
    )

    # Azure OpenAI
    AZURE_GPT_4_O = Model(provider=LLMProvider.AZURE_OPENAI, model_name="gpt-4o")
    AZURE_GPT_4_O_MINI = Model(provider=LLMProvider.AZURE_OPENAI, model_name="gpt-4o-mini")
    AZURE_O_3_MINI = Model(provider=LLMProvider.AZURE_OPENAI, model_name="o3-mini")

    @property
    def api_name(self) -> str:
        """Override the default value to return the model name."""
        return self.value.model_name

    def provider(self) -> LLMProvider:
        """Get the associated provider for the model.

        Returns:
            LLMProvider: The provider associated with the model.

        """
        return self.value.provider


SUPPORTED_OPENAI_MODELS = [
    LLMModel.GPT_4_O,
    LLMModel.GPT_4_O_MINI,
    LLMModel.GPT_3_5_TURBO,
    LLMModel.O_3_MINI,
]

SUPPORTED_ANTHROPIC_MODELS = [
    LLMModel.CLAUDE_3_5_HAIKU,
    LLMModel.CLAUDE_3_5_SONNET,
    LLMModel.CLAUDE_3_7_SONNET,
    LLMModel.CLAUDE_3_OPUS,
]

SUPPORTED_MISTRALAI_MODELS = [
    LLMModel.MISTRAL_LARGE,
]

SUPPORTED_GOOGLE_GENERATIVE_AI_MODELS = [
    LLMModel.GEMINI_2_0_FLASH,
    LLMModel.GEMINI_2_0_FLASH_LITE,
    LLMModel.GEMINI_1_5_FLASH,
]

SUPPORTED_AZURE_OPENAI_MODELS = [
    LLMModel.AZURE_GPT_4_O,
    LLMModel.AZURE_GPT_4_O_MINI,
    LLMModel.AZURE_O_3_MINI,
]


class ExecutionAgentType(Enum):
    """Enum for types of agents used for executing a step.

    Attributes:
        ONE_SHOT: The one-shot agent.
        DEFAULT: The default agent.

    """

    ONE_SHOT = "ONE_SHOT"
    DEFAULT = "DEFAULT"


class PlanningAgentType(Enum):
    """Enum for planning agents used for planning queries.

    Attributes:
        DEFAULT: The default planning agent.

    """

    DEFAULT = "DEFAULT"


class LogLevel(Enum):
    """Enum for available log levels.

    Attributes:
        DEBUG: Debug log level.
        INFO: Info log level.
        WARNING: Warning log level.
        ERROR: Error log level.
        CRITICAL: Critical log level.

    """

    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


PLANNING_MODEL_KEY = "planning_model_name"
EXECUTION_MODEL_KEY = "execution_model_name"
INTROSPECTION_MODEL_KEY = "introspection_model_name"
SUMMARISER_MODEL_KEY = "summariser_model_name"
DEFAULT_MODEL_KEY = "default_model_name"
PLANNING_DEFAULT_MODEL_KEY = "planning_default_model_name"

FEATURE_FLAG_AGENT_MEMORY_ENABLED = "feature_flag_agent_memory_enabled"


E = TypeVar("E", bound=Enum)


def parse_str_to_enum(value: str | E, enum_type: type[E]) -> E:
    """Parse a string to an enum or return the enum as is.

    Args:
        value (str | E): The value to parse.
        enum_type (type[E]): The enum type to parse the value into.

    Raises:
        InvalidConfigError: If the value cannot be parsed into the enum.

    Returns:
        E: The corresponding enum value.

    """
    if isinstance(value, str):
        try:
            return enum_type[value.upper()]
        except KeyError as e:
            raise InvalidConfigError(
                value=value,
                issue=f"Invalid value for enum {enum_type.__name__}",
            ) from e
    if isinstance(value, enum_type):
        return value

    raise InvalidConfigError(
        value=str(value),
        issue=f"Value must be a string or {enum_type.__name__}",
    )


PLANNER_DEFAULT_MODELS = {
    LLMProvider.OPENAI: LLMModel.O_3_MINI,
    LLMProvider.ANTHROPIC: LLMModel.CLAUDE_3_5_SONNET,
    LLMProvider.MISTRALAI: LLMModel.MISTRAL_LARGE,
    LLMProvider.GOOGLE_GENERATIVE_AI: LLMModel.GEMINI_2_0_FLASH,
    LLMProvider.AZURE_OPENAI: LLMModel.AZURE_O_3_MINI,
}

DEFAULT_MODELS = {
    LLMProvider.OPENAI: LLMModel.GPT_4_O,
    LLMProvider.ANTHROPIC: LLMModel.CLAUDE_3_5_SONNET,
    LLMProvider.MISTRALAI: LLMModel.MISTRAL_LARGE,
    LLMProvider.GOOGLE_GENERATIVE_AI: LLMModel.GEMINI_2_0_FLASH,
    LLMProvider.AZURE_OPENAI: LLMModel.AZURE_GPT_4_O,
}


class Config(BaseModel):
    """General configuration for the SDK.

    This class holds the configuration for the SDK, including API keys, LLM
    settings, logging options, and storage settings. It also provides validation
    for configuration consistency and offers methods for loading configuration
    from files or default values.

    Attributes:
        portia_api_endpoint: The endpoint for the Portia API.
        portia_api_key: The API key for Portia.
        openai_api_key: The API key for OpenAI.
        anthropic_api_key: The API key for Anthropic.
        mistralai_api_key: The API key for MistralAI.
        google_api_key: The API key for Google Generative AI.
        azure_openai_api_key: The API key for Azure OpenAI.
        azure_openai_endpoint: The endpoint for Azure OpenAI.
        llm_provider: The LLM provider.
        models: A dictionary of LLM models for each usage type.
        storage_class: The storage class used (e.g., MEMORY, DISK, CLOUD).
        storage_dir: The directory for storage, if applicable.
        default_log_level: The default log level (e.g., DEBUG, INFO).
        default_log_sink: The default destination for logs (e.g., sys.stdout).
        json_log_serialize: Whether to serialize logs in JSON format.
        planning_agent_type: The planning agent type.
        execution_agent_type: The execution agent type.

    """

    model_config = ConfigDict(extra="ignore", arbitrary_types_allowed=True)

    # Portia Cloud Options
    portia_api_endpoint: str = Field(
        default_factory=lambda: os.getenv("PORTIA_API_ENDPOINT") or "https://api.portialabs.ai",
        description="The API endpoint for the Portia Cloud API",
    )
    portia_dashboard_url: str = Field(
        default_factory=lambda: os.getenv("PORTIA_DASHBOARD_URL") or "https://app.portialabs.ai",
        description="The URL for the Portia Cloud Dashboard",
    )
    portia_api_key: SecretStr | None = Field(
        default_factory=lambda: (
            SecretStr(os.environ["PORTIA_API_KEY"]) if "PORTIA_API_KEY" in os.environ else None
        ),
        description="The API Key for the Portia Cloud API available from the dashboard at https://app.portialabs.ai",
    )

    # LLM API Keys
    openai_api_key: SecretStr = Field(
        default_factory=lambda: SecretStr(os.getenv("OPENAI_API_KEY") or ""),
        description="The API Key for OpenAI. Must be set if llm-provider is OPENAI",
    )
    anthropic_api_key: SecretStr = Field(
        default_factory=lambda: SecretStr(os.getenv("ANTHROPIC_API_KEY") or ""),
        description="The API Key for Anthropic. Must be set if llm-provider is ANTHROPIC",
    )
    mistralai_api_key: SecretStr = Field(
        default_factory=lambda: SecretStr(os.getenv("MISTRAL_API_KEY") or ""),
        description="The API Key for Mistral AI. Must be set if llm-provider is MISTRALAI",
    )
    google_api_key: SecretStr = Field(
        default_factory=lambda: SecretStr(os.getenv("GOOGLE_API_KEY") or ""),
        description="The API Key for Google Generative AI. Must be set if llm-provider is "
        "GOOGLE_GENERATIVE_AI",
    )
    azure_openai_api_key: SecretStr = Field(
        default_factory=lambda: SecretStr(os.getenv("AZURE_OPENAI_API_KEY") or ""),
        description="The API Key for Azure OpenAI. Must be set if llm-provider is AZURE_OPENAI",
    )
    azure_openai_endpoint: str = Field(
        default_factory=lambda: os.getenv("AZURE_OPENAI_ENDPOINT") or "",
        description="The endpoint for Azure OpenAI. Must be set if llm-provider is AZURE_OPENAI",
    )

    llm_provider: LLMProvider = Field(
        default=LLMProvider.OPENAI,
        description="Which LLM Provider to use.",
    )

    models: dict[str, LLMModel] = Field(
        default_factory=dict,
        description="A dictionary of configured LLM models for each usage.",
    )

    custom_models: dict[str, GenerativeModel] = Field(
        default_factory=dict,
        description="A dictionary of custom GenerativeModel instances for each usage.",
    )

    feature_flags: dict[str, bool] = Field(
        default={},
        description="A dictionary of feature flags for the SDK.",
    )

    @model_validator(mode="after")
    def parse_feature_flags(self) -> Self:
        """Add feature flags if not provided."""
        self.feature_flags = {
            # Fill here with any default feature flags.
            # e.g. CONDITIONAL_FLAG: True,
            FEATURE_FLAG_AGENT_MEMORY_ENABLED: False,
            **self.feature_flags,
        }
        return self

    @model_validator(mode="after")
    def add_default_models(self) -> Self:
        """Add default models if not provided."""
        self.models = {
            PLANNING_DEFAULT_MODEL_KEY: PLANNER_DEFAULT_MODELS[self.llm_provider],
            DEFAULT_MODEL_KEY: DEFAULT_MODELS[self.llm_provider],
            **self.models,
        }
        return self

    def model(self, usage: str) -> LLMModel:
        """Get the LLM model for the given usage."""
        if usage == PLANNING_MODEL_KEY:
            return self.models.get(PLANNING_MODEL_KEY, self.models[PLANNING_DEFAULT_MODEL_KEY])
        return self.models.get(usage, self.models[DEFAULT_MODEL_KEY])

    def resolve_model(self, usage: str = DEFAULT_MODEL_KEY) -> GenerativeModel:
        """Resolve a model from the config."""
        if usage in self.custom_models:
            return self.custom_models[usage]
        model = self.model(usage)
        return self._construct_model(model)

    def resolve_langchain_model(self, usage: str = DEFAULT_MODEL_KEY) -> LangChainGenerativeModel:
        """Resolve a LangChain model from the config."""
        model = self.resolve_model(usage)
        if isinstance(model, LangChainGenerativeModel):
            return model
        raise TypeError(
            f"A LangChainGenerativeModel is required, but the config for "
            f"{usage} resolved to {model}.",
        )

    def _construct_model(self, llm_model: LLMModel) -> GenerativeModel:
        """Construct a Model instance from an LLMModel."""
        match llm_model.provider():
            case LLMProvider.OPENAI:
                return OpenAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=self.openai_api_key,
                )
            case LLMProvider.ANTHROPIC:
                return AnthropicGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=self.anthropic_api_key,
                )
            case LLMProvider.MISTRALAI:
                validate_extras_dependencies("mistral")
                from portia.model import MistralAIGenerativeModel

                return MistralAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=self.mistralai_api_key,
                )
            case LLMProvider.GOOGLE_GENERATIVE_AI:
                validate_extras_dependencies("google")
                from portia.model import GoogleGenAiGenerativeModel

                return GoogleGenAiGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=self.google_api_key,
                )
            case LLMProvider.AZURE_OPENAI:
                return AzureOpenAIGenerativeModel(
                    model_name=llm_model.api_name,
                    api_key=self.azure_openai_api_key,
                    azure_endpoint=self.azure_openai_endpoint,
                )

    # Storage Options
    storage_class: StorageClass = Field(
        default_factory=lambda: StorageClass.CLOUD
        if os.getenv("PORTIA_API_KEY")
        else StorageClass.MEMORY,
        description="Where to store Plans and PlanRuns. By default these will be kept in memory"
        "if no API key is provided.",
    )

    @field_validator("storage_class", mode="before")
    @classmethod
    def parse_storage_class(cls, value: str | StorageClass) -> StorageClass:
        """Parse storage class to enum if string provided."""
        return parse_str_to_enum(value, StorageClass)

    storage_dir: str | None = Field(
        default=None,
        description="If storage class is set to DISK this will be the location where plans "
        "and runs are written in a JSON format.",
    )

    # Logging Options

    # default_log_level controls the minimal log level, i.e. setting to DEBUG will print all logs
    # where as setting it to ERROR will only display ERROR and above.
    default_log_level: LogLevel = Field(
        default=LogLevel.INFO,
        description="The log level to log at. Only respected when the default logger is used.",
    )

    @field_validator("default_log_level", mode="before")
    @classmethod
    def parse_default_log_level(cls, value: str | LogLevel) -> LogLevel:
        """Parse default_log_level to enum if string provided."""
        return parse_str_to_enum(value, LogLevel)

    # default_log_sink controls where default logs are sent. By default this is STDOUT (sys.stdout)
    # but can also be set to STDERR (sys.stderr)
    # or to a file by setting this to a file path ("./logs.txt")
    default_log_sink: str = Field(
        default="sys.stdout",
        description="Where to send logs. By default logs will be sent to sys.stdout",
    )
    # json_log_serialize sets whether logs are JSON serialized before sending to the log sink.
    json_log_serialize: bool = Field(
        default=False,
        description="Whether to serialize logs to JSON",
    )
    # Agent Options
    execution_agent_type: ExecutionAgentType = Field(
        default=ExecutionAgentType.DEFAULT,
        description="The default agent type to use.",
    )

    @field_validator("execution_agent_type", mode="before")
    @classmethod
    def parse_execution_agent_type(cls, value: str | ExecutionAgentType) -> ExecutionAgentType:
        """Parse execution_agent_type to enum if string provided."""
        return parse_str_to_enum(value, ExecutionAgentType)

    # PlanningAgent Options
    planning_agent_type: PlanningAgentType = Field(
        default=PlanningAgentType.DEFAULT,
        description="The default planning_agent_type to use.",
    )

    @field_validator("planning_agent_type", mode="before")
    @classmethod
    def parse_planning_agent_type(cls, value: str | PlanningAgentType) -> PlanningAgentType:
        """Parse planning_agent_type to enum if string provided."""
        return parse_str_to_enum(value, PlanningAgentType)

    large_output_threshold_tokens: int = Field(
        default=1_000,
        description="The threshold number of tokens before we start treating an output as a"
        "large output and write it to agent memory rather than storing it locally",
    )

    def exceeds_output_threshold(self, value: str | list[str | dict]) -> bool:
        """Determine whether the provided output value exceeds the large output threshold."""
        if not self.feature_flags.get(FEATURE_FLAG_AGENT_MEMORY_ENABLED):
            return False
        # It doesn't really matter which model we use here, so choose gpt2 for speed.
        # More details at https://chatgpt.com/share/67ee4931-a794-8007-9859-13aca611dba9
        encoding = tiktoken.get_encoding("gpt2").encode(str(value))
        return len(encoding) > self.large_output_threshold_tokens

    @model_validator(mode="after")
    def check_config(self) -> Self:
        """Validate Config is consistent."""
        # Portia API Key must be provided if using cloud storage
        if self.storage_class == StorageClass.CLOUD and not self.has_api_key("portia_api_key"):
            raise InvalidConfigError(
                "portia_api_key",
                "A Portia API key must be provided if using cloud storage. Follow the steps at "
                "https://docs.portialabs.ai/setup-account to obtain one if you don't already "
                "have one",
            )
        if self.storage_class == StorageClass.DISK and not self.storage_dir:
            raise InvalidConfigError(
                "storage_dir",
                "A storage directory must be provided if using disk storage",
            )

        def validate_llm_api_key(provider: LLMProvider) -> None:
            """Validate LLM Config."""
            if not self.has_api_key(provider.to_api_key_name()):
                raise InvalidConfigError(
                    f"{provider.to_api_key_name()}",
                    f"Must be provided if using {provider}",
                )

        validate_llm_api_key(self.llm_provider)
        for model in self.models.values():
            if isinstance(model, LLMModel):
                validate_llm_api_key(model.provider())
        return self

    @classmethod
    def from_default(cls, **kwargs) -> Config:  # noqa: ANN003
        """Create a Config instance with default values, allowing overrides.

        Returns:
            Config: The default config

        """
        return default_config(**kwargs)

    def has_api_key(self, name: str) -> bool:
        """Check if the given API Key is available."""
        try:
            self.must_get_api_key(name)
        except InvalidConfigError:
            return False
        else:
            return True

    def must_get_api_key(self, name: str) -> SecretStr:
        """Retrieve the required API key for the configured provider.

        Raises:
            ConfigNotFoundError: If no API key is found for the provider.

        Returns:
            SecretStr: The required API key.

        """
        return self.must_get(name, SecretStr)

    def must_get(self, name: str, expected_type: type[T]) -> T:
        """Retrieve any value from the config, ensuring its of the correct type.

        Args:
            name (str): The name of the config record.
            expected_type (type[T]): The expected type of the value.

        Raises:
            ConfigNotFoundError: If no API key is found for the provider.
            InvalidConfigError: If the config isn't valid

        Returns:
            T: The config value

        """
        if not hasattr(self, name):
            raise ConfigNotFoundError(name)
        value = getattr(self, name)
        if not isinstance(value, expected_type):
            raise InvalidConfigError(name, f"Not of expected type: {expected_type}")
        # ensure non-empty values
        match value:
            case str() if value == "":
                raise InvalidConfigError(name, "Empty value not allowed")
            case SecretStr() if value.get_secret_value() == "":
                raise InvalidConfigError(name, "Empty SecretStr value not allowed")
        return value


def llm_provider_default_from_api_keys(**kwargs) -> LLMProvider:  # noqa: ANN003
    """Get the default LLM provider from the API keys."""
    if os.getenv("OPENAI_API_KEY") or kwargs.get("openai_api_key"):
        return LLMProvider.OPENAI
    if os.getenv("ANTHROPIC_API_KEY") or kwargs.get("anthropic_api_key"):
        return LLMProvider.ANTHROPIC
    if os.getenv("MISTRAL_API_KEY") or kwargs.get("mistralai_api_key"):
        return LLMProvider.MISTRALAI
    if os.getenv("GOOGLE_API_KEY") or kwargs.get("google_api_key"):
        return LLMProvider.GOOGLE_GENERATIVE_AI
    if (os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")) or (
        kwargs.get("azure_openai_api_key") and kwargs.get("azure_openai_endpoint")
    ):
        return LLMProvider.AZURE_OPENAI
    raise InvalidConfigError(LLMProvider.OPENAI.to_api_key_name(), "No LLM API key found")


def default_config(**kwargs) -> Config:  # noqa: ANN003
    """Return default config with values that can be overridden.

    Returns:
        Config: The default config

    """
    llm_model_name = kwargs.pop("llm_model_name", None)
    models = kwargs.pop("models", {})
    for model_usage in [
        PLANNING_MODEL_KEY,
        INTROSPECTION_MODEL_KEY,
        EXECUTION_MODEL_KEY,
        SUMMARISER_MODEL_KEY,
    ]:
        model_name = kwargs.pop(model_usage, llm_model_name)
        if model_name and model_name not in models:
            models[model_usage] = parse_str_to_enum(model_name, LLMModel)

    llm_provider = parse_str_to_enum(
        kwargs.pop("llm_provider", llm_provider_default_from_api_keys(**kwargs)),
        LLMProvider,
    )

    default_storage_class = (
        StorageClass.CLOUD if os.getenv("PORTIA_API_KEY") else StorageClass.MEMORY
    )
    return Config(
        llm_provider=llm_provider,
        models=models,
        feature_flags=kwargs.pop("feature_flags", {}),
        storage_class=kwargs.pop("storage_class", default_storage_class),
        planning_agent_type=kwargs.pop("planning_agent_type", PlanningAgentType.DEFAULT),
        execution_agent_type=kwargs.pop("execution_agent_type", ExecutionAgentType.DEFAULT),
        **kwargs,
    )

```

## File: portia/mcp_session.py

```python
"""Configuration and client code for interactions with Model Context Protocol (MCP) servers.

This module provides a context manager for creating MCP ClientSessions, which are used to
interact with MCP servers. It supports both the SSE and stdio transports.

NB. The MCP Python SDK is asynchronous, so care must be taken when using MCP functionality
from this module in an async context.

Classes:
    SseMcpClientConfig: Configuration for an MCP client that connects via SSE.
    StdioMcpClientConfig: Configuration for an MCP client that connects via stdio.
    McpClientConfig: The configuration to connect to an MCP server.
"""

from __future__ import annotations

from contextlib import asynccontextmanager
from typing import TYPE_CHECKING, Any, Literal

from mcp import ClientSession, StdioServerParameters, stdio_client
from mcp.client.sse import sse_client
from pydantic import BaseModel, Field

if TYPE_CHECKING:
    from collections.abc import AsyncIterator


class SseMcpClientConfig(BaseModel):
    """Configuration for an MCP client that connects via SSE."""

    server_name: str
    url: str
    headers: dict[str, Any] | None = None
    timeout: float = 5
    sse_read_timeout: float = 60 * 5


class StdioMcpClientConfig(BaseModel):
    """Configuration for an MCP client that connects via stdio."""

    server_name: str
    command: str
    args: list[str] = Field(default_factory=list)
    env: dict[str, str] | None = None
    encoding: str = "utf-8"
    encoding_error_handler: Literal["strict", "ignore", "replace"] = "strict"


McpClientConfig = SseMcpClientConfig | StdioMcpClientConfig


@asynccontextmanager
async def get_mcp_session(mcp_client_config: McpClientConfig) -> AsyncIterator[ClientSession]:
    """Context manager for an MCP ClientSession.

    Args:
        mcp_client_config: The configuration to connect to an MCP server

    Returns:
        An MCP ClientSession

    """
    if isinstance(mcp_client_config, StdioMcpClientConfig):
        async with (
            stdio_client(
                StdioServerParameters(
                    command=mcp_client_config.command,
                    args=mcp_client_config.args,
                    env=mcp_client_config.env,
                    encoding=mcp_client_config.encoding,
                    encoding_error_handler=mcp_client_config.encoding_error_handler,
                ),
            ) as stdio_transport,
            ClientSession(*stdio_transport) as session,
        ):
            await session.initialize()
            yield session
    elif isinstance(mcp_client_config, SseMcpClientConfig):
        async with (
            sse_client(
                url=mcp_client_config.url,
                headers=mcp_client_config.headers,
                timeout=mcp_client_config.timeout,
                sse_read_timeout=mcp_client_config.sse_read_timeout,
            ) as sse_transport,
            ClientSession(*sse_transport) as session,
        ):
            await session.initialize()
            yield session

```

## File: portia/execution_agents/default_execution_agent.py

```python
"""The Default execution agent for hardest problems.

This agent uses multiple models (verifier, parser etc) to achieve the highest accuracy
in completing tasks.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Literal

from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel, ConfigDict, Field, ValidationError

from portia.clarification import Clarification, InputClarification
from portia.config import EXECUTION_MODEL_KEY
from portia.errors import InvalidAgentError, InvalidPlanRunStateError
from portia.execution_agents.base_execution_agent import BaseExecutionAgent
from portia.execution_agents.execution_utils import (
    MAX_RETRIES,
    AgentNode,
    next_state_after_tool_call,
    process_output,
    tool_call_or_end,
)
from portia.execution_agents.utils.step_summarizer import StepSummarizer
from portia.execution_context import get_execution_context
from portia.model import GenerativeModel, LangChainGenerativeModel, Message
from portia.tool import ToolRunContext

if TYPE_CHECKING:
    from langchain.tools import StructuredTool

    from portia.config import Config
    from portia.execution_agents.output import Output
    from portia.plan import Step
    from portia.plan_run import PlanRun
    from portia.tool import Tool


class ToolArgument(BaseModel):
    """Represents an argument for a tool as extracted from the goal and context.

    Attributes:
        name (str): The name of the argument, as requested by the tool.
        value (Any | None): The value of the argument, as provided in the goal or context.
        valid (bool): Whether the value is a valid type and/or format for the given argument.
        explanation (str): Explanation of the source for the value of the argument.

    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    name: str = Field(description="Name of the argument, as requested by the tool.")
    value: Any | None = Field(
        description="Value of the argument, as provided by in the goal or context.",
    )
    valid: bool = Field(
        description="Whether the value is a valid type and or format for the given argument.",
    )
    explanation: str = Field(description="Explanation of the source for the value of the argument.")


class ToolInputs(BaseModel):
    """Represents the inputs for a tool.

    Attributes:
        args (list[ToolArgument]): Arguments for the tool.

    """

    args: list[ToolArgument] = Field(description="Arguments for the tool.")


class VerifiedToolArgument(BaseModel):
    """Represents an argument for a tool after being verified by an agent.

    Attributes:
        name (str): The name of the argument, as requested by the tool.
        value (Any | None): The value of the argument, as provided in the goal or context.
        made_up (bool): Whether the value was made up or not. Should be false if the value was
        provided by the user.

    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    name: str = Field(description="Name of the argument, as requested by the tool.")
    value: Any | None = Field(
        description="Value of the argument, as provided by in the goal or context.",
    )

    # We call this "made_up" and not "hallucinated" because the latter was making OpenAI's model
    # produce invalid JSON.
    made_up: bool = Field(
        default=False,
        description="Whether the value was made up or not. "
        "Should be false if the value was provided by the user, even if in a different format."
        "User provided values can be in the context, in the goal or the result of previous steps.",
    )

    schema_invalid: bool = Field(
        default=False,
        description="Whether the pydantic schema is invalid or not for this arg.",
    )


class VerifiedToolInputs(BaseModel):
    """Represents the inputs for a tool after being verified by an agent.

    Attributes:
        args (list[VerifiedToolArgument]): Arguments for the tool.

    """

    args: list[VerifiedToolArgument] = Field(description="Arguments for the tool.")


class ParserModel:
    """Model to parse the arguments for a tool.

    Args:
        model (Model): The language model used for argument parsing.
        context (str): The context for argument generation.
        agent (DefaultExecutionAgent): The agent using the parser model.

    Attributes:
        arg_parser_prompt (ChatPromptTemplate): The prompt template for argument parsing.
        model (Model): The language model used.
        context (str): The context for argument generation.
        agent (DefaultExecutionAgent): The agent using the parser model.
        previous_errors (list[str]): A list of previous errors encountered during parsing.
        retries (int): The number of retries attempted for parsing.

    """

    arg_parser_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=(
                    "You are a highly capable assistant tasked with generating valid arguments for "
                    "tools based on provided input. "
                    "While you are not aware of current events, you excel at reasoning "
                    "and adhering to instructions. "
                    "Your responses must clearly explain the source of each argument "
                    "(e.g., context, past messages, clarifications). "
                    "Avoid assumptions or fabricated information."
                ),
            ),
            HumanMessagePromptTemplate.from_template(
                "Context for user input and past steps:\n{context}\n"
                "Task: {task}\n"
                "The system has a tool available named '{tool_name}'.\n"
                "Argument schema for the tool:\n{tool_args}\n"
                "Description of the tool: {tool_description}\n"
                "\n\n----------\n\n"
                "The following section contains previous errors. "
                "Ensure your response avoids these errors. "
                "The one exception to this is not providing a value for a required argument. "
                "If a value cannot be extracted from the context, you can leave it blank. "
                "Do not assume a default value that meets the type expectation or is a common testing value. "  # noqa: E501
                "Here are the previous errors:\n"
                "{previous_errors}\n"
                "\n\n----------\n\n"
                "Please provide the arguments for the tool. Adhere to the following guidelines:\n"
                "- If a tool needs to be called many times, you can repeat the argument\n"
                "- You may take values from the task, inputs, previous steps or clarifications\n"
                "- Prefer values clarified in follow-up inputs over initial inputs.\n"
                "- Do not provide placeholder values (e.g., 'example@example.com').\n"
                "- Ensure arguments align with the tool's schema and intended use.\n\n"
                "You must return the arguments in the following JSON format:\n"
                "class ToolInputs:\n"
                "  args: List[ToolArgument]  # List of tool arguments.\n\n"
                "class ToolArgument:\n"
                "  name: str  # Name of the argument requested by the tool.\n"
                "  value: Any | None  # Value of the argument from the goal or context.\n"
                "  valid: bool  # Whether the value is valid for the argument.\n"
                "  explanation: str  # Explanation of the source for the value of the argument.\n\n",  # noqa: E501
            ),
        ],
    )

    def __init__(self, model: GenerativeModel, context: str, agent: DefaultExecutionAgent) -> None:
        """Initialize the model.

        Args:
            model (Model): The language model used for argument parsing.
            context (str): The context for argument generation.
            agent (DefaultExecutionAgent): The agent using the parser model.

        """
        self.model = model
        self.context = context
        self.agent = agent
        self.previous_errors: list[str] = []
        self.retries = 0

    def invoke(self, state: MessagesState) -> dict[str, Any]:
        """Invoke the model with the given message state.

        Args:
            state (MessagesState): The current state of the conversation.

        Returns:
            dict[str, Any]: The response after invoking the model.

        Raises:
            InvalidRunStateError: If the agent's tool is not available.

        """
        if not self.agent.tool:
            raise InvalidPlanRunStateError("Parser model has no tool")

        formatted_messages = self.arg_parser_prompt.format_messages(
            context=self.context,
            task=self.agent.step.task,
            tool_name=self.agent.tool.name,
            tool_args=self.agent.tool.args_json_schema(),
            tool_description=self.agent.tool.description,
            previous_errors=",".join(self.previous_errors),
        )

        errors = []
        tool_inputs: ToolInputs | None = None
        try:
            response = self.model.get_structured_response(
                messages=[Message.from_langchain(m) for m in formatted_messages],
                schema=ToolInputs,
            )
            tool_inputs = ToolInputs.model_validate(response)
        except ValidationError as e:
            errors.append("Invalid JSON for ToolInputs: " + str(e) + "\n")
        else:
            test_args = {}
            for arg in tool_inputs.args:
                test_args[arg.name] = arg.value
                if not arg.valid:
                    errors.append(f"Error in argument {arg.name}: {arg.explanation}\n")

            # also test the ToolInputs that have come back
            # actually work for the schema of the tool
            # if not we can retry
            try:
                self.agent.tool.args_schema.model_validate(test_args)
            except ValidationError as e:
                errors.append(str(e) + "\n")

        if errors:
            self.previous_errors.extend(errors)
            self.retries += 1
            if self.retries <= MAX_RETRIES:
                return self.invoke(state)
            # Previously we would raise an error here, but this restricts the agent from
            # being able to raise clarifications for the tool arguments marked as invalid.
            # Missing tool arguments are often represented as None, which isn't a compatible
            # type for non-optional arguments.
            #
            # Here is a Linear ticket to fix this:
            # https://linear.app/portialabs/issue/POR-456

        return {"messages": [tool_inputs.model_dump_json(indent=2)] if tool_inputs else []}


class VerifierModel:
    """A model to verify the arguments for a tool.

    This model ensures that the arguments passed to a tool are valid, determining whether they are
    "made up" or not based on the context and specific rules. The verification process uses an LLM
    to analyze the context and tool arguments and returns a structured validation output.

    Attributes:
        arg_verifier_prompt (ChatPromptTemplate): The prompt template used for arg verification.
        model (Model): The model used to invoke the verification process.
        context (str): The context in which the tool arguments are being validated.
        agent (DefaultExecutionAgent): The agent responsible for handling the verification process.

    """

    arg_verifier_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="You are an expert reviewer. Your task is to validate and label arguments "
                "provided. You must return the made_up field based "
                "on the rules below.\n - An argument is made up if we cannot tell where the value "
                "came from in the goal or context.\n- You should verify that the explanations are "
                "grounded in the goal or context before trusting them."
                "\n- If an argument is marked as invalid it is likely wrong."
                "\n- We really care if the value of an argument is not in the context, a handled "
                "clarification or goal at all (then made_up should be TRUE), but it is ok if "
                "it is there but in a different format, or if it can be reasonably derived from the"
                " information that is there (then made_up should be FALSE). "
                "\n- Arguments where the value comes from a clarification should be marked as FALSE"
                "\nThe output must conform to the following schema:\n\n"
                "class VerifiedToolArgument:\n"
                "  name: str  # Name of the argument requested by the tool.\n"
                "  value: Any | None  # Value of the argument from the goal or context. "
                "USE EXACTLY the type of the argument provided in the list of arguments provided.\n"
                "  made_up: bool  # if the value is made_up based on the given rules.\n\n"
                "class VerifiedToolInputs:\n"
                "  args: List[VerifiedToolArgument]  # List of tool arguments.\n\n"
                "Please ensure the output matches the VerifiedToolInputs schema.",
            ),
            HumanMessagePromptTemplate.from_template(
                "You will need to achieve the following goal: {task}\n"
                "\n\n----------\n\n"
                "Context for user input and past steps:"
                "\n{context}\n"
                "The system has a tool available named '{tool_name}'.\n"
                "Argument schema for the tool:\n{tool_args}\n"
                "\n\n----------\n\n"
                "Label the following arguments as made up or not using the goal and context provided: {arguments}\n",  # noqa: E501
            ),
        ],
    )

    def __init__(self, model: GenerativeModel, context: str, agent: DefaultExecutionAgent) -> None:
        """Initialize the model.

        Args:
            model (Model): The model used for argument verification.
            context (str): The context for argument generation.
            agent (DefaultExecutionAgent): The agent using the verifier model.

        """
        self.model = model
        self.context = context
        self.agent = agent

    def invoke(self, state: MessagesState) -> dict[str, Any]:
        """Invoke the model with the given message state.

        Args:
            state (MessagesState): The current state of the conversation.

        Returns:
            dict[str, Any]: The response after invoking the model.

        Raises:
            InvalidRunStateError: If the agent's tool is not available.

        """
        if not self.agent.tool:
            raise InvalidPlanRunStateError("Verifier model has no tool")

        messages = state["messages"]
        tool_args = messages[-1].content
        formatted_messages = self.arg_verifier_prompt.format_messages(
            context=self.context,
            task=self.agent.step.task,
            arguments=tool_args,
            tool_name=self.agent.tool.name,
            tool_args=self.agent.tool.args_json_schema(),
        )
        response = self.model.get_structured_response(
            messages=[Message.from_langchain(m) for m in formatted_messages],
            schema=VerifiedToolInputs,
        )
        response = VerifiedToolInputs.model_validate(response)

        # Validate the arguments against the tool's schema
        response = self._validate_args_against_schema(response)
        self.agent.verified_args = response

        return {"messages": [response.model_dump_json(indent=2)]}

    def _validate_args_against_schema(self, tool_inputs: VerifiedToolInputs) -> VerifiedToolInputs:
        """Validate tool arguments against the tool's schema and mark invalid ones as made up.

        Args:
            tool_inputs (VerifiedToolInputs): The tool_inputs to validate against the tool schema.

        Returns:
            Updated VerifiedToolInputs with invalid args marked with schema_invalid=True.

        """
        arg_dict = {arg.name: arg.value for arg in tool_inputs.args}

        try:
            if self.agent.tool:
                self.agent.tool.args_schema.model_validate(arg_dict)
        except ValidationError as e:
            # Extract the arg names from the pydantic error to mark them as schema_invalid = True.
            # At this point we know the arguments are invalid, so we can trigger a clarification
            # request.
            invalid_arg_names = {
                error["loc"][0]
                for error in e.errors()
                if error.get("loc") and len(error["loc"]) > 0
            }
            [
                setattr(arg, "schema_invalid", True)
                for arg in tool_inputs.args
                if arg.name in invalid_arg_names
            ]
        # Mark any made up arguments that are None and optional as not made up.
        # We don't need to raise a clarification for these
        [
            setattr(arg, "made_up", False)
            for arg in tool_inputs.args
            if arg.value is None
            and arg.made_up
            and self.agent.tool
            and not self.agent.tool.args_schema.model_fields[arg.name].is_required()
        ]
        return tool_inputs


class ToolCallingModel:
    """Model to call the tool with the verified arguments."""

    tool_calling_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="You are very powerful assistant, but don't know current events.",
            ),
            HumanMessagePromptTemplate.from_template(
                "context:\n{verified_args}\n"
                "Make sure you don't repeat past errors: {past_errors}\n"
                "Use the provided tool with the arguments in the context, as "
                "long as they are valid.\n",
            ),
        ],
    )

    def __init__(
        self,
        model: LangChainGenerativeModel,
        context: str,
        tools: list[StructuredTool],
        agent: DefaultExecutionAgent,
    ) -> None:
        """Initialize the model.

        Args:
            model (LangChainGenerativeModel): The language model used for argument parsing.
            context (str): The context for argument generation.
            agent (DefaultExecutionAgent): The agent using the parser model.
            tools (list[StructuredTool]): The tools to pass to the model.

        """
        self.model = model
        self.context = context
        self.agent = agent
        self.tools = tools

    def invoke(self, state: MessagesState) -> dict[str, Any]:
        """Invoke the model with the given message state.

        Args:
            state (MessagesState): The current state of the conversation.

        Returns:
            dict[str, Any]: The response after invoking the model.

        Raises:
            InvalidRunStateError: If the agent's tool is not available.

        """
        verified_args = self.agent.verified_args
        if not verified_args:
            raise InvalidPlanRunStateError
        # handle any clarifications before calling
        if self.agent and self.agent.plan_run.outputs.clarifications:
            for arg in verified_args.args:
                matching_clarification = self.agent.get_last_resolved_clarification(arg.name)
                if matching_clarification and arg.value != matching_clarification.response:
                    arg.value = matching_clarification.response
                    arg.made_up = False
                    arg.schema_invalid = False

        model = self.model.to_langchain().bind_tools(self.tools)

        messages = state["messages"]
        past_errors = [msg for msg in messages if "ToolSoftError" in msg.content]
        response = model.invoke(
            self.tool_calling_prompt.format_messages(
                verified_args=verified_args.model_dump_json(indent=2),
                past_errors=past_errors,
            ),
        )
        return {"messages": [response]}


class DefaultExecutionAgent(BaseExecutionAgent):
    """Agent responsible for achieving a task by using verification.

    This agent does the following things:
     1. It uses an LLM to make sure that we have the right arguments for the tool, with
        explanations of the values and where they come from.
     2. It uses an LLM to make sure that the arguments are correct, and that they are labeled
        as provided, inferred or assumed.
     3. If any of the arguments are assumed, it will request a clarification.
     4. If the arguments are correct, it will call the tool and return the result to the user.
     5. If the tool fails, it will try again at least 3 times.

    Also, if the agent is being called a second time, it will just jump to step 4.

    Possible improvements:
     1. This approach (as well as the other agents) could be improved for arguments that are lists
    """

    def __init__(
        self,
        step: Step,
        plan_run: PlanRun,
        config: Config,
        tool: Tool | None = None,
    ) -> None:
        """Initialize the agent.

        Args:
            step (Step): The current step in the task plan.
            plan_run (PlanRun): The run that defines the task execution process.
            config (Config): The configuration settings for the agent.
            tool (Tool | None): The tool to be used for the task (optional).

        """
        super().__init__(step, plan_run, config, tool)
        self.verified_args: VerifiedToolInputs | None = None
        self.new_clarifications: list[Clarification] = []

    def clarifications_or_continue(
        self,
        state: MessagesState,
    ) -> Literal[AgentNode.TOOL_AGENT, END]:  # type: ignore  # noqa: PGH003
        """Determine if we should continue with the tool call or request clarifications instead.

        Args:
            state (MessagesState): The current state of the conversation.

        Returns:
            Literal[AgentNode.TOOL_AGENT, END]: The next node we should route to.

        """
        messages = state["messages"]
        last_message = messages[-1]
        arguments = VerifiedToolInputs.model_validate_json(str(last_message.content))

        for arg in arguments.args:
            if not arg.made_up and not arg.schema_invalid:
                continue
            matching_clarification = self.get_last_resolved_clarification(arg.name)

            if not matching_clarification:
                self.new_clarifications.append(
                    InputClarification(
                        plan_run_id=self.plan_run.id,
                        argument_name=arg.name,
                        user_guidance=f"Missing Argument: {arg.name}",
                        step=self.plan_run.current_step_index,
                    ),
                )
        if self.new_clarifications:
            return END

        state.update({"messages": [arguments.model_dump_json(indent=2)]})  # type: ignore  # noqa: PGH003
        return AgentNode.TOOL_AGENT

    def get_last_resolved_clarification(
        self,
        arg_name: str,
    ) -> Clarification | None:
        """Return the last argument clarification that matches the given arg_name.

        Args:
            arg_name (str): The name of the argument to match clarifications for

        Returns:
            Clarification | None: The matched clarification

        """
        matching_clarification = None
        for clarification in self.plan_run.outputs.clarifications:
            if (
                clarification.resolved
                and getattr(clarification, "argument_name", None) == arg_name
                and clarification.step == self.plan_run.current_step_index
            ):
                matching_clarification = clarification
        return matching_clarification

    def execute_sync(self) -> Output:
        """Run the core execution logic of the task.

        This method will invoke the tool with arguments that are parsed and verified first.

        Returns:
            Output: The result of the agent's execution, containing the tool call result.

        """
        if not self.tool:
            raise InvalidAgentError("Tool is required for DefaultExecutionAgent")
        context = self.get_system_context()
        model = self.config.resolve_langchain_model(EXECUTION_MODEL_KEY)

        tools = [
            self.tool.to_langchain_with_artifact(
                ctx=ToolRunContext(
                    execution_context=get_execution_context(),
                    plan_run_id=self.plan_run.id,
                    config=self.config,
                    clarifications=self.plan_run.get_clarifications_for_step(),
                ),
            ),
        ]
        tool_node = ToolNode(tools)

        graph = StateGraph(MessagesState)
        """
        The execution graph represented here can be generated using
        `print(app.get_graph().draw_mermaid())` on the compiled run (and running any agent
        task). The below represents the current state of the graph (use a mermaid editor
        to view e.g <https://mermaid.live/edit>)
        graph TD;
                __start__([<p>__start__</p>]):::first
                tool_agent(tool_agent)
                argument_parser(argument_parser)
                argument_verifier(argument_verifier)
                tools(tools)
                summarizer(summarizer)
                __end__([<p>__end__</p>]):::last
                __start__ --> argument_parser;
                argument_parser --> argument_verifier;
                summarizer --> __end__;
                argument_verifier -.-> tool_agent;
                argument_verifier -.-> __end__;
                tools -.-> tool_agent;
                tools -.-> summarizer;
                tools -.-> __end__;
                tool_agent -.-> tools;
                tool_agent -.-> __end__;
                classDef default fill:#f2f0ff,line-height:1.2
                classDef first fill-opacity:0
                classDef last fill:#bfb6fc
        """

        graph.add_node(AgentNode.TOOL_AGENT, ToolCallingModel(model, context, tools, self).invoke)
        if self.verified_args:
            graph.add_edge(START, AgentNode.TOOL_AGENT)
        else:
            graph.add_node(AgentNode.ARGUMENT_PARSER, ParserModel(model, context, self).invoke)
            graph.add_node(AgentNode.ARGUMENT_VERIFIER, VerifierModel(model, context, self).invoke)
            graph.add_edge(START, AgentNode.ARGUMENT_PARSER)
            graph.add_edge(AgentNode.ARGUMENT_PARSER, AgentNode.ARGUMENT_VERIFIER)
            graph.add_conditional_edges(
                AgentNode.ARGUMENT_VERIFIER,
                self.clarifications_or_continue,
            )

        graph.add_node(AgentNode.TOOLS, tool_node)
        graph.add_node(
            AgentNode.SUMMARIZER,
            StepSummarizer(self.config, model, self.tool, self.step).invoke,
        )
        graph.add_conditional_edges(
            AgentNode.TOOLS,
            lambda state: next_state_after_tool_call(self.config, state, self.tool),
        )
        graph.add_conditional_edges(
            AgentNode.TOOL_AGENT,
            tool_call_or_end,
        )
        graph.add_edge(AgentNode.SUMMARIZER, END)

        app = graph.compile()
        invocation_result = app.invoke({"messages": []})
        return process_output(
            invocation_result["messages"],
            self.tool,
            self.new_clarifications,
        )

```

## File: portia/execution_agents/context.py

```python
"""Context builder that generates contextual information for the PlanRun.

This module defines a set of functions that build various types of context
required for the run execution. It takes information about inputs,
outputs, clarifications, and execution metadata to build context strings
used by the agent to perform tasks. The context can be extended with
additional system or user-provided data.
"""

from __future__ import annotations

from datetime import UTC, datetime
from typing import TYPE_CHECKING

from portia.clarification import (
    ClarificationListType,
    InputClarification,
    MultipleChoiceClarification,
    ValueConfirmationClarification,
)

if TYPE_CHECKING:
    from portia.execution_agents.output import Output
    from portia.execution_context import ExecutionContext
    from portia.plan import Step, Variable
    from portia.plan_run import PlanRun


def generate_main_system_context() -> list[str]:
    """Generate the main system context.

    Returns:
        list[str]: A list of strings representing the system context.

    """
    return [
        "System Context:",
        f"Today's date is {datetime.now(UTC).strftime('%Y-%m-%d')}",
    ]


def generate_input_context(
    inputs: list[Variable],
    previous_outputs: dict[str, Output],
) -> list[str]:
    """Generate context for the inputs and indicate which ones were used.

    Args:
        inputs (list[Variable]): The list of inputs for the current step.
        previous_outputs (dict[str, Output]): A dictionary of previous step outputs.

    Returns:
        list[str]: A list of strings representing the input context.

    """
    input_context = ["Inputs: the original inputs provided by the planning_agent"]
    used_outputs = set()
    for ref in inputs:
        if ref.name in previous_outputs:
            input_context.extend(
                [
                    f"input_name: {ref.name}",
                    f"input_value: {previous_outputs[ref.name].get_value()}",
                    f"input_description: {ref.description}",
                    "----------",
                ],
            )
            used_outputs.add(ref.name)

    unused_output_keys = set(previous_outputs.keys()) - used_outputs
    if len(unused_output_keys) > 0:
        input_context.append(
            "Broader context: This may be useful information from previous steps that can "
            "indirectly help you.",
        )
        for output_key in unused_output_keys:
            # We truncate the output value to 10000 characters to avoid overwhelming the
            # LLM with too much information.
            output_val = (str(previous_outputs[output_key].get_value()) or "")[:10000]
            input_context.extend(
                [
                    f"output_name: {output_key}",
                    f"output_value: {output_val}",
                    "----------",
                ],
            )

    return input_context


def generate_clarification_context(clarifications: ClarificationListType, step: int) -> list[str]:
    """Generate context from clarifications for the given step.

    Args:
        clarifications (ClarificationListType): A list of clarification objects.
        step (int): The step index for which clarifications are being generated.

    Returns:
        list[str]: A list of strings representing the clarification context.

    """
    clarification_context = []
    # It's important we distinguish between clarifications for the current step where we really
    # want to use the value provided, and clarifications for other steps which may be useful
    # (e.g. consider a plan with 10 steps, each needing the same clarification, we don't want
    # to ask 10 times) but can also lead to side effects (e.g. consider a Plan with two steps where
    # both steps use different tools but with the same parameter name. We don't want to use the
    # clarification from the previous step for the second tool)
    current_step_clarifications = []
    other_step_clarifications = []

    for clarification in clarifications:
        if clarification.step == step:
            current_step_clarifications.append(clarification)
        else:
            other_step_clarifications.append(clarification)

    if current_step_clarifications:
        clarification_context.extend(
            [
                "Clarifications:",
                "This section contains the user provided response to previous clarifications",
                "for the current step. They should take priority over any other context given.",
            ],
        )
        for clarification in current_step_clarifications:
            if (
                isinstance(
                    clarification,
                    (
                        InputClarification,
                        MultipleChoiceClarification,
                        ValueConfirmationClarification,
                    ),
                )
                and clarification.step == step
            ):
                clarification_context.extend(
                    [
                        f"input_name: {clarification.argument_name}",
                        f"clarification_reason: {clarification.user_guidance}",
                        f"input_value: {clarification.response}",
                        "----------",
                    ],
                )

    return clarification_context


def generate_context_from_execution_context(context: ExecutionContext) -> list[str]:
    """Generate context from the execution context.

    Args:
        context (ExecutionContext): The execution context containing metadata and additional data.

    Returns:
        list[str]: A list of strings representing the execution context.

    """
    if not context.end_user_id and not context.additional_data:
        return []

    execution_context = ["Metadata: This section contains general context about this execution."]
    if context.end_user_id:
        execution_context.extend(
            [
                f"end_user_id: {context.end_user_id}",
            ],
        )
    for key, value in context.additional_data.items():
        execution_context.extend(
            [
                f"context_key_name: {key} context_key_value: {value}",
                "----------",
            ],
        )
    return execution_context


def build_context(ctx: ExecutionContext, step: Step, plan_run: PlanRun) -> str:
    """Build the context string for the agent using inputs/outputs/clarifications/ctx.

    Args:
        ctx (ExecutionContext): The execution context containing agent and system metadata.
        step (Step): The current step in the PlanRun including inputs.
        plan_run (PlanRun): The current run containing outputs and clarifications.

    Returns:
        str: A string containing all relevant context information.

    """
    inputs = step.inputs
    previous_outputs = plan_run.outputs.step_outputs
    clarifications = plan_run.outputs.clarifications

    system_context = generate_main_system_context()

    # exit early if no additional information
    if not inputs and not clarifications and not previous_outputs:
        return "\n".join(system_context)

    context = ["Additional context: You MUST use this information to complete your task."]

    # Generate and append input context
    input_context = generate_input_context(inputs, previous_outputs)
    context.extend(input_context)

    # Generate and append clarifications context
    clarification_context = generate_clarification_context(
        clarifications,
        plan_run.current_step_index,
    )
    context.extend(clarification_context)

    # Handle execution context
    execution_context = generate_context_from_execution_context(ctx)
    context.extend(execution_context)

    # Append System Context
    context.extend(system_context)

    return "\n".join(context)

```

## File: portia/execution_agents/one_shot_agent.py

```python
"""A simple OneShotAgent optimized for simple tool calling tasks.

This agent invokes the OneShotToolCallingModel up to four times, but each individual
attempt is a one-shot call. It is useful when the tool call is simple, minimizing cost.
However, for more complex tool calls, the DefaultExecutionAgent is recommended as it will
be more successful than the OneShotAgent.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode

from portia.config import EXECUTION_MODEL_KEY
from portia.errors import InvalidAgentError
from portia.execution_agents.base_execution_agent import BaseExecutionAgent
from portia.execution_agents.execution_utils import (
    AgentNode,
    next_state_after_tool_call,
    process_output,
    tool_call_or_end,
)
from portia.execution_agents.utils.step_summarizer import StepSummarizer
from portia.execution_context import get_execution_context
from portia.tool import ToolRunContext

if TYPE_CHECKING:
    from langchain.tools import StructuredTool

    from portia.config import Config
    from portia.execution_agents.output import Output
    from portia.model import LangChainGenerativeModel
    from portia.plan import Step
    from portia.plan_run import PlanRun
    from portia.tool import Tool


class OneShotToolCallingModel:
    """One-shot model for calling a given tool.

    This model directly passes the tool and context to the language model (LLM)
    to generate a response. It is suitable for simple tasks where the arguments
    are already correctly formatted and complete. This model does not validate
    arguments (e.g., it will not catch missing arguments).

    It is recommended to use the DefaultExecutionAgent for more complex tasks.

    Args:
        model (GenerativeModel): The language model to use for generating responses.
        context (str): The context to provide to the language model when generating a response.
        tools (list[StructuredTool]): A list of tools that can be used during the task.
        agent (OneShotAgent): The agent responsible for managing the task.

    Methods:
        invoke(MessagesState): Invokes the LLM to generate a response based on the query, context,
                               and past errors.

    """

    tool_calling_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content="You are very powerful assistant, but don't know current events.",
            ),
            HumanMessagePromptTemplate.from_template(
                [
                    "query:",
                    "{query}",
                    "context:",
                    "{context}",
                    "Use the provided tool. You should provide arguments that match the tool's"
                    "schema using the information contained in the query and context."
                    "Make sure you don't repeat past errors: {past_errors}",
                ],
            ),
        ],
    )

    def __init__(
        self,
        model: LangChainGenerativeModel,
        context: str,
        tools: list[StructuredTool],
        agent: OneShotAgent,
    ) -> None:
        """Initialize the OneShotToolCallingModel.

        Args:
            model (LangChainGenerativeModel): The language model to use for generating responses.
            context (str): The context to be used when generating the response.
            tools (list[StructuredTool]): A list of tools that can be used during the task.
            agent (OneShotAgent): The agent that is managing the task.

        """
        self.model = model
        self.context = context
        self.agent = agent
        self.tools = tools

    def invoke(self, state: MessagesState) -> dict[str, Any]:
        """Invoke the model with the given message state.

        This method formats the input for the language model using the query, context,
        and past errors, then generates a response by invoking the model.

        Args:
            state (MessagesState): The state containing the messages and other necessary data.

        Returns:
            dict[str, Any]: A dictionary containing the model's generated response.

        """
        model = self.model.to_langchain().bind_tools(self.tools)
        messages = state["messages"]
        past_errors = [msg for msg in messages if "ToolSoftError" in msg.content]
        response = model.invoke(
            self.tool_calling_prompt.format_messages(
                query=self.agent.step.task,
                context=self.context,
                past_errors=past_errors,
            ),
        )
        return {"messages": [response]}


class OneShotAgent(BaseExecutionAgent):
    """Agent responsible for achieving a task by using langgraph.

    This agent performs the following steps:
    1. Calls the tool with unverified arguments.
    2. Retries tool calls up to 4 times.

    Args:
        step (Step): The current step in the task plan.
        plan_run (PlanRun): The run that defines the task execution process.
        config (Config): The configuration settings for the agent.
        tool (Tool | None): The tool to be used for the task (optional).

    Methods:
        execute_sync(): Executes the core logic of the agent's task, using the provided tool

    """

    def __init__(
        self,
        step: Step,
        plan_run: PlanRun,
        config: Config,
        tool: Tool | None = None,
    ) -> None:
        """Initialize the OneShotAgent.

        Args:
            step (Step): The current step in the task plan.
            plan_run (PlanRun): The run that defines the task execution process.
            config (Config): The configuration settings for the agent.
            tool (Tool | None): The tool to be used for the task (optional).

        """
        super().__init__(step, plan_run, config, tool)

    def execute_sync(self) -> Output:
        """Run the core execution logic of the task.

        This method will invoke the tool with arguments

        Returns:
            Output: The result of the agent's execution, containing the tool call result.

        """
        if not self.tool:
            raise InvalidAgentError("No tool available")

        context = self.get_system_context()
        model = self.config.resolve_langchain_model(EXECUTION_MODEL_KEY)
        tools = [
            self.tool.to_langchain_with_artifact(
                ctx=ToolRunContext(
                    execution_context=get_execution_context(),
                    plan_run_id=self.plan_run.id,
                    config=self.config,
                    clarifications=self.plan_run.get_clarifications_for_step(),
                ),
            ),
        ]
        tool_node = ToolNode(tools)

        graph = StateGraph(MessagesState)
        graph.add_node(
            AgentNode.TOOL_AGENT,
            OneShotToolCallingModel(model, context, tools, self).invoke,
        )
        graph.add_node(AgentNode.TOOLS, tool_node)
        graph.add_node(
            AgentNode.SUMMARIZER,
            StepSummarizer(self.config, model, self.tool, self.step).invoke,
        )
        graph.add_edge(START, AgentNode.TOOL_AGENT)

        # Use execution manager for state transitions
        graph.add_conditional_edges(
            AgentNode.TOOL_AGENT,
            tool_call_or_end,
        )
        graph.add_conditional_edges(
            AgentNode.TOOLS,
            lambda state: next_state_after_tool_call(self.config, state, self.tool),
        )
        graph.add_edge(AgentNode.SUMMARIZER, END)

        app = graph.compile()
        invocation_result = app.invoke({"messages": []})

        return process_output(invocation_result["messages"], self.tool)

```

## File: portia/execution_agents/__init__.py

```python
"""Contains agent implementations."""

```

## File: portia/execution_agents/execution_utils.py

```python
"""Agent execution utilities.

This module contains utility functions for managing agent execution flow.
"""

from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING, Any, Literal

from langchain_core.messages import BaseMessage, ToolMessage
from langgraph.graph import END, MessagesState

from portia.clarification import Clarification
from portia.errors import InvalidAgentOutputError, ToolFailedError, ToolRetryError
from portia.execution_agents.output import LocalOutput, Output

if TYPE_CHECKING:
    from portia.config import Config
    from portia.tool import Tool


class AgentNode(str, Enum):
    """Nodes for agent execution.

    This enumeration defines the different types of nodes that can be encountered
    during the agent execution process.

    Attributes:
        TOOL_AGENT (str): A node representing the tool agent.
        SUMMARIZER (str): A node representing the summarizer.
        TOOLS (str): A node representing the tools.
        ARGUMENT_VERIFIER (str): A node representing the argument verifier.
        ARGUMENT_PARSER (str): A node representing the argument parser.

    """

    TOOL_AGENT = "tool_agent"
    SUMMARIZER = "summarizer"
    TOOLS = "tools"
    ARGUMENT_VERIFIER = "argument_verifier"
    ARGUMENT_PARSER = "argument_parser"


MAX_RETRIES = 4


def next_state_after_tool_call(
    config: Config,
    state: MessagesState,
    tool: Tool | None = None,
) -> Literal[AgentNode.TOOL_AGENT, AgentNode.SUMMARIZER, END]:  # type: ignore  # noqa: PGH003
    """Determine the next state after a tool call.

    This function checks the state after a tool call to determine if the run
    should proceed to the tool agent again, to the summarizer, or end.

    Args:
        config (Config): The configuration for the run.
        state (MessagesState): The current state of the messages.
        tool (Tool | None): The tool involved in the call, if any.

    Returns:
        Literal[AgentNode.TOOL_AGENT, AgentNode.SUMMARIZER, END]: The next state to transition to.

    Raises:
        ToolRetryError: If the tool has an error and the maximum retry limit has not been reached.

    """
    messages = state["messages"]
    last_message = messages[-1]
    errors = [msg for msg in messages if "ToolSoftError" in msg.content]

    if "ToolSoftError" in last_message.content and len(errors) < MAX_RETRIES:
        return AgentNode.TOOL_AGENT
    if (
        "ToolSoftError" not in last_message.content
        and tool
        and (
            getattr(tool, "should_summarize", False)
            # If the value is larger than the threshold value, always summarise them as they are
            # too big to store the full value locally
            or config.exceeds_output_threshold(last_message.content)
        )
        and isinstance(last_message, ToolMessage)
        and not is_clarification(last_message.artifact)
    ):
        return AgentNode.SUMMARIZER
    return END


def is_clarification(artifact: Any) -> bool:  # noqa: ANN401
    """Check if the artifact is a clarification or list of clarifications."""
    return isinstance(artifact, Clarification) or (
        isinstance(artifact, list)
        and len(artifact) > 0
        and all(isinstance(item, Clarification) for item in artifact)
    )


def tool_call_or_end(
    state: MessagesState,
) -> Literal[AgentNode.TOOLS, END]:  # type: ignore  # noqa: PGH003
    """Determine if tool execution should continue.

    This function checks if the current state indicates that the tool execution
    should continue, or if the run should end.

    Args:
        state (MessagesState): The current state of the messages.

    Returns:
        Literal[AgentNode.TOOLS, END]: The next state to transition to.

    """
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls"):
        return AgentNode.TOOLS
    return END


def process_output(  # noqa: C901
    messages: list[BaseMessage],
    tool: Tool | None = None,
    clarifications: list[Clarification] | None = None,
) -> Output:
    """Process the output of the agent.

    This function processes the agent's output based on the type of message received.
    It raises errors if the tool encounters issues and returns the appropriate output.

    Args:
        messages (list[BaseMessage]): The set of messages received from the agent's plan_run.
        tool (Tool | None): The tool associated with the agent, if any.
        clarifications (list[Clarification] | None): A list of clarifications, if any.

    Returns:
        Output: The processed output, which can be an error, tool output, or clarification.

    Raises:
        ToolRetryError: If there was a soft error with the tool and retries are allowed.
        ToolFailedError: If there was a hard error with the tool.
        InvalidAgentOutputError: If the output from the agent is invalid.

    """
    output_values: list[Output] = []
    for message in messages:
        if "ToolSoftError" in message.content and tool:
            raise ToolRetryError(tool.id, str(message.content))
        if "ToolHardError" in message.content and tool:
            raise ToolFailedError(tool.id, str(message.content))
        if clarifications and len(clarifications) > 0:
            return LocalOutput[list[Clarification]](
                value=clarifications,
            )
        if isinstance(message, ToolMessage):
            if message.artifact and isinstance(message.artifact, Output):
                output_values.append(message.artifact)
            elif message.artifact:
                output_values.append(LocalOutput(value=message.artifact))
            else:
                output_values.append(LocalOutput(value=message.content))

    if len(output_values) == 0:
        raise InvalidAgentOutputError(str([message.content for message in messages]))

    # if there's only one output return just the value
    if len(output_values) == 1:
        output = output_values[0]
        return LocalOutput(
            value=output.get_value(),
            summary=output.get_summary() or output.serialize_value(),
        )

    values = []
    summaries = []

    for output in output_values:
        values.append(output.get_value())
        summaries.append(output.get_summary() or output.serialize_value())

    return LocalOutput(value=values, summary=", ".join(summaries))

```

## File: portia/execution_agents/output.py

```python
"""Outputs from a plan run step.

These are stored and can be used as inputs to future steps
"""

from __future__ import annotations

import json
from abc import abstractmethod
from datetime import date, datetime
from enum import Enum
from typing import TYPE_CHECKING, Generic, Union

from pydantic import BaseModel, ConfigDict, Field, field_serializer

from portia.common import SERIALIZABLE_TYPE_VAR, Serializable
from portia.prefixed_uuid import PlanRunUUID

if TYPE_CHECKING:
    from portia.storage import AgentMemory


class BaseOutput(BaseModel, Generic[SERIALIZABLE_TYPE_VAR]):
    """Base interface for concrete output classes to implement."""

    @abstractmethod
    def get_value(self) -> Serializable | None:
        """Return the value of the output.

        This should not be so long that it is an issue for LLM prompts.
        """

    @abstractmethod
    def serialize_value(self) -> str:
        """Serialize the value to a string."""

    @abstractmethod
    def full_value(self, agent_memory: AgentMemory) -> Serializable | None:
        """Get the full value, fetching from remote storage or file if necessary.

        This value may be long and so is not suitable for use in LLM prompts.
        """

    @abstractmethod
    def get_summary(self) -> str | None:
        """Return the summary of the output."""


class LocalOutput(BaseOutput, Generic[SERIALIZABLE_TYPE_VAR]):
    """Output that is stored locally."""

    model_config = ConfigDict(extra="forbid")

    value: Serializable | None = Field(
        default=None,
        description="The output of the tool.",
    )

    summary: str | None = Field(
        default=None,
        description="Textual summary of the output of the tool. Not all tools generate summaries.",
    )

    def get_value(self) -> Serializable | None:
        """Get the value of the output."""
        return self.value

    def serialize_value(self) -> str:
        """Serialize the value to a string."""
        return self.serialize_value_field(self.value)

    def full_value(self, agent_memory: AgentMemory) -> Serializable | None:  # noqa: ARG002
        """Return the full value.

        As the value is stored locally, this is the same as get_value() for this type of output.
        """
        return self.value

    def get_summary(self) -> str | None:
        """Return the summary of the output."""
        return self.summary

    @field_serializer("value")
    def serialize_value_field(self, value: Serializable | None) -> str:  # noqa: C901, PLR0911
        """Serialize the value to a string.

        Args:
            value (SERIALIZABLE_TYPE_VAR | None): The value to serialize.

        Returns:
            str: The serialized value as a string.

        """
        if value is None:
            return ""

        if isinstance(value, str):
            return value

        if isinstance(value, list):
            return json.dumps(
                [
                    item.model_dump(mode="json") if isinstance(item, BaseModel) else item
                    for item in value
                ],
                ensure_ascii=False,
            )

        if isinstance(value, (dict, tuple)):
            return json.dumps(value, ensure_ascii=False)  # Ensure proper JSON formatting

        if isinstance(value, set):
            return json.dumps(
                list(value),
                ensure_ascii=False,
            )  # Convert set to list before serialization

        if isinstance(value, (int, float, bool)):
            return json.dumps(value, ensure_ascii=False)  # Ensures booleans become "true"/"false"

        if isinstance(value, (datetime, date)):
            return value.isoformat()  # Convert date/time to ISO format

        if isinstance(value, Enum):
            return str(value.value)  # Convert Enums to their values

        if isinstance(value, (BaseModel)):
            return value.model_dump_json()  # Use Pydantic's built-in serialization for models

        if isinstance(value, bytes):
            return value.decode("utf-8", errors="ignore")  # Convert bytes to string

        return str(value)  # Fallback for other types


class AgentMemoryOutput(BaseOutput, Generic[SERIALIZABLE_TYPE_VAR]):
    """Output that is stored in agent memory."""

    model_config = ConfigDict(extra="forbid")

    output_name: str
    plan_run_id: PlanRunUUID
    summary: str = Field(
        description="Textual summary of the output of the tool. Not all tools generate summaries.",
    )

    def get_value(self) -> Serializable | None:
        """Return the summary of the output as the value is too large to be retained locally."""
        return self.summary

    def serialize_value(self) -> str:
        """Serialize the value to a string.

        We use the summary as the value is too large to be retained locally.
        """
        return self.summary

    def full_value(self, agent_memory: AgentMemory) -> Serializable | None:
        """Get the full value, fetching from remote storage or file if necessary."""
        return agent_memory.get_plan_run_output(self.output_name, self.plan_run_id).get_value()

    def get_summary(self) -> str:
        """Return the summary of the output."""
        return self.summary


Output = Union[LocalOutput, AgentMemoryOutput]

```

## File: portia/execution_agents/base_execution_agent.py

```python
"""Agents are responsible for executing steps of a PlanRun.

The BaseAgent class is the base class that all agents must extend.
"""

from __future__ import annotations

from abc import abstractmethod
from typing import TYPE_CHECKING

from portia.execution_agents.context import build_context

# TODO: Remove this once the backend / evals are updated to use the new import  # noqa: FIX002, TD002, TD003, E501
from portia.execution_agents.output import Output  # noqa: TC001
from portia.execution_context import get_execution_context

if TYPE_CHECKING:
    from portia.config import Config
    from portia.plan import Step
    from portia.plan_run import PlanRun
    from portia.tool import Tool


class BaseExecutionAgent:
    """An ExecutionAgent is responsible for carrying out the task defined in the given Step.

    This BaseExecutionAgent is the class all ExecutionAgents must extend. Critically,
    ExecutionAgents must implement the execute_sync function which is responsible for
    actually carrying out the task as given in the step. They have access to copies of the
    step, plan_run and config but changes to those objects are forbidden.

    Optionally, new execution agents may also override the get_context function, which is
    responsible for building the system context for the agent. This should be done with
    thought, as the details of the system context are critically important for LLM
    performance.
    """

    def __init__(
        self,
        step: Step,
        plan_run: PlanRun,
        config: Config,
        tool: Tool | None = None,
    ) -> None:
        """Initialize the base agent with the given args.

        Importantly, the models here are frozen copies of those used by the Portia instance.
        They are meant as read-only references, useful for execution of the task
        but cannot be edited. The agent should return output via the response
        of the execute_sync method.

        Args:
            step (Step): The step that defines the task to be executed.
            plan_run (PlanRun): The run that contains the step and related data.
            config (Config): The configuration settings for the agent.
            tool (Tool | None): An optional tool associated with the agent (default is None).

        """
        self.step = step
        self.tool = tool
        self.config = config
        self.plan_run = plan_run

    @abstractmethod
    def execute_sync(self) -> Output:
        """Run the core execution logic of the task synchronously.

        Implementation of this function is deferred to individual agent implementations,
        making it simple to write new ones.

        Returns:
            Output: The output of the task execution.

        """

    def get_system_context(self) -> str:
        """Build a generic system context string from the step and run provided.

        This function retrieves the execution context and generates a system context
        based on the step and run provided to the agent.

        Returns:
            str: A string containing the system context for the agent.

        """
        ctx = get_execution_context()
        return build_context(
            ctx,
            self.step,
            self.plan_run,
        )

```

## File: portia/execution_agents/utils/step_summarizer.py

```python
"""StepSummarizer implementation.

The StepSummarizer can be used by agents to summarize the output of a given tool.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

from jinja2 import Template
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import SystemMessage
from langchain_core.messages import ToolMessage
from langgraph.graph import MessagesState  # noqa: TC002

from portia.execution_agents.output import Output
from portia.logger import logger
from portia.model import GenerativeModel, Message
from portia.planning_agents.context import get_tool_descriptions_for_tools

if TYPE_CHECKING:
    from portia.config import Config
    from portia.model import GenerativeModel
    from portia.plan import Step
    from portia.tool import Tool


class StepSummarizer:
    """Class to summarize the output of a tool using llm.

    This is used only on the tool output message.

    Attributes:
        summarizer_prompt (ChatPromptTemplate): The prompt template used to generate the summary.
        model (GenerativeModel): The language model used for summarization.
        summary_max_length (int): The maximum length of the summary.
        step (Step): The step that produced the output.

    """

    summarizer_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=(
                    "You are a highly skilled summarizer. Your task is to create a textual summary"
                    "of the provided tool output, make sure to follow the guidelines provided.\n"
                    "- Focus on the key information and maintain accuracy.\n"
                    "- Make sure to not exceed the max limit of {max_length} characters.\n"
                    "- Don't produce an overly long summary if it doesn't make sense.\n"
                ),
            ),
            HumanMessagePromptTemplate.from_template(
                "Here is original task:\n{task_description}\n"
                "Here is the description of the tool that produced "
                "the output:\n{tool_description}\n"
                "Please summarize the following output:\n{tool_output}\n",
            ),
        ],
    )

    def __init__(
        self,
        config: Config,
        model: GenerativeModel,
        tool: Tool,
        step: Step,
        summary_max_length: int = 500,
    ) -> None:
        """Initialize the model.

        Args:
            config (Config): The configuration for the run.
            model (GenerativeModel): The language model used for summarization.
            tool (Tool): The tool used for summarization.
            step (Step): The step that produced the output.
            summary_max_length (int): The maximum length of the summary. Default is 500 characters.

        """
        self.config = config
        self.model = model
        self.summary_max_length = summary_max_length
        self.tool = tool
        self.step = step

    def invoke(self, state: MessagesState) -> dict[str, Any]:
        """Invoke the model with the given message state.

        This method processes the last message in the state, checks if it's a tool message with an
        output, and if so, generates a summary of the tool's output. The summary is then added to
        the artifact of the last message.

        Args:
            state (MessagesState): The current state of the messages, which includes the output.

        Returns:
            dict[str, Any]: A dict containing the updated message state, including the summary.

        Raises:
            Exception: If an error occurs during the invocation of the summarizer model.

        """
        messages = state["messages"]
        last_message = messages[-1] if len(messages) > 0 else None
        if not isinstance(last_message, ToolMessage) or not isinstance(
            last_message.artifact,
            Output,
        ):
            return {"messages": [last_message]}

        logger().debug(f"Invoke SummarizerModel on the tool output of {last_message.name}.")
        tool_output = last_message.content
        if self.config.exceeds_output_threshold(tool_output):
            tool_output = (
                f"This is a large value (full length: {len(str(tool_output))} characters) - it is "
                "too long to provide the full value, but it starts with:"
                f"{self._truncate(tool_output, self.config.large_output_threshold_tokens)}"
            )

        try:
            response: Message = self.model.get_response(
                messages=[
                    Message.from_langchain(m)
                    for m in self.summarizer_prompt.format_messages(
                        tool_output=tool_output,
                        max_length=self.summary_max_length,
                        tool_description=get_tool_descriptions_for_tools([self.tool]),
                        task_description=self.step.task,
                    )
                ],
            )
            summary = response.content
            last_message.artifact.summary = summary  # type: ignore[attr-defined]
        except Exception as e:  # noqa: BLE001 - we want to catch all exceptions
            logger().error("Error in SummarizerModel invoke (Skipping summaries): " + str(e))

        return {"messages": [last_message]}

    def _truncate(self, content: str | dict | list[str | dict], max_len_chars: int) -> str:
        """Truncate a value so it is no longer than max_len_chars."""
        content_str = Template("{{ content }}").render(content=str(content))
        return content_str[:max_len_chars]

```

## File: portia/execution_agents/utils/__init__.py

```python
"""Contains llm implementations."""

```

## File: portia/execution_agents/utils/final_output_summarizer.py

```python
"""Utility class for final output summarizer."""

from __future__ import annotations

from typing import TYPE_CHECKING

from portia.config import SUMMARISER_MODEL_KEY
from portia.introspection_agents.introspection_agent import PreStepIntrospectionOutcome
from portia.model import Message

if TYPE_CHECKING:
    from portia.config import Config
    from portia.plan import Plan
    from portia.plan_run import PlanRun


class FinalOutputSummarizer:
    """Utility class responsible for summarizing the run outputs for final output's summary.

    Attributes:
        config (Config): The configuration for the llm.

    """

    SUMMARIZE_TASK = (
        "Summarize all tasks and outputs that answers the query given. Make sure the "
        "summary is including all the previous tasks and outputs and biased towards "
        "the last step output of the plan. Your summary "
        "should be concise and to the point with maximum 500 characters. Do not "
        "include 'Summary:' in the beginning of the summary. Do not make up information "
        "not used in the context.\n"
    )

    def __init__(self, config: Config) -> None:
        """Initialize the summarizer agent.

        Args:
            config (Config): The configuration for the llm.

        """
        self.config = config

    def _build_tasks_and_outputs_context(self, plan: Plan, plan_run: PlanRun) -> str:
        """Build the query, tasks and outputs context.

        Args:
            plan(Plan): The plan containing the steps.
            plan_run(PlanRun): The run to get the outputs from.

        Returns:
            str: The formatted context string

        """
        context = []
        context.append(f"Query: {plan.plan_context.query}")
        context.append("----------")
        outputs = plan_run.outputs.step_outputs
        for step in plan.steps:
            if step.output in outputs:
                output_value = (
                    outputs[step.output].get_summary()
                    if outputs[step.output].get_value()
                    in (
                        PreStepIntrospectionOutcome.SKIP,
                        PreStepIntrospectionOutcome.COMPLETE,
                        PreStepIntrospectionOutcome.FAIL,
                    )
                    else outputs[step.output].get_value()
                )
                context.append(f"Task: {step.task}")
                context.append(f"Output: {output_value}")
                context.append("----------")
        return "\n".join(context)

    def create_summary(self, plan: Plan, plan_run: PlanRun) -> str | None:
        """Execute the summarizer llm and return the summary as a string.

        Args:
            plan (Plan): The plan containing the steps.
            plan_run (PlanRun): The run to summarize.

        Returns:
            str | None: The generated summary or None if generation fails.

        """
        model = self.config.resolve_model(SUMMARISER_MODEL_KEY)
        context = self._build_tasks_and_outputs_context(plan, plan_run)
        response = model.get_response(
            [Message(content=self.SUMMARIZE_TASK + context, role="user")],
        )
        return str(response.content) if response.content else None

```

## File: portia/introspection_agents/introspection_agent.py

```python
"""BaseIntrospectionAgent is the interface for all introspection agents."""

from abc import ABC, abstractmethod

from pydantic import BaseModel, Field

from portia.common import PortiaEnum
from portia.config import Config
from portia.plan import Plan
from portia.plan_run import PlanRun


class PreStepIntrospectionOutcome(PortiaEnum):
    """The Outcome of the introspection."""

    CONTINUE = "CONTINUE"
    SKIP = "SKIP"
    FAIL = "FAIL"
    COMPLETE = "COMPLETE"


class PreStepIntrospection(BaseModel):
    """The outcome of a pre-step introspection."""

    outcome: PreStepIntrospectionOutcome = Field(
        default=PreStepIntrospectionOutcome.CONTINUE,
        description="What action should be taken next based on the state of the plan run.",
    )
    reason: str = Field(
        description="The reason the given outcome was decided on.",
    )


class BaseIntrospectionAgent(ABC):
    """Interface for introspection.

    This class defines the interface for introspection.
    By introspection we mean looking at the state of a plan run and making decisions
    about whether to continue.

    Attributes:
        config (Config): Configuration settings for the PlanningAgent.

    """

    def __init__(self, config: Config) -> None:
        """Initialize the BaseIntrospectionAgent with configuration.

        Args:
            config (Config): The configuration to initialize the BaseIntrospectionAgent.

        """
        self.config = config

    @abstractmethod
    def pre_step_introspection(
        self,
        plan: Plan,
        plan_run: PlanRun,
    ) -> PreStepIntrospection:
        """pre_step_introspection is introspection run before a plan happens.."""
        raise NotImplementedError("pre_step_introspection is not implemented")

```

## File: portia/introspection_agents/default_introspection_agent.py

```python
"""The default introspection agent.

This agent looks at the state of a plan run between steps
and makes decisions about whether execution should continue.
"""

from datetime import UTC, datetime

from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import SystemMessage

from portia.config import INTROSPECTION_MODEL_KEY, Config
from portia.introspection_agents.introspection_agent import (
    BaseIntrospectionAgent,
    PreStepIntrospection,
)
from portia.model import Message
from portia.plan import Plan
from portia.plan_run import PlanRun


class DefaultIntrospectionAgent(BaseIntrospectionAgent):
    """Default Introspection Agent.

    Implements the BaseIntrospectionAgent interface using an LLM to make decisions about what to do.

    Attributes:
        config (Config): Configuration settings for the DefaultIntrospectionAgent.

    """

    def __init__(self, config: Config) -> None:
        """Initialize the DefaultIntrospectionAgent with configuration.

        Args:
            config (Config): The configuration to initialize the DefaultIntrospectionAgent.

        """
        self.config = config

        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=(
                        "You are a highly skilled reviewer who reviews in flight plan execution."
                        "Your job is to examine the state of a plan execution (PlanRun) and "
                        "decide what action should be taken next."
                        "You should use the current_step_index field to identify the current step "
                        "in the plan, and the PlanRun state to know what has happened so far."
                        "The actions that can be taken are:"
                        " - COMPLETE -> complete execution and return the result so far."
                        " - SKIP -> skip the current step execution."
                        " - FAIL -> stop and fail execution entirely."
                        " - CONTINUE -> Continue execution for the current step."
                        "You should choose an outcome based on the following logic in order:\n"
                        " - If the overarching goal of the plan "
                        "has already been met return COMPLETE.\n"
                        " - If the current step has a condition that is false you return SKIP.\n"
                        " - If you cannot evaluate the condition"
                        " because it's impossible to evaluate return FAIL.\n"
                        " - If you cannot evaluate the condition because some data had been skipped"
                        "  in previous steps then return SKIP.\n"
                        " - Otherwise return CONTINUE.\n"
                        "Return the outcome and reason in the given format.\n"
                    ),
                ),
                HumanMessagePromptTemplate.from_template(
                    "Today's date is {current_date} and today is {current_day_of_week}.\n"
                    "Review the following plan + current PlanRun.\n"
                    "Current Plan: {plan}\n"
                    "Current PlanRun: {plan_run}\n",
                ),
            ],
        )

    def pre_step_introspection(
        self,
        plan: Plan,
        plan_run: PlanRun,
    ) -> PreStepIntrospection:
        """Ask the LLM whether to continue, skip or fail the plan_run."""
        return self.config.resolve_model(INTROSPECTION_MODEL_KEY).get_structured_response(
            schema=PreStepIntrospection,
            messages=[
                Message.from_langchain(m)
                for m in self.prompt.format_messages(
                    current_date=datetime.now(UTC).strftime("%Y-%m-%d"),
                    current_day_of_week=datetime.now(UTC).strftime("%A"),
                    plan_run=plan_run.model_dump_json(),
                    plan=plan.model_dump_json(),
                )
            ],
        )

```

## File: portia/introspection_agents/__init__.py

```python
"""Contains introspection agent implementations."""

```

## File: portia/_unstable/__init__.py

```python
from .browser_tool import BrowserTool

__all__ = ["BrowserTool"]

```

## File: portia/_unstable/browser_tool.py

```python
"""Browser tools.

This module contains tools that can be used to navigate to a URL, authenticate the user,
and complete tasks.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import sys
from abc import ABC, abstractmethod
from enum import Enum
from functools import cached_property
from typing import TYPE_CHECKING, Any

from browser_use import Agent, Browser, BrowserConfig, Controller
from browserbase import Browserbase
from pydantic import BaseModel, ConfigDict, Field, HttpUrl
from pydantic_core import PydanticUndefined

from portia.clarification import ActionClarification
from portia.errors import ToolHardError
from portia.model import LangChainGenerativeModel  # noqa: TC001 - used in Pydantic Schema
from portia.tool import Tool, ToolRunContext

if TYPE_CHECKING:
    from browserbase.types import SessionCreateResponse

logger = logging.getLogger(__name__)

NotSet: Any = PydanticUndefined


class BrowserToolForUrlSchema(BaseModel):
    """Input schema for the BrowserToolForUrl."""

    task: str = Field(
        ...,
        description="The task to be completed by the Browser tool.",
    )


class BrowserToolSchema(BaseModel):
    """Input schema for the BrowserTool."""

    url: HttpUrl = Field(
        ...,
        description="The URL to navigate to.",
    )
    task: str = Field(
        ...,
        description="The task to be completed by the Browser tool.",
    )


class BrowserAuthOutput(BaseModel):
    """Output of the Browser tool's authentication check."""

    human_login_required: bool
    login_url: str | None = Field(
        default=None,
        description="The URL to navigate to for login if the user is not authenticated.",
    )
    user_login_guidance: str | None = Field(
        default=None,
        description="Guidance for the user to login if they are not authenticated.",
    )


class BrowserTaskOutput(BaseModel):
    """Output of the Browser tool's task."""

    task_output: str
    human_login_required: bool = Field(
        default=False,
        description="Whether the user needs to login to complete the task.",
    )
    login_url: str | None = Field(
        default=None,
        description="The URL to navigate to for login if the user is not authenticated.",
    )
    user_login_guidance: str | None = Field(
        default=None,
        description="Guidance for the user to login if they are not authenticated.",
    )


class BrowserInfrastructureOption(Enum):
    """Options for the browser infrastructure provider."""

    LOCAL = "local"
    BROWSERBASE = "browserbase"


class BaseBrowserTool(Tool[str]):
    """TODO: Document this."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    id: str = Field(init_var=True, default="browser_tool")
    name: str = Field(init_var=True, default="Browser Tool")
    description: str = Field(
        init_var=True,
        default=(
            "General purpose browser tool. Can be used to navigate to a URL and "
            "complete tasks. Should only be used if the task requires a browser "
            "and you are sure of the URL."
        ),
    )
    args_schema: type[BaseModel] = Field(init_var=True, default=BrowserToolSchema)
    output_schema: tuple[str, str] = ("str", "The Browser tool's response to the user query.")

    model: LangChainGenerativeModel | None = Field(
        default=None,
        exclude=True,
        description="The model to use for the BrowserTool. If not provided, "
        "the model will be resolved from the config.",
    )

    infrastructure_option: BrowserInfrastructureOption = Field(
        default=BrowserInfrastructureOption.BROWSERBASE,
        description="The infrastructure provider to use for the browser tool.",
    )

    @cached_property
    def infrastructure_provider(self) -> BrowserInfrastructureProvider:
        """Get the infrastructure provider instance (cached)."""
        if self.infrastructure_option == BrowserInfrastructureOption.BROWSERBASE:
            return BrowserInfrastructureProviderBrowserBase()
        return BrowserInfrastructureProviderLocal()

    def run(self, ctx: ToolRunContext, url: str, task: str) -> str | ActionClarification:
        """Run the BrowserTool."""
        model = self.model or ctx.config.resolve_langchain_model()
        llm = model.to_langchain()

        async def run_browser_tasks() -> str | ActionClarification:
            # First auth check
            auth_agent = Agent(
                task=(
                    f"Go to {url}. If the user is not signed in, please go to the sign in page, "
                    "and indicate that human login is required by returning "
                    "human_login_required=True, and the url of the sign in page as well as "
                    "what the user should do to sign in. If the user is signed in, please "
                    "return human_login_required=False."
                ),
                llm=llm,
                browser=self.infrastructure_provider.setup_browser(ctx),
                controller=Controller(
                    output_model=BrowserAuthOutput,
                ),
            )
            result = await auth_agent.run()
            auth_result = BrowserAuthOutput.model_validate(json.loads(result.final_result()))  # type: ignore reportArgumentType
            if auth_result.human_login_required:
                if auth_result.user_login_guidance is None or auth_result.login_url is None:
                    raise ToolHardError(
                        "Expected user guidance and login URL if human login is required",
                    )
                return ActionClarification(
                    user_guidance=auth_result.user_login_guidance,
                    action_url=HttpUrl(
                        self.infrastructure_provider.construct_auth_clarification_url(
                            ctx,
                            auth_result.login_url,
                        ),
                    ),
                    plan_run_id=ctx.plan_run_id,
                )

            # Main task
            task_agent = Agent(
                task=task,
                llm=llm,
                browser=self.infrastructure_provider.setup_browser(ctx),
                controller=Controller(
                    output_model=BrowserTaskOutput,
                ),
            )
            result = await task_agent.run()
            task_result = BrowserTaskOutput.model_validate(json.loads(result.final_result()))  # type: ignore reportArgumentType
            if task_result.human_login_required:
                if task_result.user_login_guidance is None or task_result.login_url is None:
                    raise ToolHardError(
                        "Expected user guidance and login URL if human login is required",
                    )
                return ActionClarification(
                    user_guidance=task_result.user_login_guidance,
                    action_url=HttpUrl(
                        self.infrastructure_provider.construct_auth_clarification_url(
                            ctx,
                            task_result.login_url,
                        ),
                    ),
                    plan_run_id=ctx.plan_run_id,
                )
            return task_result.task_output

        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(run_browser_tasks())


class BrowserTool(BaseBrowserTool):
    """General purpose browser tool. Customizable to user requirements.

    This tool is designed to be used for tasks that require a browser. If authentication is
    required, the tool will return an ActionClarification with the user guidance and login URL.
    If authentication is not required, the tool will return the task output. It uses
    (BrowserUse)[https://browser-use.com/] for the task navigation.

    When using the tool, you should ensure that once the user has authenticated, that they
    indicate that authentication is completed and resume the plan run.

    The tool supports both local and BrowserBase infrastructure providers for running the web
    based tasks. If using local, a local Chrome instance will be used, and the tool will not
    support end_user_id. If using BrowserBase, a BrowserBase API key is required and the tool
    can handle separate end users. The infrastructure provider can be specified using the
    `infrastructure_option` argument.

    Args:
        id (str, optional): Custom identifier for the tool. Defaults to "browser_tool".
        name (str, optional): Display name for the tool. Defaults to "Browser Tool".
        description (str, optional): Custom description of the tool's purpose. Defaults to a
            general description of the browser tool's capabilities.
        infrastructure_option (BrowserInfrastructureOption, optional): The infrastructure
            provider to use. Can be either "local" or "browserbase". Defaults to "browserbase".

    """

    def run(self, ctx: ToolRunContext, url: str, task: str) -> str | ActionClarification:
        """Run the BrowserTool."""
        return super().run(ctx, url, task)


class BrowserToolForUrl(BaseBrowserTool):
    """Browser tool for a specific URL.

    This tool is designed to be used for browser-based tasks on the specified URL.
    If authentication is required, the tool will return an ActionClarification with the user
    guidance and login URL. If authentication is not required, the tool will return the task
    output. It uses (BrowserUse)[https://browser-use.com/] for the task navigation.

    When using the tool, the developer should ensure that once the user has completed
    authentication, that they resume the plan run.

    The tool supports both local and BrowserBase infrastructure providers for running the web
    based tasks. If using local, a local Chrome instance will be used, and the tool will not
    support end_user_id. If using BrowserBase, a BrowserBase API key is required and the tool
    can handle separate end users. The infrastructure provider can be specified using the
    `infrastructure_option` argument.

    Args:
        url (str): The URL that this browser tool will navigate to for all tasks.
        id (str, optional): Custom identifier for the tool. If not provided, will be generated
            based on the URL's domain.
        name (str, optional): Display name for the tool. If not provided, will be generated
            based on the URL's domain.
        description (str, optional): Custom description of the tool's purpose. If not provided,
            will be generated with the URL.

    """

    url: str = Field(
        ...,
        description="The URL to navigate to.",
    )

    def __init__(  # noqa: PLR0913
        self,
        url: str,
        id: str | None = None,  # noqa: A002
        name: str | None = None,
        description: str | None = None,
        model: LangChainGenerativeModel | None = NotSet,
        infrastructure_option: BrowserInfrastructureOption | None = NotSet,
    ) -> None:
        """Initialize the BrowserToolForUrl."""
        http_url = HttpUrl(url)
        if not http_url.host:
            raise ToolHardError("Invalid URL, host must be provided.")
        domain_parts = http_url.host.split(".")
        formatted_domain = "_".join(domain_parts)
        if not id:
            id = f"browser_tool_for_url_{formatted_domain}"  # noqa: A001
        if not name:
            name = f"Browser Tool for {formatted_domain}"
        if not description:
            description = (
                f"Browser tool for the URL {url}. Can be used to navigate to the URL and complete "
                "tasks."
            )
        super().__init__(
            id=id,
            name=name,
            description=description,
            args_schema=BrowserToolForUrlSchema,
            url=url,  # type: ignore reportCallIssue
            model=model,
            infrastructure_option=infrastructure_option,
        )

    def run(self, ctx: ToolRunContext, task: str) -> str | ActionClarification:  # type: ignore reportIncompatibleMethodOverride
        """Run the BrowserToolForUrl."""
        return super().run(ctx, self.url, task)


class BrowserInfrastructureProvider(ABC):
    """Abstract base class for browser infrastructure providers."""

    @abstractmethod
    def setup_browser(self, ctx: ToolRunContext) -> Browser:
        """Get a Browser instance."""

    @abstractmethod
    def construct_auth_clarification_url(self, ctx: ToolRunContext, sign_in_url: str) -> HttpUrl:
        """Construct the URL for the auth clarification."""


class BrowserInfrastructureProviderLocal(BrowserInfrastructureProvider):
    """Browser infrastructure provider for local browser instances."""

    @staticmethod
    def get_chrome_instance_path() -> str:
        """Get the path to the Chrome instance based on the operating system or env variable."""
        chrome_path_from_env = os.environ.get("PORTIA_BROWSER_LOCAL_CHROME_EXEC")
        if chrome_path_from_env:
            return chrome_path_from_env

        match sys.platform:
            case "darwin":  # macOS
                return "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
            case "win32":  # Windows
                return r"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
            case "linux":  # Linux
                return "/usr/bin/google-chrome"
            case _:
                raise RuntimeError(f"Unsupported platform: {sys.platform}")

    def __init__(self, chrome_path: str | None = None) -> None:
        """Initialize the BrowserInfrastructureProviderLocal."""
        self.chrome_path = chrome_path or self.get_chrome_instance_path()

    def setup_browser(self, ctx: ToolRunContext) -> Browser:
        """Get a Browser instance."""
        if ctx.execution_context.end_user_id:
            logger.warning(
                "BrowserTool is using a local browser instance and does not support "
                "end_user_id. end_user_id will be ignored.",
            )
        return Browser(config=BrowserConfig(chrome_instance_path=self.chrome_path))

    def construct_auth_clarification_url(self, ctx: ToolRunContext, sign_in_url: str) -> HttpUrl:  # noqa: ARG002
        """Construct the URL for the auth clarification."""
        return HttpUrl(sign_in_url)


class BrowserInfrastructureProviderBrowserBase(BrowserInfrastructureProvider):
    """Browser infrastructure provider for BrowserBase."""

    def __init__(self, api_key: str | None = None) -> None:
        """Initialize the BrowserBase infrastructure provider."""
        api_key = api_key or os.environ["BROWSERBASE_API_KEY"]
        if not api_key:
            raise ToolHardError("BROWSERBASE_API_KEY is not set")
        self.bb = Browserbase(api_key=api_key)

    def get_context_id(self, bb: Browserbase) -> str:
        """Get the Browserbase context id.

        This method can be overridden to return a saved context ID for a user.
        """
        return bb.contexts.create(project_id=os.environ["BROWSERBASE_PROJECT_ID"]).id

    def create_session(
        self,
        bb_context_id: str,
    ) -> SessionCreateResponse:
        """Get a fresh session with the given context ID."""
        return self.bb.sessions.create(
            project_id=os.environ["BROWSERBASE_PROJECT_ID"],
            browser_settings={
                "context": {
                    "id": bb_context_id,
                    "persist": True,
                },
            },
            # keep_alive is needed so that the session can last through clarification resolution.
            keep_alive=True,
        )

    def get_or_create_session(self, context: ToolRunContext, bb: Browserbase) -> str:
        """Get or create a Browserbase session."""
        context_id = context.execution_context.additional_data.get(
            "bb_context_id",
            self.get_context_id(bb),
        )
        context.execution_context.additional_data["bb_context_id"] = context_id

        session_id = context.execution_context.additional_data.get("bb_session_id", None)
        session_connect_url = context.execution_context.additional_data.get(
            "bb_session_connect_url",
            None,
        )

        if not session_id or not session_connect_url:
            session = self.create_session(context_id)
            session_connect_url = session.connect_url
            context.execution_context.additional_data["bb_session_id"] = session_id = session.id
            context.execution_context.additional_data["bb_session_connect_url"] = (
                session_connect_url
            )

        return session_connect_url

    def construct_auth_clarification_url(self, ctx: ToolRunContext, sign_in_url: str) -> HttpUrl:  # noqa: ARG002
        """Construct the URL for the auth clarification."""
        if not ctx.execution_context.additional_data["bb_session_id"]:
            raise ToolHardError("Session ID not found")
        live_view_link = self.bb.sessions.debug(
            ctx.execution_context.additional_data["bb_session_id"],
        )
        return HttpUrl(live_view_link.debugger_fullscreen_url)

    def setup_browser(self, ctx: ToolRunContext) -> Browser:
        """Get a Browser instance."""
        session_connect_url = self.get_or_create_session(ctx, self.bb)

        return Browser(
            config=BrowserConfig(
                cdp_url=session_connect_url,
            ),
        )

```

## File: portia/planning_agents/base_planning_agent.py

```python
"""PlanningAgents module creates plans from queries.

This module contains the PlanningAgent interfaces and implementations used for generating plans
based on user queries. It supports the creation of plans using tools and example plans, and
leverages LLMs to generate detailed step-by-step plans. It also handles errors gracefully and
provides feedback in the form of error messages when the plan cannot be created.
"""

from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING

from pydantic import BaseModel, ConfigDict, Field

from portia.plan import Plan, Step

if TYPE_CHECKING:
    from portia.config import Config
    from portia.tool import Tool

logger = logging.getLogger(__name__)


class BasePlanningAgent(ABC):
    """Interface for planning.

    This class defines the interface for PlanningAgents that generate plans based on queries.
    A PlanningAgent will implement the logic to generate a plan or an error given a query,
    a list of tools, and optionally, some example plans.

    Attributes:
        config (Config): Configuration settings for the PlanningAgent.

    """

    def __init__(self, config: Config) -> None:
        """Initialize the PlanningAgent with configuration.

        Args:
            config (Config): The configuration to initialize the PlanningAgent.

        """
        self.config = config

    @abstractmethod
    def generate_steps_or_error(
        self,
        query: str,
        tool_list: list[Tool],
        examples: list[Plan] | None = None,
    ) -> StepsOrError:
        """Generate a list of steps for the given query.

        This method should be implemented to generate a list of steps to accomplish the query based
        on the provided query and tools.

        Args:
            query (str): The user query to generate a list of steps for.
            tool_list (list[Tool]): A list of tools available for the plan.
            examples (list[Plan] | None): Optional list of example plans to guide the PlanningAgent.

        Returns:
            StepsOrError: A StepsOrError instance containing either the generated steps or an error.

        """
        raise NotImplementedError("generate_steps_or_error is not implemented")


class StepsOrError(BaseModel):
    """A list of steps or an error.

    This model represents either a list of steps for a plan or an error message if
    the steps could not be created.

    Attributes:
        steps (list[Step]): The generated steps if successful.
        error (str | None): An error message if the steps could not be created.

    """

    model_config = ConfigDict(extra="forbid")

    steps: list[Step]
    error: str | None = Field(
        default=None,
        description="An error message if the steps could not be created.",
    )

```

## File: portia/planning_agents/context.py

```python
"""Context helpers for PlanningAgents."""

from __future__ import annotations

from datetime import UTC, datetime
from typing import TYPE_CHECKING

from portia.templates.example_plans import DEFAULT_EXAMPLE_PLANS
from portia.templates.render import render_template

if TYPE_CHECKING:
    from portia.plan import Plan
    from portia.tool import Tool


def render_prompt_insert_defaults(
    query: str,
    tool_list: list[Tool],
    examples: list[Plan] | None = None,
) -> str:
    """Render the prompt for the PlanningAgent with defaults inserted if not provided."""
    system_context = default_query_system_context()

    if examples is None:
        examples = DEFAULT_EXAMPLE_PLANS

    tools_with_descriptions = get_tool_descriptions_for_tools(tool_list=tool_list)

    return render_template(
        "default_planning_agent.xml.jinja",
        query=query,
        tools=tools_with_descriptions,
        examples=examples,
        system_context=system_context,
    )


def default_query_system_context() -> list[str]:
    """Return the default system context."""
    return [f"Today is {datetime.now(UTC).strftime('%Y-%m-%d')}"]


def get_tool_descriptions_for_tools(tool_list: list[Tool]) -> list[dict[str, str]]:
    """Given a list of tool names, return the descriptions of the tools."""
    return [
        {
            "id": tool.id,
            "name": tool.name,
            "description": tool.description,
            "args": tool.args_schema.model_json_schema()["properties"],
            "output_schema": str(tool.output_schema),
        }
        for tool in tool_list
    ]

```

## File: portia/planning_agents/__init__.py

```python
"""PlanningAgent implementations."""

from portia.planning_agents.base_planning_agent import BasePlanningAgent
from portia.planning_agents.default_planning_agent import DefaultPlanningAgent

__all__ = ["BasePlanningAgent", "DefaultPlanningAgent"]

```

## File: portia/planning_agents/default_planning_agent.py

```python
"""DefaultPlanningAgent is a single best effort attempt at planning based on the given query + tools."""  # noqa: E501

from __future__ import annotations

import logging
from typing import TYPE_CHECKING

from portia.config import PLANNING_MODEL_KEY
from portia.model import Message
from portia.open_source_tools.llm_tool import LLMTool
from portia.planning_agents.base_planning_agent import BasePlanningAgent, StepsOrError
from portia.planning_agents.context import render_prompt_insert_defaults

if TYPE_CHECKING:
    from portia.config import Config
    from portia.plan import Plan, Step
    from portia.tool import Tool

logger = logging.getLogger(__name__)


class DefaultPlanningAgent(BasePlanningAgent):
    """DefaultPlanningAgent class."""

    def __init__(self, config: Config) -> None:
        """Init with the config."""
        self.model = config.resolve_model(PLANNING_MODEL_KEY)

    def generate_steps_or_error(
        self,
        query: str,
        tool_list: list[Tool],
        examples: list[Plan] | None = None,
    ) -> StepsOrError:
        """Generate a plan or error using an LLM from a query and a list of tools."""
        prompt = render_prompt_insert_defaults(
            query,
            tool_list,
            examples,
        )
        response = self.model.get_structured_response(
            schema=StepsOrError,
            messages=[
                Message(
                    role="system",
                    content="""
You are an outstanding task planner who can leverage many tools at their disposal. Your job is
to provide a detailed plan of action in the form of a set of steps to respond to a user's prompt.

IMPORTANT GUIDLINES:
- When using multiple tools, pay attention to the  tools to make sure the chain of steps works,
 but DO NOT provide any examples or assumptions  in the task descriptions.
- If you are missing information do not  make up placeholder variables like example@example.com.
- When creating the description for a step of the plan, if you need information from the previous
 step, DO NOT guess what that step will produce - instead, specify the previous step's output as an
 input for this step and allow this to be handled when we execute the plan.
- If you can't come up with a plan provide a descriptive error instead - do not
 return plans with no steps.
- For EVERY tool that requires an id as an input, make sure to check
 if there's a corresponding tool call that provides the id from natural language if possible.
 For example, if a tool asks for a user ID check if there's a tool call that provides
 the user IDs before making the tool call that  requires the user ID.
- For conditional steps:
  1. Task field: Write only the task description without conditions.
  2. Condition field: Write the condition in concise natural language.
- Do not use the condition field for non-conditional steps.
                    """,
                ),
                Message(role="user", content=prompt),
            ],
        )

        if not response.error:
            response.error = self._validate_tools_in_response(response.steps, tool_list)

        # Add LLMTool to the steps that don't have a tool_id.
        for step in response.steps:
            if step.tool_id is None:
                step.tool_id = LLMTool.LLM_TOOL_ID

        return StepsOrError(
            steps=response.steps,
            error=response.error,
        )

    def _validate_tools_in_response(self, steps: list[Step], tool_list: list[Tool]) -> str | None:
        """Validate that all tools in the response steps exist in the provided tool list.

        Args:
            steps (list[Step]): List of steps from the response
            tool_list (list[Tool]): List of available tools

        Returns:
            Error message if tools are missing, None otherwise

        """
        tool_ids = [tool.id for tool in tool_list]
        missing_tools = [
            step.tool_id for step in steps if step.tool_id and step.tool_id not in tool_ids
        ]
        return (
            f"Missing tools {', '.join(missing_tools)} from the provided tool_list"
            if missing_tools
            else None
        )

```

## File: portia/open_source_tools/image_understanding_tool.py

```python
"""Tool for responding to prompts and completing tasks that are related to image understanding."""

from __future__ import annotations

import base64
import mimetypes
from pathlib import Path
from typing import Any, Self

from langchain.schema import HumanMessage
from pydantic import BaseModel, Field, model_validator

from portia.errors import ToolHardError
from portia.model import LangChainGenerativeModel  # noqa: TC001 - used in Pydantic Schema
from portia.tool import Tool, ToolRunContext


class ImageUnderstandingToolSchema(BaseModel):
    """Input for Image Understanding Tool."""

    task: str = Field(
        ...,
        description="The task to be completed by the Image tool.",
    )
    image_url: str | None = Field(
        default=None,
        description="Image URL for processing.",
    )
    image_file: str | None = Field(
        default=None,
        description="Image file for processing.",
    )

    @model_validator(mode="after")
    def check_image_url_or_file(self) -> Self:
        """Check that only one of image_url or image_file is provided."""
        has_image_url = self.image_url is not None
        has_image_file = self.image_file is not None
        if not has_image_url ^ has_image_file:
            raise ValueError("One of image_url or image_file is required")
        return self


class ImageUnderstandingTool(Tool[str]):
    """General purpose image understanding tool. Customizable to user requirements."""

    id: str = "image_understanding_tool"
    name: str = "Image Understanding Tool"
    description: str = (
        "Tool for understanding images from a URL. Capable of tasks like object detection, "
        "OCR, scene recognition, and image-based Q&A. This tool uses its native capabilities "
        "to analyze images and provide insights."
    )
    args_schema: type[BaseModel] = ImageUnderstandingToolSchema
    output_schema: tuple[str, str] = (
        "str",
        "The Image understanding tool's response to the user query about the provided image.",
    )
    prompt: str = """
        You are an Image understanding tool used to analyze images and respond to queries.
        You can perform tasks like object detection, OCR, scene recognition, and image-based Q&A.
        Provide concise and accurate responses based on the image provided.
        """
    tool_context: str = ""

    model: LangChainGenerativeModel | None = Field(
        default=None,
        exclude=True,
        description="The model to use for the ImageUnderstandingTool. If not provided, "
        "the model will be resolved from the config.",
    )

    def run(self, ctx: ToolRunContext, **kwargs: Any) -> str:
        """Run the ImageTool."""
        model = self.model or ctx.config.resolve_langchain_model()

        tool_schema = ImageUnderstandingToolSchema(**kwargs)

        # Define system and user messages
        context = (
            "Additional context for the Image tool to use to complete the task, provided by the "
            "plan run information and results of other tool calls. Use this to resolve any "
            "tasks"
        )
        if self.tool_context:
            context += f"\nTool context: {self.tool_context}"
        content = (
            tool_schema.task
            if not len(context.split("\n")) > 1
            else f"{context}\n\n{tool_schema.task}"
        )

        if tool_schema.image_url:
            image_url = tool_schema.image_url
        elif tool_schema.image_file:  # pragma: no cover
            with Path(tool_schema.image_file).open("rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
                mime_type = mimetypes.guess_type(tool_schema.image_file)[0]
                image_url = f"data:{mime_type};base64,{image_data}"
        else:  # pragma: no cover
            raise ToolHardError("No image URL or file provided")

        messages = [
            HumanMessage(content=self.prompt),
            HumanMessage(
                content=[
                    {"type": "text", "text": content},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url},
                    },
                ],
            ),
        ]

        response = model.to_langchain().invoke(messages)
        return str(response.content)

```

## File: portia/open_source_tools/local_file_reader_tool.py

```python
"""Tool for reading files from disk."""

from __future__ import annotations

import json
from pathlib import Path

import pandas as pd
from pydantic import BaseModel, Field

from portia.clarification import Clarification, MultipleChoiceClarification
from portia.errors import ToolHardError
from portia.tool import Tool, ToolRunContext


class FileReaderToolSchema(BaseModel):
    """Schema defining the inputs for the FileReaderTool."""

    filename: str = Field(
        ...,
        description="The path (either full or relative) where the file should be read from",
    )


class FileReaderTool(Tool[str]):
    """Finds and reads content from a local file on Disk."""

    id: str = "file_reader_tool"
    name: str = "File reader tool"
    description: str = "Finds and reads content from a local file on Disk"
    args_schema: type[BaseModel] = FileReaderToolSchema
    output_schema: tuple[str, str] = ("str", "A string dump or JSON of the file content")

    def run(self, ctx: ToolRunContext, filename: str) -> str | Clarification:  # noqa: PLR0911
        """Run the FileReaderTool."""
        file_path = Path(filename)
        suffix = file_path.suffix.lower()

        if file_path.is_file():
            match suffix:
                case ".csv":
                    return pd.read_csv(file_path).to_string()
                case ".json":
                    with file_path.open("r", encoding="utf-8") as json_file:
                        return json.dumps(json.load(json_file), indent=4)
                case ".xls":
                    return pd.read_excel(file_path).to_string()
                case ".xlsx":
                    return pd.read_excel(file_path).to_string()
                case ".txt":
                    return file_path.read_text(encoding="utf-8")
                case ".log":
                    return file_path.read_text(encoding="utf-8")
                case _:
                    raise ToolHardError(
                        f"Unsupported file format: {suffix}."
                        "Supported formats are .txt, .log, .csv, .json, .xls, .xlsx.",
                    )

        alt_file_paths = self.find_file(file_path)
        if alt_file_paths:
            return MultipleChoiceClarification(
                plan_run_id=ctx.plan_run_id,
                argument_name="filename",
                user_guidance=(
                    f"Found {filename} in these location(s). "
                    f"Pick one to continue:\n{alt_file_paths}"
                ),
                options=alt_file_paths,
            )

        raise ToolHardError(f"No file found on disk with the path {filename}.")

    def find_file(self, file_path: Path) -> list[str]:
        """Return a full file path or None."""
        search_path = file_path.parent
        filename = file_path.name
        return [str(filepath) for filepath in search_path.rglob(filename) if filepath.is_file()]

```

## File: portia/open_source_tools/local_file_writer_tool.py

```python
"""Local file writer tool."""

from pathlib import Path

from pydantic import BaseModel, Field

from portia.tool import Tool, ToolRunContext


class FileWriterToolSchema(BaseModel):
    """Schema defining the inputs for the FileWriterTool."""

    filename: str = Field(
        ...,
        description="The location where the file should be saved",
    )
    content: str = Field(
        ...,
        description="The content to write to the file",
    )


class FileWriterTool(Tool[str]):
    """Writes content to a file."""

    id: str = "file_writer_tool"
    name: str = "File writer tool"
    description: str = "Writes content to a file locally"
    args_schema: type[BaseModel] = FileWriterToolSchema
    output_schema: tuple[str, str] = ("str", "A string indicating where the content was written to")

    def run(self, _: ToolRunContext, filename: str, content: str) -> str:
        """Run the FileWriterTool."""
        filepath = Path(filename)
        if filepath.is_file():
            with Path.open(filepath, "w") as file:
                file.write(content)
        else:
            with Path.open(filepath, "x") as file:
                file.write(content)
        return f"Content written to {filename}"

```

## File: portia/open_source_tools/__init__.py

```python
"""Sample example tools."""

```

## File: portia/open_source_tools/calculator_tool.py

```python
"""Simple Calculator Implementation."""

import re

from pydantic import BaseModel, Field

from portia.errors import ToolHardError
from portia.tool import Tool, ToolRunContext


class CalculatorToolSchema(BaseModel):
    """Input for the CalculatorTool."""

    math_question: str = Field(
        ...,
        description="The mathematical question to be evaluated in natural language",
    )


class CalculatorTool(Tool[float]):
    """Takes a basic maths question in natural language and returns the result.

    Works best for maths expressions containing only numbers and the operators +, -, *, x, /.
    """

    id: str = "calculator_tool"
    name: str = "Calculator Tool"
    description: str = "Takes a basic maths question and returns the result."
    "Works best for maths expressions containing only numbers and the operators +, -, *, x, /."
    args_schema: type[BaseModel] = CalculatorToolSchema
    output_schema: tuple[str, str] = ("str", "A string dump of the computed result")

    def run(self, _: ToolRunContext, math_question: str) -> float:
        """Run the CalculatorTool."""
        expression = self.math_expression(math_question)
        if not expression:
            raise ToolHardError("No valid mathematical expression found in the input.")

        try:
            result = eval(expression)  # noqa: S307
            return float(result)
        except ZeroDivisionError as e:
            raise ToolHardError("Error: Division by zero.") from e
        except Exception as e:
            raise ToolHardError(f"Error evaluating expression: {e}") from e

    def math_expression(self, prompt: str) -> str:  # noqa: C901, PLR0912
        """Convert words and phrases to standard operators."""
        prompt = prompt.lower()
        prompt = prompt.replace("added to", "+").replace("plus", "+").replace("and", "+")
        prompt = prompt.replace("minus", "-")
        prompt = prompt.replace("times", "*")
        prompt = prompt.replace("what is ", "").replace("?", "")
        prompt = prompt.replace("x", "*")  # Convert 'x' to '*' for multiplication

        # Handle "subtracted from" and "subtract from" separately
        if "subtracted from" in prompt:
            parts = prompt.split("subtracted from")
            if len(parts) == 2:  # noqa: PLR2004
                prompt = parts[1].strip() + " - " + parts[0].strip()
        elif "subtract" in prompt and "from" in prompt:
            match = re.search(r"subtract\s+(.+)\s+from\s+(.+)", prompt)
            if match:
                prompt = f"{match.group(2)} - {match.group(1)}"
        else:
            prompt = prompt.replace("subtract", "-")

        # Handle "divided by" and "divide by" separately
        if "divided by" in prompt:
            parts = prompt.split("divided by")
            if len(parts) == 2:  # noqa: PLR2004
                prompt = parts[0].strip() + " / " + parts[1].strip()
        elif "divide" in prompt and "by" in prompt:
            match = re.search(r"divide\s+(.+)\s+by\s+(.+)", prompt)
            if match:
                prompt = f"{match.group(1)} / {match.group(2)}"
        else:
            prompt = prompt.replace("divide", "/")

        # Handle "multiply by" and "multiplied by"
        if "multiply" in prompt and "by" in prompt:
            match = re.search(r"multiply\s+(.+)\s+by\s+(.+)", prompt)
            if match:
                prompt = f"{match.group(1)} * {match.group(2)}"
        elif "multiplied by" in prompt:
            parts = prompt.split("multiplied by")
            if len(parts) == 2:  # noqa: PLR2004
                prompt = parts[0].strip() + " * " + parts[1].strip()
        else:
            prompt = prompt.replace("multiply", "*")

        # Extract the mathematical expression
        return "".join(re.findall(r"[\d\+\-\*/\(\)\.\s]", prompt))

```

## File: portia/open_source_tools/weather.py

```python
"""Tool to get the weather from openweathermap."""

from __future__ import annotations

import os

import httpx
from pydantic import BaseModel, Field

from portia.errors import ToolHardError, ToolSoftError
from portia.tool import Tool, ToolRunContext


class WeatherToolSchema(BaseModel):
    """Input for WeatherTool."""

    city: str = Field(..., description="The city to get the weather for")


class WeatherTool(Tool[str]):
    """Get the weather for a given city."""

    id: str = "weather_tool"
    name: str = "Weather Tool"
    description: str = "Get the weather for a given city"
    args_schema: type[BaseModel] = WeatherToolSchema
    output_schema: tuple[str, str] = ("str", "String output of the weather with temp and city")

    def run(self, _: ToolRunContext, city: str) -> str:
        """Run the WeatherTool."""
        api_key = os.getenv("OPENWEATHERMAP_API_KEY")
        if not api_key or api_key == "":
            raise ToolHardError("OPENWEATHERMAP_API_KEY is required")
        url = (
            f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
        )
        response = httpx.get(url)
        response.raise_for_status()
        data = response.json()
        if "weather" not in data:
            raise ToolSoftError(f"No data found for: {city}")
        weather = data["weather"][0]["description"]
        if "main" not in data:
            raise ToolSoftError(f"No main data found for city: {city}")
        temp = data["main"]["temp"]
        return f"The current weather in {city} is {weather} with a temperature of {temp}°C."

```

## File: portia/open_source_tools/registry.py

```python
"""Example registry containing simple tools."""

import logging

from portia.open_source_tools.calculator_tool import CalculatorTool
from portia.open_source_tools.image_understanding_tool import ImageUnderstandingTool
from portia.open_source_tools.llm_tool import LLMTool
from portia.open_source_tools.local_file_reader_tool import FileReaderTool
from portia.open_source_tools.local_file_writer_tool import FileWriterTool
from portia.open_source_tools.search_tool import SearchTool
from portia.open_source_tools.weather import WeatherTool
from portia.tool_registry import (
    ToolRegistry,
)

logger = logging.getLogger(__name__)

example_tool_registry = ToolRegistry(
    [CalculatorTool(), WeatherTool(), SearchTool()],
)


open_source_tool_registry = ToolRegistry(
    [
        CalculatorTool(),
        WeatherTool(),
        SearchTool(),
        LLMTool(),
        FileWriterTool(),
        FileReaderTool(),
        ImageUnderstandingTool(),
    ],
)

```

## File: portia/open_source_tools/search_tool.py

```python
"""Simple Search Tool."""

from __future__ import annotations

import os

import httpx
from pydantic import BaseModel, Field

from portia.errors import ToolHardError, ToolSoftError
from portia.tool import Tool, ToolRunContext

MAX_RESULTS = 3


class SearchToolSchema(BaseModel):
    """Input for SearchTool."""

    search_query: str = Field(
        ...,
        description=(
            "The query to search for. For example, 'what is the capital of France?' or "
            "'who won the US election in 2020?'"
        ),
    )


class SearchTool(Tool[str]):
    """Searches the internet to find answers to the search query provided.."""

    id: str = "search_tool"
    name: str = "Search Tool"
    description: str = (
        "Searches the internet (using Tavily) to find answers to the search query provided and "
        "returns those answers, including images, links and a natural language answer. "
        "The search tool has access to general information but can not return specific "
        "information on users or information not available on the internet"
    )
    args_schema: type[BaseModel] = SearchToolSchema
    output_schema: tuple[str, str] = ("str", "str: output of the search results")
    should_summarize: bool = True

    def run(self, _: ToolRunContext, search_query: str) -> str:
        """Run the Search Tool."""
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key or api_key == "":
            raise ToolHardError("TAVILY_API_KEY is required to use search")

        url = "https://api.tavily.com/search"

        payload = {
            "query": search_query,
            "include_answer": True,
            "api_key": api_key,
        }
        headers = {"Content-Type": "application/json"}

        response = httpx.post(url, headers=headers, json=payload)
        response.raise_for_status()
        json_response = response.json()
        if "answer" in json_response:
            results = json_response["results"]
            return results[:MAX_RESULTS]
        raise ToolSoftError(f"Failed to get answer to search: {json_response}")

```

## File: portia/open_source_tools/llm_tool.py

```python
"""Tool for responding to prompts and completing tasks that don't require other tools."""

from __future__ import annotations

from typing import ClassVar

from pydantic import BaseModel, Field

from portia.model import GenerativeModel, Message
from portia.tool import Tool, ToolRunContext


class LLMToolSchema(BaseModel):
    """Input for LLM Tool."""

    task: str = Field(
        ...,
        description="The task to be completed by the LLM tool.",
    )


class LLMTool(Tool[str]):
    """General purpose LLM tool. Customizable to user requirements. Won't call other tools."""

    LLM_TOOL_ID: ClassVar[str] = "llm_tool"
    id: str = LLM_TOOL_ID
    name: str = "LLM Tool"
    description: str = (
        "Jack of all trades tool to respond to a prompt by relying solely on LLM capabilities. "
        "YOU NEVER CALL OTHER TOOLS. You use your native capabilities as an LLM only. "
        "This includes using your general knowledge, your in-built reasoning "
        "and your code interpreter capabilities. This tool can be used to summarize the outputs of "
        "other tools, make general language model queries or to answer questions. This should be "
        "used only as a last resort when no other tool satisfies a step in a task, however if "
        "there are no other tools that can be used to complete a step or for steps that don't "
        "require a tool call, this SHOULD be used"
    )
    args_schema: type[BaseModel] = LLMToolSchema
    output_schema: tuple[str, str] = (
        "str",
        "The LLM's response to the user query.",
    )
    prompt: str = """
        You are a Jack of all trades used to respond to a prompt by relying solely on LLM.
        capabilities. YOU NEVER CALL OTHER TOOLS. You use your native capabilities as an LLM
         only. This includes using your general knowledge, your in-built reasoning and
         your code interpreter capabilities. You exist as part of a wider system of tool calls
         for a multi-step task to be used to answers questions, summarize outputs of other tools
         and to make general language model queries. You might not have all the context of the
         wider task, so you should use your general knowledge and reasoning capabilities to make
         educated guesses and assumptions where you don't have all the information. Be concise and
         to the point.
        """
    tool_context: str = ""

    model: GenerativeModel | None = Field(
        default=None,
        exclude=True,
        description="The model to use for the LLMTool. If not provided, "
        "the model will be resolved from the config.",
    )

    def run(self, ctx: ToolRunContext, task: str) -> str:
        """Run the LLMTool."""
        model = self.model or ctx.config.resolve_model()

        # Define system and user messages
        context = (
            "Additional context for the LLM tool to use to complete the task, provided by the "
            "run information and results of other tool calls. Use this to resolve any "
            "tasks"
        )
        if self.tool_context:
            context += f"\nTool context: {self.tool_context}"
        content = task if not len(context.split("\n")) > 1 else f"{context}\n\n{task}"
        messages = [
            Message(role="user", content=self.prompt),
            Message(role="user", content=content),
        ]
        response = model.get_response(messages)
        return str(response.content)

```

## File: portia/templates/__init__.py

```python
"""Contains templates for LLM Context."""

from portia.templates.render import render_template

__all__ = ["render_template"]

```

## File: portia/templates/example_plans.py

```python
"""Default examples that are passed to the query planning_agent if none are provided."""

from portia.plan import Plan, PlanContext, Step, Variable

DEFAULT_EXAMPLE_PLANS: list[Plan] = [
    Plan(
        plan_context=PlanContext(
            query="Compare the weather of a city in the Southern hemisphere with that of a city in the Northern hemisphere. Email the results to hello@portialabs.ai.",  # noqa: E501
            tool_ids=[
                "search_tool",
                "portia::google_gmail::send_email_tool",
                "portia::provider::other_tool",
                "weather_tool",
            ],
        ),
        steps=[
            Step(
                task="What is a city in the Southern hemisphere?",
                tool_id="search_tool",
                output="$southern_hemisphere_city",
            ),
            Step(
                task="What is a city in the Northern hemisphere?",
                tool_id="search_tool",
                output="$northern_hemisphere_city",
            ),
            Step(
                task="What is the weather in the city in the input?",
                inputs=[
                    Variable(
                        name="$southern_hemisphere_city",
                        description="City in the southern hemisphere",
                    ),
                ],
                tool_id="weather_tool",
                output="$southern_hemisphere_city_weather",
            ),
            Step(
                task="What is the weather in the city in the input?",
                inputs=[
                    Variable(
                        name="$northern_hemisphere_city",
                        description="City in the norther hemisphere",
                    ),
                ],
                tool_id="weather_tool",
                output="$northern_hemisphere_city_weather",
            ),
            Step(
                task="Compare the weather of the 2 cities ($southern_hemisphere_city_weather and $northern_hemisphere_city_weather) and write a comparison summarizing the similarities and differences",  # noqa: E501
                inputs=[
                    Variable(
                        name="$southern_hemisphere_city_weather",
                        description="Weather of a city in the southern hemisphere",
                    ),
                    Variable(
                        name="$northern_hemisphere_city_weather",
                        description="Weather of a city in the northern hemisphere",
                    ),
                ],
                output="$weather_comparison",
            ),
            Step(
                task="Email hello@portialabs.ai with a $weather_comparison",
                inputs=[
                    Variable(
                        name="$weather_comparison",
                        description="Comparison of the weather in the two cities",
                    ),
                ],
                tool_id="portia::google_gmail::send_email_tool",
                output="$email_sent",
            ),
        ],
    ),
    Plan(
        plan_context=PlanContext(
            query="If the weather in London hotter than 10C, sum it with the weather in Cairo and "
            "send the result to hello@portialabs.ai",
            tool_ids=[
                "weather_tool",
                "portia::google_gmail::send_email_tool",
                "portia::provider::other_tool",
            ],
        ),
        steps=[
            Step(
                task="Get the weather for London",
                tool_id="weather_tool",
                output="$london_weather",
            ),
            Step(
                task="Get the weather for Cairo",
                tool_id="weather_tool",
                output="$cairo_weather",
                condition="if $london_weather is hotter than 10C",
            ),
            Step(
                task="Sum the weather in London and Cairo",
                inputs=[
                    Variable(
                        name="$london_weather",
                        description="Weather in London",
                    ),
                    Variable(
                        name="$cairo_weather",
                        description="Weather in Cairo",
                    ),
                ],
                output="$weather_sum",
                condition="if $london_weather is hotter than 10C",
            ),
            Step(
                task="Email hello@portialabs.ai with $weather_sum",
                inputs=[
                    Variable(
                        name="$weather_sum",
                        description="Sum of the weather in London and Cairo",
                    ),
                ],
                tool_id="portia::google_gmail::send_email_tool",
                output="$email_sent",
                condition="if $london_weather is hotter than 10C",
            ),
        ],
    ),
    Plan(
        plan_context=PlanContext(
            query="Get my (john@jo.co) availability from Google Calendar tomorrow between \
              10:00 and 17:00\n- Schedule a 30 minute meeting with hello@jo.co at a time \
              that works for me",
            tool_ids=[
                "portia::google_calendar::get_availability",
                "portia::google_calendar::create_event",
            ],
        ),
        steps=[
            Step(
                task="Get the availability of john@jo.co from Google Calendar tomorrow \
                    between 10:00 and 17:00",
                tool_id="portia::google_calendar::get_availability",
                output="$availability",
            ),
            Step(
                task="Schedule a 30 minute meeting with hello@jo.co at a time that works for me",
                tool_id="portia::google_calendar::create_event",
                inputs=[
                    Variable(
                        name="$availability",
                        description="Availability of john@jo.co",
                    ),
                ],
                output="$event_created",
            ),
        ],
    ),
    Plan(
        plan_context=PlanContext(
            query="Get the latest messages on the Dev channel and send a summary to nathan",
            tool_ids=[
                "portia::slack::bot::list_conversation_ids",
                "portia::slack::bot::conversation_history",
                "portia::slack::bot::list_user_ids",
                "portia::slack::bot::send_message",
            ],
        ),
        steps=[
            Step(
                task="Get the id of the Dev channel",
                tool_id="portia::slack::bot::list_conversation_ids",
                output="$conversation_ids",
            ),
            Step(
                task="Get the latest messages on the Dev channel",
                inputs=[
                    Variable(
                        name="$conversation_ids",
                        description="The id of the Dev channel",
                    ),
                ],
                tool_id="portia::slack::bot::conversation_history",
                output="$conversation_history",
            ),
            Step(
                task="get the user id of nathan",
                tool_id="portia::slack::bot::list_user_ids",
                output="$nathan_user_id",
            ),
            Step(
                task="send a summary of the conversation to nathan",
                inputs=[
                    Variable(
                        name="$conversation_history",
                        description="The conversation history",
                    ),
                    Variable(
                        name="$nathan_user_id",
                        description="The user id of nathan",
                    ),
                ],
                tool_id="portia::slack::bot::send_message",
                output="$message_sent",
            ),
        ],
    ),
]

```

## File: portia/templates/render.py

```python
"""Render templates.

This module provides a utility function to render Jinja templates. It loads a template from the file
system and renders it to a string, allowing for dynamic generation of content with provided
keyword arguments.
"""

import importlib.resources
from typing import Any

from jinja2 import Environment, FileSystemLoader


def render_template(file_name: str, **kwargs: Any) -> str:
    """Render a Jinja template from the file system into a string.

    This function loads a template file from the `portia.templates` package,
    and using Jinja2 renders the template with the provided keyword arguments.

    Args:
        file_name (str): The name of the template file to be rendered.
        **kwargs (Any): Keyword arguments that will be passed to the template for rendering.

    Returns:
        str: The rendered template as a string.

    Example:
        rendered = render_template("example_template.html", user_name="Alice")

    """
    from portia import templates

    source = importlib.resources.files(templates).joinpath(file_name)
    with importlib.resources.as_file(source) as template_path:
        env = Environment(loader=FileSystemLoader(template_path.parent), autoescape=True)
        template = env.get_template(file_name)
        return template.render(**kwargs)

```

## File: .github/pull_request_template.md

```markdown
# Description

Please include a summary of the change. Please also include relevant motivation and context. List any dependencies that are required for this change.

Ticket Link: N/A 

## Type of change

(select all that apply)

- [ ] Bug fix 
- [ ] New feature 
- [ ] Breaking change 
- [ ] Refactor
- [ ] Requires sync with platform release
- [ ] Documentation update

## Screenshots

(If applicable, add screenshots to help explain your changes)

## Changelog

(If applicable, add a changelog [entry](https://keepachangelog.com/en/))

```

## File: .github/workflows/evals.yml

```yaml
name: Run Evals for portia-sdk-python

# Note: Any changes made here should be reflected in the evals.yml file in the platform repo
# Actions from private repos are inaccessible from public repos

on:
  workflow_dispatch:
  pull_request_target:
    branches:
      - main
    types:
      - synchronize
      - labeled

permissions:
    contents: read
    pull-requests: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event.pull_request.number }}
  cancel-in-progress: true

jobs:
  evals:
    runs-on: ubuntu-latest
    # Skip PRs unless they have the 'ready_to_eval' label
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'ready_to_eval')
    steps:
      - name: Checkout portia-sdk-python repo
        uses: actions/checkout@v4
        with:
          path: portia-sdk-python
          repository: portiaAI/portia-sdk-python
          ref: ${{ github.head_ref || 'main' }}
          token: ${{ secrets.PORTIA_GH_TOKEN }}

      - name: Checkout platform repo
        uses: actions/checkout@v4
        with:
          path: platform
          repository: portiaAI/platform
          token: ${{ secrets.PORTIA_GH_TOKEN }}

      - uses: actions/setup-python@v4
        with:
          python-version: "3.12"
      
      - name: Install UV
        run: pip install uv
      
      - name: Install dependencies
        working-directory: ./platform/evals
        run: |
          uv add ../../portia-sdk-python/
          uv sync --locked --no-dev
      
      - name: Check tool IDs
        working-directory: ./platform/evals
        id: check_tool_ids
        env:
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          uv run check_tool_ids.py

      - name: eval query planner
        id: eval_query_planner
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ secrets.LANGCHAIN_PROJECT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
        run: |
          EVAL_OUTPUT=$(uv run cli.py query-planner eval --model=claude-3-5-sonnet-latest --threshold_file=query_planner/thresholds/claude-3-5-sonnet-latest/thresholds_local.yaml --reps 1 --metadata "pr=${{ github.event.pull_request.number }},author=${{ github.event.pull_request.user.login || github.actor }},run=pr,env=local,repo=sdk" --slice_name main  --max_concurrency 32)
          echo "eval_url=$(echo "$EVAL_OUTPUT" | grep -o '${LANGCHAIN_ENDPOINT}/.*')" >> $GITHUB_OUTPUT
          echo "eval_name=$(echo "$EVAL_OUTPUT" | grep -oP "experiment:\s*'\K[^']+")" >> $GITHUB_OUTPUT
          if echo "$EVAL_OUTPUT" | grep -q "EVAL BREACH"; then
            BREACHES=$(echo "$EVAL_OUTPUT" | grep "EVAL BREACH:" | tr '\n' ' ' | sed 's/"/\\"/g')
            echo "metric_breaches=${BREACHES}" >> $GITHUB_OUTPUT
            echo "has_failing_eval_planner_scores=true" >> $GITHUB_OUTPUT
          fi

      - name: eval agent (verifier)
        id: eval_agent_verifier
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ secrets.LANGCHAIN_PROJECT }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
        run: |
          EVAL_OUTPUT=$(uv run cli.py agent eval --slice_name=main --model=claude-3-5-sonnet-latest --threshold_file=agents/thresholds/claude-3-5-sonnet-latest/thresholds_local.yaml --reps 1 --metadata "pr=${{ github.event.pull_request.number }},author=${{ github.event.pull_request.user.login || github.actor }},run=pr,env=local,repo=sdk"  --max_concurrency 32)
          echo "eval_url=$(echo "$EVAL_OUTPUT" | grep -o 'https://smith.langchain.com/.*')" >> $GITHUB_OUTPUT
          echo "eval_name=$(echo "$EVAL_OUTPUT" | grep -oP "experiment:\s*'\K[^']+")" >> $GITHUB_OUTPUT
          if echo "$EVAL_OUTPUT" | grep -q "EVAL BREACH"; then
            BREACHES=$(echo "$EVAL_OUTPUT" | grep "EVAL BREACH:" | tr '\n' ' ' | sed 's/"/\\"/g')
            echo "BREACHES: $BREACHES"
            echo "metric_breaches=${BREACHES}" >> $GITHUB_OUTPUT
            echo "has_failing_eval_agent_scores=true" >> $GITHUB_OUTPUT
          fi

      - name: Summary results
        id: summary_results
        working-directory: ./platform/evals
        env:
          LANGCHAIN_TRACING_V2: "true"
          LANGCHAIN_ENDPOINT: "https://api.smith.langchain.com"
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_PROJECT: ${{ secrets.LANGCHAIN_PROJECT }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_EVAL_API_KEY }}
          PORTIA_API_ENDPOINT: ${{ secrets.PORTIA_EVAL_API_ENDPOINT }}
          AGENT_VERIFIER_EXPERIMENT_ID: ${{ steps.eval_agent_verifier.outputs.eval_name }}
          QUERY_PLANNER_EXPERIMENT_ID: ${{ steps.eval_query_planner.outputs.eval_name }}
        run: |
          uv run jupyter nbconvert --to markdown --execute github_analysis.ipynb --output notebook_output.md --no-input
          cat notebook_output.md
          # Removes style blocks that GitHub won't render properly
          sed -i '/<style[^>]*>/,/<\/style>/d' notebook_output.md
          cat notebook_output.md
          cat notebook_output.md >> $GITHUB_STEP_SUMMARY

      - name: Check for evaluation failures
        run: |
          CONTAINS_THRESHOLD_BREACH=false

          # Check if the query planner has failing scores
          if [[ "${{ steps.eval_query_planner.outputs.has_failing_eval_planner_scores }}" == "true" ]]; then
            echo "Query planner eval failed or has breaches."
            echo "Breaches: ${{ steps.eval_query_planner.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi

          # Check if the verifier agent has failing scores
          if [[ "${{ steps.eval_agent_verifier.outputs.has_failing_eval_agent_scores }}" == "true" ]]; then
            echo "Agent eval (verifier) failed or has breaches."
            echo "Breaches: ${{ steps.eval_agent_verifier.outputs.metric_breaches }}"
            CONTAINS_THRESHOLD_BREACH=true
          fi

          # Exit with a non-zero status if any failures were detected
          if [[ "$CONTAINS_THRESHOLD_BREACH" == "true" ]]; then
            echo "One or more evaluations failed."
            exit 1
          fi

```

## File: .github/workflows/docs.yml

```yaml
name: Update Docs

on:
  push:
    branches:
      - main

jobs:
  update-docs:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout SDK repo
        uses: actions/checkout@v4

      - name: Generate documentation
        run: |
          pip install pydoc-markdown
          pydoc-markdown

      - name: Checkout Docs repo
        uses: actions/checkout@v4
        with:
          repository: portiaAI/docs
          token: ${{ secrets.DOCS_REPO_PAT }}
          path: docs-repo

      - name: Copy generated docs
        run: |
          rm -rf docs-repo/docs/SDK
          mv docs/SDK docs-repo/docs/SDK

      - name: Commit and push changes
        run: |
          cd docs-repo
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git commit -m "Update docs from SDK [skip ci]" || echo "No changes to commit"
          git push origin main

```

## File: .github/workflows/create-release-tag.yml

```yaml
name: Create Release Tag

on:
  pull_request:
    types: [closed]
    branches:
      - main

jobs:
  create-tag:
    if: github.event.pull_request.merged == true && startsWith(github.event.pull_request.head.ref, 'release/v')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
  
      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

      - name: Get version from pyproject.toml
        id: get_version
        run: |
          VERSION=$(poetry version -s)
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Create tag
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            const version = 'v${{ steps.get_version.outputs.version }}';
            const sha = context.sha;
            
            try {
              await github.rest.git.createRef({
                owner: context.repo.owner,
                repo: context.repo.repo,
                ref: `refs/tags/${version}`,
                sha: sha
              });
              console.log(`Successfully created tag ${version}`);
            } catch (error) {
              core.setFailed(`Failed to create tag: ${error.message}`);
            } 
```

## File: .github/workflows/release.yml

```yaml
name: Version Tag Creation

on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to create tag from'
        required: true
        default: 'production'
        type: choice
        options:
          - main
          - production

permissions:
  contents: write
  actions: write

jobs:
  create-tag:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install Poetry
        run: pipx install poetry

      - name: Fetch tags
        run: git fetch --tags

      - name: Get version from pyproject.toml
        id: get_version
        run: |
          VERSION=$(poetry version --short)
          echo "Version: $VERSION"
          echo "VERSION=$VERSION" >> $GITHUB_ENV

      - name: Check if version is already tagged
        run: |
          if git rev-parse "$VERSION" >/dev/null 2>&1; then
            echo "Tag $VERSION already exists. Skipping tag creation."
            echo "tag_exists=true" >> $GITHUB_ENV
          else
            echo "Tag $VERSION does not exist. Creating new tag."
          fi

      - name: Create Git tag
        if: env.tag_exists != 'true'
        run: |
          git tag $VERSION

      - name: Push changes
        if: env.tag_exists != 'true'
        run: |
          git push origin $VERSION
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Publish to PyPI
        if: env.tag_exists != 'true'
        run: |
          export POETRY_PYPI_TOKEN_PYPI=${{secrets.POETRY_PYPI_TOKEN_PYPI}}
          poetry build
          poetry publish -n

```

## File: .github/workflows/licence.yml

```yaml
name: License Check

on:
  push:
    branches: [main]
  pull_request:

jobs:
  license-check:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Poetry
        run: pipx install poetry

      - name: Install dependencies
        run: |
          poetry install --all-extras
          # install license check via pip so it doesn't check itself
          poetry run pip install licensecheck pipdeptree

      - name: Check licenses
        run: |
          echo "Checking for disallowed licenses..."
          echo "::group::License Report"
          # Run license check and always show output
          poetry run python -m licensecheck --zero | tee license_output.txt
          EXIT_CODE=${PIPESTATUS[0]}

          # If there are failures, extract incompatible packages and show their reverse dependencies
          if [ $EXIT_CODE -ne 0 ]; then
            echo "❌ The following packages have incompatible licenses:"
            INCOMPATIBLE=$(cat license_output.txt | grep "^│ ✖" | sed 's/│ ✖/❌/g')
            echo "$INCOMPATIBLE"
            
            if [ ! -z "$INCOMPATIBLE" ]; then
              echo -e "\n📦 Reverse dependency information:"
              echo "$INCOMPATIBLE" | while read -r line; do
                PKG=$(echo "$line" | awk '{print $3}')
                echo -e "\nDependency tree for $PKG:"
                poetry run pipdeptree --reverse --packages "$PKG"
              done
            fi
            
            echo "::endgroup::"
            echo "::error::License check failed! Please ensure all dependencies use permissive licenses (MIT, Apache-2.0, BSD, ISC)."
            exit 1
          fi

          echo "::endgroup::"
          echo "✅ All dependency licenses are compliant!"

```

## File: .github/workflows/branch_checks.yml

```yaml
name: Branch is up-to-date

on:
  pull_request:
    branches:
      - 'main'
      - 'production'

jobs:
  check-branch:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - run: |
          BASE_BRANCH="origin/${GITHUB_BASE_REF}"
          CURRENT_BRANCH=$(git branch --show-current)
          if ! git merge-base --is-ancestor $BASE_BRANCH ${{ github.event.pull_request.head.sha }}; then
            echo "Error: The current branch is not based on $BASE_BRANCH."
            exit 1
          fi

```

## File: .github/workflows/ruff.yml

```yaml
name: Formatting (ruff)
on: [pull_request]
permissions:
  contents: read
  pull-requests: read
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v1
        with:
          args: check

```

## File: .github/workflows/ready_to_eval_labeller.yml

```yaml
name: Ready to eval Labeller

on:
  pull_request:
    types: [review_requested]

jobs:
  add-label:
    runs-on: ubuntu-latest
    steps:
      - name: Add 'ready_to_eval' label
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.EVALS_LABELLER_GITHUB_TOKEN }}
          script: |
            github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              labels: ['ready_to_eval']
            })
```

## File: .github/workflows/pyright.yml

```yaml
name: Run Pyright

on:
  pull_request:
    branches:
      - "*"

jobs:
  pyright:
    name: Static Type Checking with Pyright
    runs-on: ubuntu-latest

    steps:
      # Checkout the code
      - name: Checkout code
        uses: actions/checkout@v4

      # Set up Node.js (required for Pyright)
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "16"

      # Install Pyright globally
      - name: Install Pyright
        run: npm install -g pyright

        # Install Poetry
      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      # Install dependencies using Poetry
      - name: Install dependencies
        run: poetry install --no-interaction --all-extras

      # Run Pyright
      - name: Run Pyright
        run: poetry run pyright

```

## File: .github/workflows/create-release-pr.yml

```yaml
name: Create Release PR

on:
  workflow_dispatch:
    inputs:
      release_type:
        description: 'Release type (alpha/release)'
        required: true
        type: choice
        options:
          - alpha
          - release
      bump_type:
        description: 'Version bump type (v{major}.{minor}.{patch})'
        required: true
        type: choice
        options:
          - patch
          - minor
          - major
        default: 'patch'

jobs:
  create-release-pr:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -

      - name: Get current version
        id: current_version
        run: |
          CURRENT_VERSION=$(poetry version -s)
          echo "current_version=$CURRENT_VERSION" >> $GITHUB_OUTPUT

      - name: Bump version
        id: bump_version
        run: |
          # Remove potential alpha suffix from current version
          BASE_VERSION=$(echo "${{ steps.current_version.outputs.current_version }}" | sed 's/-alpha//')
          
          # Bump version according to input
          case "${{ inputs.bump_type }}" in
            patch)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$NF = $NF + 1;} 1' | sed 's/ /./g')
              ;;
            minor)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$(NF-1) = $(NF-1) + 1; $NF = 0;} 1' | sed 's/ /./g')
              ;;
            major)
              NEW_VERSION=$(echo $BASE_VERSION | awk -F. '{$1 = $1 + 1; $(NF-1) = 0; $NF = 0;} 1' | sed 's/ /./g')
              ;;
          esac
          
          # Add alpha suffix if requested
          if [ "${{ inputs.release_type }}" = "alpha" ]; then
            NEW_VERSION="${NEW_VERSION}-alpha"
          fi
          
          # Set output for next steps
          echo "new_version=$NEW_VERSION" >> $GITHUB_OUTPUT

      - name: Create release branch and update version
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
          git checkout -b release/v${{ steps.bump_version.outputs.new_version }}
          
          # Update version in release branch
          poetry version ${{ steps.bump_version.outputs.new_version }}
          
          git add pyproject.toml
          git commit -m "Bump version to v${{ steps.bump_version.outputs.new_version }}"
          git push origin release/v${{ steps.bump_version.outputs.new_version }}

      - name: Create Pull Request
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.DEPLOY_PAT_TOKEN }}
          script: |
            const { data: pr } = await github.rest.pulls.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Release v${{ steps.bump_version.outputs.new_version }}`,
              body: `Automated PR for version bump to v${{ steps.bump_version.outputs.new_version }}`,
              head: `release/v${{ steps.bump_version.outputs.new_version }}`,
              base: 'main'
            });
            
            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr.number,
              labels: ['release']
            });
```

## File: .github/workflows/testing.yml

```yaml
name: Python Testing

on:
  pull_request_target:
    types:
      - opened
      - synchronize
      - labeled

permissions:
  contents: read
  pull-requests: read

jobs:
  testing:
    timeout-minutes: 10
    runs-on: ubuntu-latest-16core
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: install poetry
        run: pipx install poetry

      # https://github.com/pydantic/ollama-action
      - uses: pydantic/ollama-action@v3
        with:
          model: qwen2:0.5b

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "poetry"

      - name: install dependencies
        run: poetry install --all-extras

      - name: unit_tests + coverage
        env:
          PORTIA_API_ENDPOINT: "https://api.porita.dev"
        run: |
          set -o pipefail
          poetry run pytest tests/unit

      - name: tests + coverage
        env:
          PORTIA_API_ENDPOINT: "https://api.porita.dev"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          PORTIA_API_KEY: ${{ secrets.PORTIA_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          set -o pipefail
          poetry run pytest -n 8 --cov --cov-fail-under 100 --log-cli-level=WARNING --junitxml=pytest.xml | tee pytest-coverage.txt

```

## File: .github/workflows/deploy_pypi.yml

```yaml
name: Release to PyPI

on:
  push:
    tags:
      - 'v[0-9]+.[0-9]+.[0-9]+'
      - 'v[0-9]+.[0-9]+.[0-9]+-alpha'

permissions:
  contents: read
  actions: write

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install Poetry
        run: pipx install poetry

      - name: Extract version from tag
        id: get_tag_version
        run: |
          # Remove 'v' prefix from tag
          TAG_VERSION=${GITHUB_REF#refs/tags/v}
          echo "tag_version=$TAG_VERSION" >> $GITHUB_OUTPUT

      - name: Get version from pyproject.toml
        id: get_project_version
        run: |
          PROJECT_VERSION=$(poetry version -s)
          echo "project_version=$PROJECT_VERSION" >> $GITHUB_OUTPUT

      - name: Verify versions match
        id: verify_versions
        run: |
          if [ "${{ steps.get_tag_version.outputs.tag_version }}" != "${{ steps.get_project_version.outputs.project_version }}" ]; then
            echo "Tag version (${{ steps.get_tag_version.outputs.tag_version }}) does not match project version (${{ steps.get_project_version.outputs.project_version }})"
            exit 1
          fi

      - name: Build and publish to PyPI
        id: pypi_publish
        run: |
          poetry config pypi-token.pypi ${{ secrets.POETRY_PYPI_TOKEN_PYPI }}
          poetry build
          poetry publish --no-interaction
        
      - name: Notify Slack on success
        if: success()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          channel-id: '${{ vars.SLACK_DEV_CHANNEL }}'
          slack-message: "✅ Successfully published SDK version ${{ steps.get_tag_version.outputs.tag_version }} to PyPI! 🎉"
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}

      - name: Notify Slack on failure
        if: failure()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          channel-id: '${{ vars.SLACK_RUN_CHANNEL }}'
          slack-message: "❌ Failed to publish version ${{ steps.get_tag_version.outputs.tag_version }} to PyPI.\nSee: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}

```

